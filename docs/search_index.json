[["1-ch-likelihood.html", "Chapter 1 Likelihood-based methods", " Chapter 1 Likelihood-based methods This chapter is about methods for statistical inference based on the likelihood function. We start by discussing how we can find a point estimate of a parameter by using the maximum likelihood estimate. Once we have a point estimate we want to know how certain or uncertain this estimate is. This can be done by either simply calculating the standard deviation of the estimate or by calculating a confidence interval or by doing a hypothesis test. We then discuss the likelihood ratio test which is a general way of testing hypothesis based on the likelihood function. However, in many situations the distribution of the likelihood ratio under the null hypothesis is difficult to find, so we turn to asymptotic results. I.e. results that are valid when the sample size is large. It turns out that maximum likelihood estimates in general have an asymptotic normal distribution, with the true parameter value as expected value and variance given by the Fisher information, which can be calculated from the log-likelihood function. Based in this we find three different types of asymptotic hypothesis tests. Finally we will see how this can be applied to the binary regression model. Readings for this chapter is: ASI 2.1, 2.2, 2.4 ASI 3.2, 3.2 (not 3.2.5), 3.3 ASI 4.1, 4.2 (not 4.2.1), 4.4 "],["1.1-the-likelihood-function.html", "1.1 The likelihood function", " 1.1 The likelihood function In this section we introduce the likelihood function, the tool we will use in this chapter for doing inference. Let us begin with a simple example. Imagine that we toss a coin with an unknown probability \\(p\\) of landing on heads. Each time the coin lands on heads we record a 1 and each time it lands on tails a 0. We repeat the experiment 10 times and observe the sequence: ## [1] 0 0 1 0 0 1 0 1 0 0 We can calculate the probability of observing this exact sequence. The probability of observing 1 is \\(p\\) and of observing 0 is \\(1-p\\). Since the observations are independent, the total probability will be the product of each probability and so the probability is \\[ p^3(1-p)^7. \\] We can think of this as a function of \\(p\\) that tells us the probability of observing what we actually observed. For example, if \\(p=0.4\\) the probability is \\(1.79 \\cdot 10^{-3}\\), or if \\(p=0.5\\) the probability is \\(9.77 \\cdot 10^{-4}\\). The principle that we will follow to estimate parameters is that since \\(p=0.4\\) gives a higher probability to our observation than \\(p=0.5\\) we will prefer \\(0.4\\) over \\(0.5\\) as an estimate of \\(p\\). We call this function the likelihood function and denote it \\(L\\). That is \\[ L(p) := p^3(1-p)^7. \\] Figure 1.1: Likelihood of the sample In the picture we see that the likelihood function attains its highest value at \\(p=0.3\\). Using the above principle, we therefore prefer \\(0.3\\) as an estimate of \\(p\\) over any other value. This is the maximum likelihood estimate (MLE) of \\(p\\). Let us define the likelihood function in general. We have a parametric model of our observations, that is we assume that we observe random variables that are independent identically distributed, with a probability function (or density) \\(p_\\theta(x)\\). Here \\(\\theta\\) is an unknown parameter. For a random sample \\(x_1,\\ldots , x_n\\) of size \\(n\\) the likelihood function is \\[ L_n(\\theta) = \\prod_{i=1}^n p_\\theta(x_i). \\] That is, the likelihood is the probability (or density) of our observation, as a function of the unknown parameter. As a second example, say that we observe random variables \\(X_i\\) that are iid \\(\\mathsf N(\\mu,\\sigma^2)\\), where \\(\\mu\\) and \\(\\sigma^2\\) are unknown parameters. In other words, the joint density of an iid sample is \\[ p(x_1,\\ldots, x_n) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2}. \\] The likelihood function is then \\[ L_n(\\mu,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2}. \\] That is, the density and the likelihood function are written using the same formulas, but the density is a function of \\(x_i\\), while the likelihood is a function of the unknown parameters \\(\\mu\\) and \\(\\sigma^2\\). "],["1.2-maximum-likelihood-estimation.html", "1.2 Maximum likelihood estimation", " 1.2 Maximum likelihood estimation In this section we discuss how to find the maximum likelihood estimate. As in the previous section we consider an example: We will assume that time until something happens is exponentially distributed with parameter \\(\\lambda\\). Therefore the probability density of observing \\(T=t\\) is \\[ p_T(t) = \\lambda e^{-\\lambda t}. \\] On the other hand, we can also think of this as the likelihood function of the parameter \\(\\lambda\\), \\[ L(\\lambda) = \\lambda e^{-\\lambda t}. \\] If we have \\(n\\) independent observations, the joint density of these observations is the product of the marginal densities. Therefore the likelihood is also the product of the marginal likelihoods. That is, if we write \\(L_n(\\lambda)\\) for the likelihood of \\(n\\) observations, \\[ L_n(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda t_i}. \\] Many times it is easier to instead consider the logarithm of the likelihood, the log-likelihood, \\[ l_n(\\lambda):=\\ln L_n(\\lambda) = \\sum_{i=1}^n( \\ln \\lambda - \\lambda t_i ) = n\\ln\\lambda - \\lambda\\sum_{i=1}^n t_i= n(\\ln\\lambda - \\lambda \\bar t) , \\] where we as usual let \\(\\bar t = \\frac{1}{n}\\sum_{i=1}^n t_i\\). Let us say we observe a sample of size 100. Figure 1.2: Histogram of the sample We define a function in R that calculates the log-likelihood logLn &lt;- function(lambda, data){ n &lt;- length(data) tbar &lt;- mean(data) n*(log(lambda) - lambda*tbar) } Then we may calculate the log-likelihood of, for example, \\(\\lambda = 0.1\\), logLn(0.1, t) ## [1] -316.1482 Here the variable t is a vector that contains the observations. Let us plot the log-likelihood for a range of \\(\\lambda\\)-values. Figure 1.3: Log likelihood of the sample We can find the maximum likelihood estimate as the \\(\\lambda\\) that maximizes the likelihood (or log likelihood). That is: \\[ \\hat\\lambda = \\operatorname*{argmax}_\\lambda L_n(\\lambda) = \\operatorname*{argmax}_\\lambda l_n(\\lambda). \\] Since the likelihood is simple, we can find \\(\\hat \\lambda\\) directly: \\[ \\partial_\\lambda l_n(\\lambda)\\Big|_{\\lambda = \\hat\\lambda} = n\\left(\\frac{1}{\\hat \\lambda} - \\bar t\\right) = 0. \\] With solution \\(\\hat\\lambda =1/ \\bar t\\). For this sample: lambdaHat &lt;- 1/mean(t) lambdaHat ## [1] 0.1164284 We may also find the estimate using numerical optimization. optimResult &lt;- optimise( logLn, lower = 0.01, upper = 10.0, data = t, maximum = TRUE) optimResult$maximum ## [1] 0.1164178 "],["1.3-hypothesis-testing.html", "1.3 Hypothesis testing", " 1.3 Hypothesis testing In the previous section we saw how to estimate unknown parameters using maximum likelihood. While this is all well and good, we would like to also be able to test hypotheses regarding parameters. Consider Figure 1.4. Figure 1.4: Log likelihood of the sample There we have an estimate of \\(\\theta\\) which is \\(\\hat\\theta = 0.9\\). We would like to test \\(H_0: \\theta=\\theta_0 = 1\\) against \\(\\theta \\neq \\theta_0\\). This should be based on how far away, in some sense, the maximum likelihood estimate is from \\(\\theta_0\\). Looking at the figure, we can see three different ways of measuring how far away \\(\\hat\\theta\\) and \\(\\theta_0\\) are from each other. One way would be to measure the vertical distance between the log-likelihood function in \\(\\hat\\theta\\) and \\(\\theta_0\\). I.e. we would calculate: \\[ l(\\hat\\theta) - l(\\theta_0). \\] This is know as the likelihood ratio test. Another option is to calculate the horizontal distance between \\(\\hat\\theta\\) and \\(\\theta_0\\). I.e. to calculate the distance \\[ |\\hat\\theta - \\theta_0|. \\] This is known as the Wald test. Lastly we know that \\(\\partial_\\theta l(\\theta)|_{\\theta=\\hat\\theta} = 0\\). So we could calculate \\[ |\\partial_\\theta l(\\theta)|_{\\theta=\\theta_0}|, \\] and see how close it is to 0. This is known as the Score test. In the following sections we examine each test in detail. "],["1.4-likelihood-ratio-test.html", "1.4 Likelihood ratio test", " 1.4 Likelihood ratio test We are interested in testing the following hypotheses \\[ H_0:\\theta \\in \\Theta_0\\text{ vs. } H_1:\\theta \\in \\Theta_0^\\complement. \\] Here \\(\\Theta_0\\) is some set of parameter values. It could for example be that \\(\\Theta_0 = (-\\infty,\\theta_0)\\) or simply \\(\\Theta_0 = \\theta_0\\). We define the likelihood ratio as \\[ \\lambda_{\\text{LR}} := 2(l(\\hat\\theta) - l(\\hat\\theta_0)), \\] where \\[ l(\\hat\\theta_0) = \\sup_{\\theta\\in\\Theta_0}l(\\theta) \\] and \\[ l(\\hat\\theta) = \\sup_{\\theta\\in\\Theta}l(\\theta). \\] Since \\(l(\\hat\\theta)\\geq l(\\hat\\theta_0)\\), we have that \\(\\lambda_{\\text{LR}} \\geq 0\\) and data agrees well with \\(H_0\\) if \\(\\lambda_{\\text{LR}}\\) is small. Therefore the rejection region will be of the form \\(\\lambda_{\\text{LR}} &gt; k\\), where \\(k\\) is determined to get the correct size of the test. We can not say more in general, the continuation depends on the particular problem and tends to become complicated for anything but simple models. As before let us say that we have an iid sample from an exponential distribution and wish to test \\(H_0:~\\lambda = \\lambda_0\\) against \\(H_1:~\\lambda \\neq \\lambda_0\\), with \\(\\lambda_0=0.1\\). We have already seen that \\[ l(\\lambda) = n(\\ln \\lambda - \\lambda \\bar t) \\] and that \\(\\hat\\lambda = 1/\\bar t\\). Therefore \\[ l(\\lambda) = n \\left( \\ln\\lambda - \\frac{\\lambda}{\\hat\\lambda} \\right). \\] Now, we can write \\(l(\\hat\\lambda) = n\\left(\\ln\\hat\\lambda - 1\\right)\\). The likelihood ratio is then: \\[ \\lambda_{\\text{LR}} = 2(l(\\hat\\lambda) - l(\\lambda_0)) = 2n\\left( \\ln\\hat\\lambda - 1 - \\ln \\lambda_0 + \\frac{\\lambda_0}{\\hat\\lambda} \\right) = 2n\\left(\\ln \\frac{\\hat\\lambda}{\\lambda_0} + \\frac{\\lambda_0-\\hat\\lambda}{\\hat\\lambda}\\right). \\] Recall that we should reject \\(H_0\\) if \\(\\lambda_{\\text{LR}}&gt;k\\), and that \\(k\\) is set to get the correct size. But to do this we need to know the distribution of \\(\\lambda_{\\text{LR}}\\) and looking at the formula above, this seems difficult. Instead we search for something which is equivalent to \\(\\lambda_{\\text{LR}}&gt;k\\), but with a known distribution. Towards this we plot \\(\\lambda_{LR}(\\hat\\lambda)\\): Figure 1.5: Illustration of the likelihood ratio We see that \\(\\lambda_{LR}\\) is decreasing for \\(\\hat\\lambda &lt; \\lambda_0\\) and increasing for \\(\\hat\\lambda &gt; \\lambda_0\\) with minimum at \\(\\hat\\lambda = \\lambda_0\\). Therefore \\(\\lambda_{LR}&gt;k\\) is equivalent to \\(\\hat\\lambda &lt; k_L\\) or \\(\\hat\\lambda &gt; k_U\\), for some choices of \\(k_L\\) and \\(k_U\\). These should be determined so that the test gets the correct size. The size of the test is \\[\\begin{align} &amp;1- P_{\\lambda_0} \\left(k_L&lt; \\hat\\lambda &lt; k_U \\right) = 1- P_{\\lambda_0} \\left(k_L&lt; \\frac{1}{\\bar t} &lt; k_U \\right) = 1- P_{\\lambda_0} \\left(1/k_U&lt; \\bar t &lt; 1/k_L \\right)\\\\ =&amp; 1- P_{\\lambda_0} \\left(\\tilde k_L&lt; \\sum_{i=1}^n T_i&lt; \\tilde k_U \\right), \\end{align}\\] with \\(\\tilde k_L := n/k_U\\) and similarly for \\(\\tilde k_U\\). This probability can be calculated since we know that \\(\\sum_{i=1}^n T_i \\sim \\Gamma(n,\\lambda)\\). If we let \\(\\Gamma_{\\alpha}(n,\\lambda)\\) be the \\(\\alpha\\)-quantile of the gamma distribution, i.e. the number such that \\[ \\alpha = P\\left( \\Gamma(n,\\lambda )&gt; \\Gamma_\\alpha(n,\\lambda) \\right), \\] we see that the rejection region for a size \\(\\alpha\\) test is \\[ \\left\\{ T_i\\mid \\sum_{i=1}^n T_i &gt; \\Gamma_{\\alpha/2}(n,\\lambda_0) \\text{ or } \\sum_{i=1}^n T_i &lt; \\Gamma_{1-\\alpha/2}(n,\\lambda_0) \\right\\}. \\] or equivalently \\[ \\left\\{ \\hat \\lambda \\mid \\hat\\lambda &lt; \\frac{n}{\\Gamma_{\\alpha/2}(n,\\lambda_0)} \\text{ or } \\hat\\lambda &gt; \\frac{n}{\\Gamma_{1-\\alpha/2}(n,\\lambda_0)} \\right\\}. \\] Let us implement this: alpha &lt;- 0.05 lambda0 &lt;- 0.1 n &lt;- 100 upperCriticalValue &lt;- n / qgamma(alpha/2, shape = n, rate = lambda0) lowerCriticalValue &lt;- n / qgamma(1-alpha/2, shape = n, rate = lambda0) upperCriticalValue ## [1] 0.1229045 lowerCriticalValue ## [1] 0.08296762 In this case we had \\(\\hat\\lambda =\\) 0.1164 and so we would not reject \\(\\lambda \\neq 0.1\\). Another option is to calculate the p-value. Recall that the p-value is the smallest level for which \\(H_0\\) is rejected. That is, it is the \\(\\alpha\\) that solves \\(\\Gamma_{1-\\alpha/2}(n,\\lambda_0) = n/\\hat\\lambda\\). But, by definition \\[ \\alpha = 2 P \\left( \\Gamma(n,\\lambda_0) &lt; \\Gamma_{1-\\alpha/2}) \\right) \\] and therefore the p-value is \\[ 2P\\left( \\Gamma(n,\\lambda_0) &lt; \\frac{n}{\\hat\\lambda} \\right). \\] 2* pgamma(n/lambdaHat, shape = n, rate = lambda0) ## [1] 0.1471398 Again we see that we would not reject \\(H_0\\) on the 5%-level. "],["1.5-mathematical-aside-taylor-expansion.html", "1.5 Mathematical aside: Taylor expansion", " 1.5 Mathematical aside: Taylor expansion In this section we see how to approximate functions by polynomials. This will be useful when we search for the asymptotic distribution of the MLE. We would like to approximate a function \\(f(x)\\) by a polynomial \\(p(x)\\) of degree \\(n\\), around a point \\(x_0\\). That is, if \\(x\\approx x_0\\) we would like \\(f(x)\\approx p(x)\\). How should we choose \\(p(x)\\)? Let us write \\[ p(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)^2 + \\cdots + c_n(x-x_0)^n. \\] To ensure that \\(f(x)\\approx p(x)\\) close to \\(x_0\\), we first require that \\[ f(x_0)=p(x_0)=c_0, \\] so that we have found the first parameter. To make the approximation better, we further require that the first derivatives are the same at \\(x_0\\), \\[ f&#39;(x_0) = p&#39;(x_0) = c_1 + 2c_2(x-x_0) + 3c_3(x-x_0)^2+\\cdots + nc_n(x-x_0)^{n-1}|_{x=x_0} = c_1, \\] and so the second parameter of \\(p(x)\\) has been determined. Continuing, we want \\[ f&#39;&#39;(x_0) = p&#39;&#39;(x_0) = 2c_2 + 2\\cdot 3c_3(x-x_0) + 3\\cdot 4c_4(x-x_0)^2 + \\cdot (n-1)nc_n(x-x_0)^{n-2}|_{x=x_0} = 2c_2, \\] so that \\(c_2 = f&#39;&#39;(x_0)/2\\). For the \\(k\\)th derivative, \\[ f^{(k)}(x_0) = p^{(k)}(x_0) = 2\\cdot 3\\cdots kc_k = k!c_k, \\] so that \\(c_k = f^{(k)}(x_0)/k!\\). To summarize, the order \\(n\\) polynomial approximation of \\(f(x)\\) close to \\(x_0\\) is \\[ f(x) \\approx f(x_0) + f&#39;(x_0)(x-x_0) + \\frac{f&#39;&#39;(x_0)}{2!}(x-x_0)^2 + \\cdots \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n. \\] This is known as a Taylor series. In fact, we can do even better by giving a formula for the error. Taylor’s theorem says that If \\(p(x)\\) is the order \\(n\\) polynomial approximation of \\(f(x)\\) as given above, then the approximation error is \\[ f(x) -p(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}, \\] where \\(\\xi\\) is some number between \\(x\\) and \\(x_0\\). As an example, let us consider the approximation of \\(\\ln x\\) around 1. We begin by calculating the derivatives, \\[\\begin{align} \\ln 1 &amp;= 0 ,\\\\ \\partial_x\\ln x|_{x=1} &amp;= \\frac{1}{x}|_{x=1} = 1 \\\\ \\partial_x^2 \\ln x|_{x=1} &amp;= -\\frac{1}{x^2}|_{x=1} = -1,\\\\ \\partial_x^3 \\ln x|_{x=1} &amp;= \\frac{2}{x^3}|_{x=1} = 2. \\end{align}\\] Therefore the 3rd order polynomial approximation of \\(\\ln x\\) is \\[ \\ln x \\approx (x-1) - \\frac{1}{2}(x-1)^2 + \\frac{1}{3}(x-1)^3. \\] Figure 1.6: Taylor series approximation of ln x "],["1.6-asymptotic-distribution-of-the-mle.html", "1.6 Asymptotic distribution of the MLE", " 1.6 Asymptotic distribution of the MLE Here we examine the asymptotic properties of maximum likelihood estimators. But first let us recall some properties of the normal distribution, that will be used repeatedly in this chapter. If \\(X\\sim \\mathsf N(\\mu,\\sigma^2)\\) then \\(a+bX\\) also has a normal distribution and \\[\\begin{align*} E[a+bX] &amp;= a+bE[X] = a+b\\mu,\\\\ \\operatorname{Var}(a+bX) &amp;= b^2\\operatorname{Var}(X) = b^2\\sigma^2. \\end{align*}\\] That is \\(a+bX\\sim \\mathsf N(a+b\\mu,b^2\\sigma^2)\\). In particular, \\[ \\frac{X-\\mu}{\\sigma} \\sim \\mathsf N(0,1). \\] Now let us turn to the maximum likelhood estimators. To start, we imagine that we observe a random variable \\(X\\), from a parameterized distribution with density \\(p_\\theta\\). Our discussion will also be valid if we have a discrete random variable with a probability function \\(p_\\theta\\). Then we have a log-likelihood \\(l_x(\\theta):=\\ln p_\\theta(x)\\). Here we will calculate the expected value and variance of the random variable \\(l_X&#39;(\\theta):=\\partial_\\theta \\ln p_\\theta(X)\\). First note that since \\[ \\int p_\\theta(x)\\mathrm{d} x = 1, \\] therefore \\[ 0 = \\partial_\\theta \\int p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta (\\ln p_\\theta(x))p_\\theta(x)\\mathrm{d} x = \\int l&#39;_x(\\theta) p_\\theta(x)\\mathrm{d} x = E[l_X&#39;(\\theta)]. \\] This means, \\[ E[l_X&#39;(\\theta)] =0. \\] To find the variance, we instead consider, \\[\\begin{align*} 0 &amp;= \\partial^2_\\theta \\int p_\\theta(x)\\mathrm{d} x = \\int \\partial_\\theta(l&#39;_x(\\theta) p_\\theta(x))\\mathrm{d} x =\\int (l&#39;&#39;_x(\\theta) p_\\theta(x) + (l&#39;_x(\\theta) )^2p_\\theta(x))\\mathrm{d} x \\\\ &amp;= E[l&#39;&#39;_X(\\theta)] + E[(l&#39;_X(\\theta))^2]. \\end{align*}\\] Therefore, \\[ \\operatorname{Var}(l&#39;_X(\\theta)) = E[(l&#39;_X(\\theta))^2] = -E[l&#39;&#39;_X(\\theta)] =: I(\\theta). \\] Here, \\(I(\\theta)\\) is called the Fisher information. This calculation was for a sample \\(X\\) of size 1. If we have an independent sample of size \\(n\\), we define the log-likelihood as \\[ l_n(\\theta) = \\sum_{i=1}^n l_{X_i}(\\theta). \\] Then we can also calculate \\[\\begin{align*} E[l&#39;_n(\\theta)] &amp;= E[\\partial_\\theta \\sum_{i=1}^n l_{X_i}(\\theta)] =\\sum_{i=1}^n E[\\partial_\\theta l_{X_i}(\\theta)] = 0,\\\\ Var(l&#39;_n(\\theta)) &amp;= Var(\\partial_\\theta \\sum_{i=1}^n l_{X_i}(\\theta)) = \\sum_{i=1}^n Var(l_{X_i}&#39;(\\theta)) = n I(\\theta) =:I_n(\\theta). \\end{align*}\\] Let us now recall the law of large numbers and the central limit theorem. They state that if \\(X_1,\\ldots X_n\\) are iid random variables with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), then for large \\(n\\) \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^n X_i&amp; \\overset{asym.}{\\sim} \\mu,\\\\ \\frac{1}{\\sqrt{n}}\\Big(\\sum_{i=1}^n X_i-\\mu\\Big)&amp; \\overset{asym.}{\\sim} \\mathsf N(0,\\sigma^2). \\end{align*}\\] These asymptotic results in practice mean that we approximate the distribution of the left side with the right side if \\(n\\) is large. For example \\[ P\\left(\\frac{1}{\\sqrt{n}}\\Big(\\sum_{i=1}^n X_i-\\mu\\Big) \\leq x \\right) \\approx P\\left(\\sigma Z\\leq x \\right), \\] with \\(Z\\sim \\mathsf N(0,1)\\). Since \\(l&#39;_n(\\theta)\\) is a sum of random variables \\(l_{X_i}&#39;(\\theta)\\), and we now know the expected value and variance, we can apply the law of large numbers and central limit theorem to get the following asymptotics: \\[\\begin{align*} \\frac{1}{n}l&#39;_n(\\theta) &amp;\\overset{asym.}{\\sim} 0, \\\\ \\frac{1}{\\sqrt{n}}l&#39;_n(\\theta) &amp;\\overset{asym.}{\\sim} \\mathsf{N}(0,I(\\theta)), \\\\ -\\frac{1}{n}l&#39;&#39;_n(\\theta) &amp;\\overset{asym.}{\\sim} I(\\theta). \\end{align*}\\] Now we are ready to find the asymptotic distribution of \\(\\hat\\theta_n\\). We make a first-order Taylor expansion of \\(l_n&#39;(\\hat\\theta_n)\\) around \\(\\theta\\), \\[ l&#39;_n(\\hat\\theta_n) \\approx l&#39;_n(\\theta) + (\\hat\\theta_n-\\theta)l&#39;&#39;_n(\\theta). \\] Then use that \\(l&#39;_n(\\hat\\theta_n)=0\\) and rewrite as \\[ \\sqrt{n} (\\hat\\theta_n-\\theta) \\approx -\\frac{l&#39;_n(\\theta)/\\sqrt{n}}{l&#39;&#39;_n(\\theta)/n}. \\] Now, using the above asymptotics we arrive at: \\[ \\sqrt{n} (\\hat\\theta_n-\\theta) \\overset{asym.}{\\sim} \\frac{\\mathsf N(0,I(\\theta))}{I(\\theta)} \\overset{d}{=} \\mathsf N(0,I^{-1}(\\theta)). \\] In particular, this implies that \\(\\hat\\theta_n - \\theta \\overset{asym.}{\\sim } 0\\), or in other words that \\(\\hat\\theta_n\\) is a consistent estimator of \\(\\theta\\). One problem with the above is however that \\(I(\\theta)\\) is often difficult to calculate. However \\(-l&#39;&#39;_n(\\theta)/n=:\\hat I(\\theta)\\) is a consistent estimator of \\(I(\\theta)\\). Then we may also write: \\[ \\sqrt{n \\hat I(\\theta)}(\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] A further problem is that \\(\\theta\\) is in general unknown. But since \\(\\hat\\theta_n\\) is consistent, we may simply replace \\(\\theta\\) by \\(\\hat\\theta_n\\), \\[ \\sqrt{n \\hat I(\\hat\\theta_n)}(\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] This is an amazing result. Without knowing in detail how \\(\\hat\\theta\\) is determined from the sample; perhaps from some numerical optimization, we can say what the large-sample distribution is. We can rewrite this as \\[ \\hat \\theta_n \\overset{asym.}{\\sim} \\mathsf N\\left(\\theta, \\frac{1}{n \\hat I(\\hat\\theta_n)}\\right). \\] Now let us apply this to the example of the exponential distribution. We had that \\(\\hat\\lambda_n = 1/\\bar t\\). Further \\[ l&#39;&#39;(\\lambda) = -\\frac{1}{\\lambda^2}. \\] Thus, the Fisher information is simply \\[ I(\\lambda) = -E[l&#39;&#39;_T(\\lambda)] = \\frac{1}{\\lambda^2}. \\] The asymptotic distribution of \\(\\hat \\lambda_n\\) is therefore: \\[ \\hat\\lambda_n \\approx \\mathsf N\\left(\\lambda, \\frac{\\hat\\lambda_n^2}{n} \\right). \\] "],["1.7-the-delta-method.html", "1.7 The delta method", " 1.7 The delta method In this section we discuss how to find the asymptotic distribution of a function of the estimate. Let us assume that we already know that \\[ \\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} N(0,\\sigma^2). \\] This might be because \\(\\hat\\theta_n\\) is the MLE and we have used the results from the previous section or we have applied some central limit theorem. We have a function \\(f\\) and we would like to know the asymptotic distribution of \\(f(\\hat\\theta_n)\\). Let us again write a Taylor expansion \\[ f(\\hat\\theta_n) \\approx f(\\theta) + f&#39;(\\theta)(\\hat\\theta_n - \\theta). \\] Rearranging and multiplying by \\(\\sqrt n\\) gives, \\[ \\sqrt{n}(f(\\hat\\theta_n) - f(\\theta)) \\approx f&#39;(\\theta)\\sqrt{n}(\\hat\\theta_n - \\theta). \\] The right side is asymptotically normal, by our assumption. We have then arrived at the first order delta method: \\[ \\sqrt{n}(f(\\hat\\theta_n) - f(\\theta)) \\overset{asym.}{\\sim} N(0,\\sigma^2f&#39;(\\theta)^2) . \\] For this to make sense we need that \\(f&#39;(\\theta)\\neq 0\\). If this is not the case we can instead do a second order Taylor expansion \\[ f(\\hat\\theta_n) \\approx f(\\theta) + f&#39;(\\theta)(\\hat\\theta_n - \\theta) + \\frac{f&#39;&#39;(\\theta)}{2}(\\hat\\theta_n - \\theta)^2 = f(\\theta) + \\frac{f&#39;&#39;(\\theta)}{2}(\\hat\\theta_n - \\theta)^2. \\] Rearranging gives, \\[ n(f(\\hat\\theta_n) - f(\\theta)) \\approx \\frac{f&#39;&#39;(\\theta)}{2}(\\sqrt{n}(\\hat\\theta_n-\\theta))^2. \\] We assumed that \\(\\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} N(0,\\sigma^2)\\). The continuous mapping theorem (not covered here) states that if \\(f\\) is a continuous function and if \\(X_n \\overset{asym.}{\\sim} X\\), then \\(f(X_n) \\overset{asym.}{\\sim} f(X)\\). Therefore, if we let \\(Z\\sim N(0,1),\\) we can write our assumption as \\(\\sqrt{n} (\\hat\\theta_n - \\theta) \\overset{asym.}{\\sim} \\sigma Z\\) and thus, recalling that the square of a standard normal random variable has a \\(\\chi^2_1\\)-distribution, \\[ (\\sqrt{n}(\\hat\\theta_n-\\theta))^2 \\overset{asym.}{\\sim} \\sigma^2Z^2 \\overset{d}{=} \\sigma^2 \\chi^2_1. \\] With that we get the second order Delta method: \\[ n(f(\\hat\\theta_n) - f(\\theta)) \\overset{asym.}{\\sim} \\frac{f&#39;&#39;(\\theta)}{2}\\sigma^2\\chi_1^2. \\] Now let us apply the delta method to the exponential distribution. We would like to estimate the probability that the time until the next event is larger than 10. That is the probability \\[ p = P(T&gt;10) = e^{-10\\lambda}. \\] The MLE follows from the invariance principle of maximum likelihood, i.e. \\(\\hat p = e^{-10\\hat\\lambda}\\). The distribution of \\(\\hat p\\) can be found by the delta method if we let \\(p=f(\\lambda) = e^{-10\\lambda}\\). Then \\(f&#39;(\\lambda) = -10e^{-10\\lambda} = -10p\\). In this case \\(f&#39;(\\lambda)\\neq 0\\), so we may apply the first order delta method. Recall from the previous section that \\[ \\sqrt n\\left( \\hat\\lambda_n - \\lambda \\right) \\overset{asym.}{\\sim} \\mathsf N(0, \\lambda^2), \\] that is, the \\(\\sigma^2\\) appearing in the delta method is \\(\\lambda^2\\). Now, applying the delta method gives \\[ \\sqrt n(\\hat p_n- p) \\overset{asym.}{\\sim} \\mathsf N(0,100p^2\\lambda^2). \\] As usual, we may replace unknown parameters with a consistent estimate, i.e. \\(p\\) with \\(\\hat p\\) and \\(\\lambda\\) with \\(\\hat\\lambda\\). Therefore, \\[ \\sqrt n(\\hat p_n- p) \\overset{asym.}{\\sim} \\mathsf N(0,100\\hat p^2\\hat\\lambda^2) = \\mathsf N(0,0.132), \\] or \\[ \\hat p_n \\overset{asym.}{\\sim} \\mathsf N(p,0.132/n). \\] "],["1.8-wilks-test.html", "1.8 Wilks’ test", " 1.8 Wilks’ test In the previous sections we found the asymptotic distribution of \\(\\hat\\theta_n\\). Here we seek the asymptotic distribution of the likelihood ratio \\(-2(l_n(\\theta_0) - l_n(\\hat\\theta_n)))\\). The reason is that, as we have seen, finding the exact distribution is difficult. If we have the approximate, asymptotic, distribution, we can use that to do for example hypothesis testing. For ease of notation we suppress the \\(n\\) and write \\(l\\) and \\(\\hat\\theta\\). We will use the following results, that we have seen before: \\[\\begin{align*} l&#39;(\\hat\\theta) &amp;= 0,\\\\ -\\frac{1}{n}l&#39;&#39;(\\hat\\theta) &amp;\\overset{asym.}{\\sim} -\\frac{1}{n}l&#39;&#39;(\\theta) \\overset{asym.}{\\sim} I(\\theta),\\\\ \\sqrt{n}(\\hat\\theta - \\theta) &amp;\\overset{asym.}{\\sim} I^{-1/2}(\\theta )Z, \\end{align*}\\] with \\(Z\\sim \\mathsf N(0,1)\\). Just as in the delta method, we now do a Taylor expansion of \\(l(\\theta)\\) around \\(\\hat\\theta\\): \\[\\begin{align} l(\\theta) \\approx&amp; l(\\hat \\theta) + l&#39;(\\hat\\theta)(\\theta-\\hat\\theta) + \\frac{l&#39;&#39;(\\hat\\theta)}{2}(\\theta-\\hat\\theta) ^2\\\\ =&amp; l(\\hat\\theta) + \\frac{l&#39;&#39;(\\hat\\theta)}{2}(\\theta-\\hat\\theta) ^2 = l(\\hat \\theta) +\\frac{1}{2} \\frac{1}{n}l&#39;&#39;(\\hat\\theta)n(\\theta-\\hat\\theta) ^2\\\\ \\overset{asym.}{\\sim} &amp; l(\\hat\\theta) -\\frac{1}{2} I(\\theta)I^{-1}(\\theta)Z^2 \\overset{d}{=} l(\\hat\\theta) - \\frac{1}{2}\\chi_1^2. \\end{align}\\] In other words, for a large sample, \\[ \\lambda_{LR} = 2( l_n(\\hat\\theta)-l_n(\\theta_0))\\overset{asym.}{\\sim} \\chi_1^2. \\] Which is Wilks’ theorem. Let us again apply this to the exponential distribution. Of course, we have already found the exact likelihood ratio test, so we would in reality not use an asymptotic test in this case. Nonetheless, we can calculate it as: lrStatistic &lt;- 2*(logLn(optimResult$maximum, t) - logLn(0.1, t)) lrStatistic ## [1] 2.200652 Recall that we reject \\(H_0\\) if \\(\\lambda_{LR}\\) is large. Therefore the p-value is 1 - pchisq(lrStatistic, 1) ## [1] 0.1379523 "],["1.9-walds-test.html", "1.9 Wald’s test", " 1.9 Wald’s test Another way to measure if \\(\\hat\\theta\\) agrees with the null hypothesis is to calculate \\(\\hat\\theta - \\theta_0\\). If this is large, in absolute value, the test should reject the null hypothesis. We have already seen that, under the assumption of \\(H_0\\) \\[ \\frac{\\hat\\theta- \\theta_0}{\\textrm{Sd}(\\hat\\theta)} \\overset{asym.}{\\sim} \\mathsf{N}(0,1), \\] where the standard deviation can be calculated as \\[ Sd(\\hat\\theta) = (n\\hat I(\\hat\\theta))^{-1/2}. \\] Therefore the test can be done by comparing the left hand-side with the appropriate quantile of the Normal distribution. Note that this implies also that \\[ \\frac{(\\hat \\theta - \\theta_0)^2}{Var(\\hat\\theta)} \\overset{asym.}{\\sim} \\chi^2_1, \\] which is similar to Wilks’ test. Let us apply this again to the exponential distribution. We have already seen that \\(I(\\lambda) = 1/\\lambda^2\\) and so the standard deviation is: \\[ Sd(\\hat \\lambda) = \\frac{\\hat\\lambda}{\\sqrt n}. \\] Since we have a two-sided test, the test statistic is: \\[ \\frac{\\left| \\hat\\lambda - \\lambda_0 \\right|}{\\hat\\lambda / \\sqrt n }. \\] waldStatistics &lt;- abs(lambdaHat - lambda0)/(lambdaHat/sqrt(n)) waldStatistics ## [1] 1.41103 This is now compared to \\(z_{\\alpha/2}=\\) 1.96 if \\(\\alpha = 0.05\\) and so we do not reject \\(H_0\\). The p-value is: 2*(1-pnorm(waldStatistics)) ## [1] 0.1582359 Here we found the standard deviation of \\(\\hat \\lambda\\) by knowing the asymptotic distribution of the MLE. It is also possible to calculate this directly from the delta method. That is, we know that \\[\\begin{align} E\\left[ T\\right] &amp;= \\frac{1}{\\lambda},\\\\ Var\\left( T \\right) &amp;= \\frac{1}{\\lambda^2}. \\end{align}\\] So by the central limit theorem (\\(\\bar t\\) is a sum of random variables): \\[ \\sqrt n (\\bar t - 1/\\lambda) \\overset{asym.}{\\sim} N(0,1/\\lambda^2). \\] If \\(f(x)=1/x\\), then \\(\\hat\\lambda = f(\\bar t) = 1/\\bar t\\) and applying the delta method gives, \\[ \\sqrt n (\\hat\\lambda - \\lambda) \\overset{asym.}{\\sim} N(0,f&#39;(\\bar t)^2/\\lambda^2) = N(0,f&#39;(1/\\lambda)^2/\\lambda^2) = N(0,\\lambda^2). \\] This agrees with what we obtained previously. "],["1.10-score-test.html", "1.10 Score test", " 1.10 Score test In this section we discuss the score test, sometimes called the Rao test or the Lagrange multiplier (LM) test. If \\(\\hat\\theta\\) is close to \\(\\theta_0\\) then we should have that \\(l&#39;(\\theta_0)\\approx 0\\). The score test is therefore that we reject \\(H_0\\) if \\(\\left| l&#39;(\\theta_0)\\right|&gt;k\\), for some \\(k\\) chosen depending on the size of the test. We can use the asymptotics we already calculated, that is \\[ \\frac{l_n&#39;(\\theta_0)}{\\sqrt{I_n(\\theta_0)}} \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] So the score test of size \\(\\alpha\\) is to reject \\(H_0\\) if \\[ \\frac{\\left|l_n&#39;(\\theta_0)\\right|}{\\sqrt{I_n(\\theta_0)}}&gt;z_{\\alpha/2}. \\] Note that this test statistic does not require us to calculate the MLE \\(\\hat\\theta\\). By squaring the test statistic we get, as in the previous section, a test statistic that is \\(\\chi^2_1\\)-distributed, similar to Wilks’ test. We apply this to the exponential distribution. We have already calculated everything we need so it is just a matter of putting it together: lp &lt;- n*( 1/lambda0 - mean(t) ) fisherInfo &lt;- n/lambda0^2 scoreStatistic &lt;- lp / sqrt(fisherInfo) scoreStatistic ## [1] 1.41103 With p-value: 2*(1-pnorm(scoreStatistic)) ## [1] 0.1582359 In this particular case the score test and the Wald test are exactly the same. This is not true in general. "],["1.11-confidence-intervals.html", "1.11 Confidence intervals", " 1.11 Confidence intervals We have derived a number of different tests. In principle all of them can be turned into confidence intervals since there is a correspondence between hypothesis tests and confidence intervals. Let us first examine how we can use the Wald test to construct confidence intervals. The Wald test is based on that for large \\(n\\): \\[ \\frac{\\hat\\theta- \\theta}{\\textrm{Sd}(\\hat\\theta)} \\overset{asym.}{\\sim} \\mathsf{N}(0,1), \\] Therefore we can write \\[ 1-\\alpha = P\\left( -z_{\\alpha/2} &lt; \\frac{\\hat\\theta- \\theta}{\\textrm{Sd}(\\hat\\theta)} \\leq z_{\\alpha/2} \\right) = P\\left( \\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\leq \\theta \\leq \\hat\\theta + \\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\right). \\] Which means that \\(\\left[\\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}, \\hat\\theta +\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\right]\\) is a \\(1-\\alpha\\) confidence interval for \\(\\theta\\). Let us derive the same CI in a slightly different way. In the Wald test we would accept \\(H_0: \\theta=\\theta_0\\) if \\(\\left| \\hat\\theta - \\theta_0 \\right|/Sd(\\hat\\theta)&lt;z_{\\alpha/2}\\). Solving this for \\(\\theta_0\\) gives \\[ \\hat\\theta -\\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}\\leq \\theta_0 \\leq \\hat\\theta + \\textrm{Sd}(\\hat\\theta) z_{\\alpha/2}. \\] If we replace \\(\\theta_0\\) with \\(\\theta\\) we obtain the CI above. Therefore we may think of the CI as being the set of \\(\\theta_0\\) that we would accept in a hypothesis test. The same principle can be applied to convert any hypothesis test to a corresponding CI. For example the score test accepts \\(H_0\\) if \\[ \\frac{\\left| l&#39;_n(\\theta_0)\\right |}{\\sqrt{I_n(\\theta_0)}}&lt; z_{\\alpha/2}, \\] and so solving this for \\(\\theta_0\\) gives a CI. However, in most cases it is not possible to obtain a closed form solution and we have solve it numerically. Let us as an example do it for the exponential distribution: \\[\\begin{align} l&#39;_n(\\lambda) &amp;= n\\left( \\frac{1}{\\lambda} - \\frac{1}{\\hat\\lambda} \\right),\\\\ I_n(\\lambda) &amp;= - l&#39;&#39;_n(\\lambda) = \\frac{n}{\\lambda^2}. \\end{align}\\] We plot the score statistic as a function of \\(\\lambda\\): Figure 1.7: Score statistic and confidence intervall To find the CI we need to solve \\[ \\frac{\\left| l&#39;_n(\\lambda)\\right |}{\\sqrt{I_n(\\lambda)}} = z_{\\alpha/2}, \\] in terms of \\(\\lambda\\). Looking at the figure, this has two solutions, one for \\(\\lambda &gt; \\hat\\lambda\\) and one for \\(\\lambda &lt; \\hat\\lambda\\). These will be the left and right endpoints of the CI. alpha = 0.05 scoreStatistic &lt;- function(lambda){ abs( n*(1/lambda - 1/lambdaHat) ) / sqrt( n/lambda^2 ) } f &lt;- function(lambda){ scoreStatistic(lambda) - qnorm(1-alpha/2) } rootResults &lt;- uniroot(f, interval = c(lambdaHat-0.05,lambdaHat)) leftCILimit &lt;- rootResults$root rootResults &lt;- uniroot(f, interval = c(lambdaHat+0.05,lambdaHat)) rightCILimit &lt;- rootResults$root leftCILimit ## [1] 0.09360885 rightCILimit ## [1] 0.1392479 We can compare this to the Wald based CI: alpha = 0.05 z = qnorm(1-alpha/2) leftCILimit &lt;- lambdaHat - qnorm(1-alpha/2)*lambdaHat/sqrt(n) rightCILimit &lt;- lambdaHat + qnorm(1-alpha/2)*lambdaHat/sqrt(n) leftCILimit ## [1] 0.09360885 rightCILimit ## [1] 0.1392479 Again, in this particular example, the two intervals are the same. "],["1.12-an-application.html", "1.12 An application", " 1.12 An application Here we present an application of what we have learned in this chapter. Consider the binary regression model, where we observe random variables \\(Y_i\\) that take on the values 0 or 1. The distribution of \\(Y_i\\) depends on the value of a covariate \\(X_i\\), \\[ P(Y_i=1\\mid X_i=x_i) = s(\\beta x_i), \\] where \\(\\beta\\) is a parameter and \\[ s(x) = \\frac{e^{x}}{1+e^{x}} \\] is the logistic function. We have a sample of size \\(n=\\) 1000 and we would like to do inference on \\(\\beta\\). First we plot our data. Figure 1.8: Observed sample We will estimate \\(\\beta\\) by maximum likelihood, so we begin by writing the likelihood of observation \\(i\\), \\[ L_i(\\beta) = s(\\beta x_i)^{y_i}\\left( 1-s(\\beta x_i)\\right)^{(1-y_i)} . \\] So that the log-likelihood of the \\(i\\)th observation is \\[ l_i(\\beta) = y_i\\ln s(\\beta x_i) + (1-y_i)\\ln ( 1-s(\\beta x_i)). \\] Since we assume that our observations are iid, the total log-likelihood is then \\[ l(\\beta) = \\sum_{i=1}^n l_i(\\beta). \\] Let us implement what we have so far. s &lt;- function(x) { exp(x) / (exp(x) + 1) } logLn &lt;- function(beta, data){ x &lt;- data$x y &lt;- data$y s &lt;- s(x * beta) sum(y * log(s) + (1 - y) * log(1 - s)) } To maximize the likelihood there are now two options. Either we ask the computer to solve \\[ \\underset{\\beta}{\\text{argma}x}~ l(\\beta), \\] or we calculate and solve \\(l&#39;(\\beta)=0\\). For practice we do both ways here. optimResult &lt;- optimize( logLn, lower = 0.0, upper = 3.0, data = data.df, maximum = TRUE ) betahat &lt;- optimResult$maximum betahat ## [1] 1.791579 For the second way, we need \\[\\begin{align} s&#39;(x) &amp;= \\frac{e^x}{\\left(1+e^x\\right)^2},\\\\ l&#39;(\\beta) &amp;= \\frac{x y s&#39;(x \\beta )}{s(x \\beta )}-\\frac{x (1-y) s&#39;(x \\beta )}{1-s(x \\beta )}. \\end{align}\\] In R: sp &lt;- function(x){ exp(x)/(1+exp(x))^2 } logLp &lt;- function(beta, data){ x &lt;- data$x y &lt;- data$y s &lt;- s(x * beta) sp &lt;- sp(x * beta) sum( -x * (1-y) * sp / (1-s) + x * y * sp / s ) } rootResults &lt;- uniroot( logLp, interval = c(0,3), data = data.df ) rootResults$root ## [1] 1.791589 Both methods giving the same result. To confirm that we indeed found the MLE we plot the log-likelihood. Figure 1.9: Log likelihood of the sample Figure 1.10: Observed sample and fitted model Now we turn to hypothesis testing. Let us say we want to test \\(H_0: \\beta = 2\\) against \\(H_1:\\beta \\neq 2\\). First we do the asymptotic likelihood ratio test. So we need to calculate \\(\\lambda_{\\text{LR}}\\): lr &lt;- function(beta0, data){ 2*(logLn(betahat, data) - logLn(beta0, data)) } lr(2.0, data.df) ## [1] 2.97212 If \\(H_0\\) is true, this is an observation of a \\(\\chi_1^2\\)-distributed random variable. Therefore the p-value is 1 - pchisq(lr(2.0, data.df), 1) ## [1] 0.0847108 Next we do a Wald’s test. For this we need an estimate of the standard deviation of the MLE. Perhaps the easiest way is to calculate the Fisher information, that is \\(-l&#39;&#39;(\\beta)\\). Here there are again two options, we can do it numerically or exactly. First we calculate it numerically: observedFisherInfo &lt;- function(beta, data){ drop(-pracma::hessian(logLn, beta, data = data)) } observedFisherInfo(betahat, data.df) ## [1] 72.64594 Calculating the second derivative exactly involves more work but is preferable whenever possible. We get, \\[ l&#39;&#39;(\\beta)=(1-y) \\left(-\\frac{x^2 s &#39;(x \\beta )^2}{(1-s (x \\beta )^2}-\\frac{x^2 s &#39;&#39;(x \\beta )}{1-s (x \\beta )}\\right)+y \\left(-\\frac{x^2 s &#39;(x \\beta )^2}{s (x \\beta )^2}+\\frac{x^2 s &#39;&#39;(x \\beta )}{s (x \\beta )}\\right) \\] Implemented in R: spp &lt;- function(x){ -exp(x)*(exp(x)-1)/(exp(x)+1)^3 } logLpp &lt;- function(beta, data){ x &lt;- data$x y &lt;- data$y s &lt;- s(x*beta) sp &lt;- sp(x*beta) spp &lt;- spp(x*beta) sum( (1-y)*(-x^2*sp^2/(1-s)^2 - x^2*spp/(1-s))+y*(-x^2*sp^2/s^2 + x^2*spp/s) ) } observedFisherInfo &lt;- function(beta, data){ -logLpp(beta, data) } observedFisherInfo(betahat, data.df) ## [1] 72.64591 Recall that Wald’s test statistic is standard normal under \\(H_0\\). So we may calculate the p-value: zWald &lt;- function(beta0, data){ abs(betahat- beta0)*sqrt(observedFisherInfo(betahat, data)) } 2 * ( 1 - pnorm( zWald(2.0, data.df) ) ) ## [1] 0.07566311 We might also do a Score test. Here, all we need is \\(l&#39;\\) and \\(l&#39;&#39;\\), which we have already calculated. The score statistic is again standard normal under \\(H_0\\). zScore &lt;- function(beta0, data){ abs(pracma::grad(logLn, beta0, data = data) / sqrt(observedFisherInfo(beta0, data))) } 2 * ( 1 - pnorm(zScore(2.0, data.df) ) ) ## [1] 0.0754018 Lastly, we might calculate a CI on \\(\\beta\\). Using the Wald’s statistic, this would be: alpha &lt;- 0.05 leftCILimit &lt;- betahat - qnorm(1-alpha/2) / sqrt(observedFisherInfo(betahat, data.df)) rightCILimit &lt;- betahat + qnorm(1-alpha/2) / sqrt(observedFisherInfo(betahat, data.df)) leftCILimit ## [1] 1.561624 rightCILimit ## [1] 2.021534 For a score based CI we first plot the score statistic. Figure 1.11: Observed sample and fitted model We need to find the points where the score statistic is \\(z_{\\alpha/2}\\), which are the limits of the CI. alpha = 0.05 f &lt;- function(beta, data){ zScore(beta, data) - qnorm(1-alpha/2) } rootResults &lt;- uniroot(f, interval = c(betahat-1,betahat), data = data.df) leftCILimit &lt;- rootResults$root rootResults &lt;- uniroot(f, interval = c(betahat+1,betahat), data = data.df) rightCILimit &lt;- rootResults$root leftCILimit ## [1] 1.561916 rightCILimit ## [1] 2.021279 "],["1.13-summary.html", "1.13 Summary", " 1.13 Summary Since there is a lot of new material in this chapter, in this section we summarize what we have learned. First, recall that the following is equivalent \\[\\begin{align} X &amp; \\sim \\mathsf N(\\mu,\\sigma^2),\\\\ \\frac{X-\\mu}{\\sigma} &amp; \\sim \\mathsf N(0,1). \\end{align}\\] Also recall from the B-course that for a large sample \\[ \\bar X \\sim \\mathsf N(\\mu,\\sigma^2/n). \\] The variance is like this because of \\[ Var(\\bar X) = Var\\left( \\frac{1}{n}\\sum_{i=1}^n X_i \\right) = \\frac{1}{n^2}\\sum_{i=1}^n Var(X_i) = \\frac{\\sigma^2}{n}. \\] If we use the above, we get that \\[ \\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathsf N(0,1). \\] This we used to construct what we called the large sample test. That is, we should accept \\(\\mu=\\mu_0\\) as opposed to \\(\\mu\\neq \\mu_0\\) if \\[ \\frac{|\\bar X-\\mu_0|}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}. \\] In this chapter we have seen that this is true in more generallity, whenever we have an MLE. That is \\[ \\hat\\theta \\overset{asym.}{\\sim} \\mathsf N(\\theta,\\sigma^2_{\\hat\\theta}), \\] the same as \\[ \\frac{\\hat\\theta - \\theta}{\\sigma_{\\hat\\theta}} \\overset{asym.}{\\sim} \\mathsf N(0,1). \\] So we can again use this for hypothesis testing or CI, the only remaining challenge is finding \\(\\sigma_{\\hat\\theta}\\). We have a couple of different tools to do this. Let us consider yet another example. We have a sample of size \\(n\\) from \\(\\mathsf{Be}(p)\\). We could also say that we have a sample of size 1 from \\(\\mathsf{Bin}(n,p)\\), the analysis will be the same. But let us stick with \\(\\mathsf{Be}(p)\\). We want to do inference on \\(p\\). The MLE of \\(p\\) is \\(\\hat p = \\bar x\\) and we know that \\[ \\hat p \\overset{asym.}{\\sim} \\mathsf N(p,\\sigma^2_{\\hat p}). \\] Here we can find \\(\\sigma^2_{\\hat p}\\) by direct calculation. \\[ \\sigma^2_{\\hat p} = Var(\\hat p) = Var(\\bar X) = \\frac{1}{n}Var(X_i) = \\frac{p(1-p)}{n}. \\] Therefore, \\[ \\hat p \\overset{asym.}{\\sim} \\mathsf N\\left(p,\\frac{p(1-p)}{n}\\right). \\] If we write this on the form of a test statistic, \\[ \\frac{\\hat p - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\overset{asym.}{\\sim} \\mathsf N\\left(0,1\\right). \\] The problem is that the denominator contains the unkown \\(p\\). Since \\(\\hat p\\) is the MLE, it is consistent, and we may therefore also say that, \\[ \\hat p \\overset{asym.}{\\sim} \\mathsf N\\left(p,\\frac{\\hat p(1-\\hat p)}{n}\\right). \\] Or, \\[ \\frac{\\hat p - p}{\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}} \\overset{asym.}{\\sim} \\mathsf N\\left(0,1\\right), \\] which can be used for constructing the Wald test. What this then means is that, if \\(n\\) is large, and we where to estimate \\(p\\) with \\(\\hat p\\) for many different samples, the distribution of the estimates would be approximately distributed as \\(\\mathsf N(0,1)\\). Let us verify this with a simulation. set.seed(42) n &lt;- 100 p0 &lt;- 0.5 replications &lt;- 1000 p.hats &lt;- array(dim = replications) for (i in seq_len(replications)) { x &lt;- sample(c(0,1), size = n, replace = TRUE) p.hats[i] &lt;- mean(x) } wald &lt;- (p.hats - p0)/sqrt(p.hats*(1-p.hats)/n) Figure 1.12: Simulated density of the Wald statistic and the standard normal density In this case we were lucky that we could calculate \\(Var(\\hat p)\\) directly. Another path is to use the Fisher information. We found that \\[ \\hat \\theta \\overset{asym.}{\\sim} \\mathsf N\\left(\\theta, \\frac{1}{nI(\\theta)}\\right). \\] The Fisher information is \\[ I(\\theta) = -E[l&#39;&#39;(\\theta)]. \\] Note that here \\(I(\\theta)\\) is the Fisher information of a sample of size 1, and so also \\(l(\\theta)\\) is the log-likelihood of a sample of size 1. Here, \\[\\begin{align} L(p) &amp;= p^{x}(1-p)^{1-x},~0\\leq p \\leq 1,\\\\ l(p) &amp;= x\\ln p + (1-x)\\ln (1-p),\\\\ l&#39;(p) &amp;= \\frac{x}{p} - \\frac{1-x}{1-p},\\\\ l&#39;&#39;(p) &amp;= -\\frac{x}{p^2} -\\frac{1-x}{(1-p)^2},\\\\ I(p) &amp;= -E[l&#39;&#39;(p)] = E\\left[ \\frac{X}{p^2} + \\frac{1-X}{(1-p)^2} \\right]\\\\ &amp;= \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} = \\frac{1}{p(1-p)}. \\end{align}\\] So that we recover the same variance as above. Another option is to use a Score test. The score statistic is \\[ \\frac{l_n&#39;(\\theta_0)}{\\sqrt{I_n(\\theta_0)}}. \\] Here \\(l_n(\\theta_0)\\) is the log-likelihood of the sample of size \\(n\\). In our case \\[ l_n&#39;(p) = \\sum_{i=1}^n l_i&#39;(p) = \\sum_{i=1}^n \\left( \\frac{x_i}{p} - \\frac{1-x_i}{1-p} \\right) = \\frac{n\\bar x}{p} - \\frac{n-\\bar x}{1-p} = \\frac{n(\\bar x -p)}{p(1-p)} = \\frac{n(\\hat p -p)}{p(1-p)}. \\] Also, \\(I_n(\\theta) = nI(\\theta)\\) is the Fisher information of the sample of size \\(n\\). For us \\[ I_n(p) = \\frac{n}{p(1-p)}. \\] The score statistic is then \\[ \\frac{l_n&#39;(p_0)}{\\sqrt{I_n(p_0)}} = \\frac{n(\\hat p -p_0)}{p_0(1-p_0)}\\sqrt{\\frac{p_0(1-p_0)}{n}} = \\frac{\\hat p -p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}, \\] which is asymptotically distributed as \\(\\mathsf N(0,1)\\) if \\(p=p_0\\). This is similar to the Wald statistic. The difference is that \\(\\hat p\\) in the denominator is replaced by \\(p_0\\). The final test is the asymptotic likelihood ratio test. It states that, if \\(p=p_0\\), \\[ 2(l_n(\\hat p)-l_n(p_0)) \\overset{asym.}{\\sim} \\chi^2_1. \\] This is perhaps the easiest test to perform, since it is only a matter of evaluating the log-likelihood and comparing the the appropriate quantile of \\(\\chi^2_1\\). "],["1.14-review-questions.html", "1.14 Review questions", " 1.14 Review questions What is the likelihood function? How is the maximum likelihood estimate calculated? How can one use numerical optimisation to calculate the maximum likelihood estimate? What is the likelihood ratio? What is the likelihood ratio test? How is the Taylor expansion calculated? What is the Fisher information? What is the asymptotic distribution of the MLE? What is the delta method? What is the difference between the first and second order delta methods? What is Wilk’s test? What is Wald’s test? What is the score test? How does one construct a confidence interval based on the Wald test? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
