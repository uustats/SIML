<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Bayesian statistics | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Bayesian statistics | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Bayesian statistics | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2023-08-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-bayesian-statistics.html"/>
<link rel="next" href="2.2-choosing-prior.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1.1-the-likelihood-function.html"><a href="1.1-the-likelihood-function.html"><i class="fa fa-check"></i><b>1.1</b> The likelihood function</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-maximum-likelihood-estimation.html"><a href="1.2-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.3" data-path="1.3-hypothesis-testing.html"><a href="1.3-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-likelihood-ratio-test.html"><a href="1.4-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.4</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-mathematical-aside-taylor-expansion.html"><a href="1.5-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.5</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.6" data-path="1.6-asymptotic-distribution-of-the-mle.html"><a href="1.6-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.6</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.7" data-path="1.7-the-delta-method.html"><a href="1.7-the-delta-method.html"><i class="fa fa-check"></i><b>1.7</b> The delta method</a></li>
<li class="chapter" data-level="1.8" data-path="1.8-wilks-test.html"><a href="1.8-wilks-test.html"><i class="fa fa-check"></i><b>1.8</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.9" data-path="1.9-walds-test.html"><a href="1.9-walds-test.html"><i class="fa fa-check"></i><b>1.9</b> Wald’s test</a></li>
<li class="chapter" data-level="1.10" data-path="1.10-score-test.html"><a href="1.10-score-test.html"><i class="fa fa-check"></i><b>1.10</b> Score test</a></li>
<li class="chapter" data-level="1.11" data-path="1.11-confidence-intervals.html"><a href="1.11-confidence-intervals.html"><i class="fa fa-check"></i><b>1.11</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.12" data-path="1.12-an-application.html"><a href="1.12-an-application.html"><i class="fa fa-check"></i><b>1.12</b> An application</a></li>
<li class="chapter" data-level="1.13" data-path="1.13-summary.html"><a href="1.13-summary.html"><i class="fa fa-check"></i><b>1.13</b> Summary</a></li>
<li class="chapter" data-level="1.14" data-path="1.14-review-questions.html"><a href="1.14-review-questions.html"><i class="fa fa-check"></i><b>1.14</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-bayesian-statistics.html"><a href="2-bayesian-statistics.html"><i class="fa fa-check"></i><b>2</b> Bayesian statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2.1-bayesian-statistics-1.html"><a href="2.1-bayesian-statistics-1.html"><i class="fa fa-check"></i><b>2.1</b> Bayesian statistics</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-choosing-prior.html"><a href="2.2-choosing-prior.html"><i class="fa fa-check"></i><b>2.2</b> Choosing prior</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-multiparameter-problems.html"><a href="2.3-multiparameter-problems.html"><i class="fa fa-check"></i><b>2.3</b> Multiparameter problems</a></li>
<li class="chapter" data-level="2.4" data-path="2.4-markov-chain-monte-carlo.html"><a href="2.4-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>2.4</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-an-application-1.html"><a href="2.5-an-application-1.html"><i class="fa fa-check"></i><b>2.5</b> An application</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-summary-1.html"><a href="2.6-summary-1.html"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-review-questions-1.html"><a href="2.7-review-questions-1.html"><i class="fa fa-check"></i><b>2.7</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-bootstrap.html"><a href="3-ch-bootstrap.html"><i class="fa fa-check"></i><b>3</b> Bootstrap</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-parametric-vs-non-parametric.html"><a href="3.1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>3.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="3.2" data-path="3.2-non-parametric-estimation.html"><a href="3.2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>3.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="3.3" data-path="3.3-bootstrap.html"><a href="3.3-bootstrap.html"><i class="fa fa-check"></i><b>3.3</b> Bootstrap</a></li>
<li class="chapter" data-level="3.4" data-path="3.4-parametric-bootstrap.html"><a href="3.4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>3.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.5" data-path="3.5-an-application-2.html"><a href="3.5-an-application-2.html"><i class="fa fa-check"></i><b>3.5</b> An application</a></li>
<li class="chapter" data-level="3.6" data-path="3.6-summary-2.html"><a href="3.6-summary-2.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="3.7-review-questions-2.html"><a href="3.7-review-questions-2.html"><i class="fa fa-check"></i><b>3.7</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-classification.html"><a href="4.1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4.2-support-vector-machines-i.html"><a href="4.2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4.3-hoeffdings-inequality.html"><a href="4.3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4.4-generalization-error.html"><a href="4.4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-vc-dimension.html"><a href="4.5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4.6-support-vector-machines-ii.html"><a href="4.6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4.7-bias-variance-decomposition.html"><a href="4.7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4.8-regression-regularization.html"><a href="4.8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4.9-model-selection.html"><a href="4.9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4.10-an-application-i.html"><a href="4.10-an-application-i.html"><i class="fa fa-check"></i><b>4.10</b> An application I</a></li>
<li class="chapter" data-level="4.11" data-path="4.11-an-application-ii.html"><a href="4.11-an-application-ii.html"><i class="fa fa-check"></i><b>4.11</b> An application II</a></li>
<li class="chapter" data-level="4.12" data-path="4.12-review-questions-3.html"><a href="4.12-review-questions-3.html"><i class="fa fa-check"></i><b>4.12</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity.html"><a href="5-beyond-linearity.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-an-application-i-1.html"><a href="5.1-an-application-i-1.html"><i class="fa fa-check"></i><b>5.1</b> An application I</a></li>
<li class="chapter" data-level="5.2" data-path="5.2-an-application-ii-1.html"><a href="5.2-an-application-ii-1.html"><i class="fa fa-check"></i><b>5.2</b> An application II</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-review-questions-4.html"><a href="5.3-review-questions-4.html"><i class="fa fa-check"></i><b>5.3</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-statistics-1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Bayesian statistics<a href="2.1-bayesian-statistics-1.html#bayesian-statistics-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous section we saw that the difference between the frequentist methods and the Bayesian methods is that the frequentist averages over different samples while the Bayesian averages over different parameter values. The <em>Bayesian method</em> is therefore to assign a prior distribution <span class="math inline">\(p(\theta)\)</span> to the unknown parameter. This distribution reflects our belief about the parameter before we see the data.</p>
<p>We model how data is generated by <span class="math inline">\(p(x\mid \theta)\)</span>. That is, the probability or density of obtaining a particular sample, conditioned on knowing <span class="math inline">\(\theta\)</span>.</p>
We then calculate the <em>posterior distribution</em> of the parameter, given the observations
<span class="math display">\[
p(\theta \mid x) = \frac{p(x\mid \theta)p(\theta)}{p(x)}.
\]</span>
<p>
Here <span class="math inline">\(p(x)\)</span> is the marginal density of the observed data. We can write it formally as
</p>
<span class="math display">\[
p(x) = \int p(x,\theta)d\theta = \int p(x\mid \theta)p(\theta)d\theta.
\]</span>
<p>
However, note that since <span class="math inline">\(x\)</span> is fixed and we are interested in the distribution of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(p(x)\)</span> can be regarded as a constant. Also, <span class="math inline">\(p(x\mid \theta)\)</span> is the likelihood, <span class="math inline">\(L(\theta)\)</span>. We may therefore write
</p>

<div class="note">
<span class="math display">\[
p(\theta\mid x) = c L(\theta)p(\theta) \propto L(\theta)p(\theta).
\]</span>
</div>
<p>That is, the posterior is proportional to the likelihood times the prior. Another way to write this is by applying the logarithm.
<span class="math display">\[
\ln p(\theta\mid x) = l(\theta) + \ln p(\theta) + c
\]</span>
We see that the total information regarding <span class="math inline">\(\theta\)</span>, expressed as the posterior distribution, is a combination of the likelihood obtained from the observations and the knowledge before the observations, expressed in the prior distribution.</p>
<p>Once we have the posterior, we can keep it as it is. Perhaps <span class="math inline">\(\theta\)</span> is one part of a larger model and by using the full distribution we are able to take full account of the uncertainty. But we might also summarize the posterior in the posterior mean
<span class="math display">\[
\bar \theta = \int \theta p(\theta\mid x)d\theta,
\]</span>
or construct a <span class="math inline">\(1-\alpha\)</span> posterior interval That is <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that
<span class="math display">\[
P\left( a\leq \theta \leq b \right) = \int_a^b p(\theta\mid x)d\theta = 1-\alpha.
\]</span>
We might also want to predict a new observation <span class="math inline">\(x^\text{new}\)</span>,
<span class="math display">\[
p(x^\text{new}\mid x) = \int p(x^\text{new}\mid \theta) p(\theta\mid x)d\theta.
\]</span></p>
<p>
Let us examine a simple example: We flip a coin <span class="math inline">\(n\)</span> times and we want to make Bayesian inference regarding the probability of heads, <span class="math inline">\(\theta\)</span>. The first step is to decide on the data generating model. It seems natural to assume that <span class="math inline">\(X_1,\ldots X_n \overset{iid}\sim \mathsf{Bernoulli}(\theta)\)</span>. The second step is to decide on a prior distribution of <span class="math inline">\(\theta\)</span>. Since <span class="math inline">\(\theta\)</span> represents a probability, the prior distribution should be confined to <span class="math inline">\([0,1]\)</span>. A popular choice of prior distribution on probabilities is the beta distribution. It has density,
</p>
<span class="math display">\[
p(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)},\quad 0\leq \theta\leq 1,
\]</span>
<p>
where <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\beta&gt;0\)</span> are parameters. Also, <span class="math inline">\(B(\alpha,\beta)\)</span> is the beta function, defined simply such that the density integrates to one. We plot the density for a few choices of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.
</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betaDist"></span>
<img src="02-bayesian_files/figure-html/betaDist-1.png" alt="Density of the beta distribution" width="80%" />
<p class="caption">
Figure 2.1: Density of the beta distribution
</p>
</div>
<p>We see from the figure that by changing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> the beta prior can reflect different kinds of prior information. For example, <span class="math inline">\(\alpha = \beta = 1\)</span> is a uniform distribution, that is a prior that gives equal probability to any value of <span class="math inline">\(\theta\)</span>. On the other hand, <span class="math inline">\(\alpha =3\)</span>, <span class="math inline">\(\beta = 2\)</span> gives small probability to <span class="math inline">\(\theta\)</span> close to 0 and 1 and puts more probability to <span class="math inline">\(\theta&gt;0.5\)</span> than <span class="math inline">\(\theta &lt; 0.5\)</span>.</p>
<p>Now let us calculate the posterior distribution of <span class="math inline">\(\theta\)</span> after observing <span class="math inline">\(x=(x_1,\ldots, x_n)\)</span>. First we need the likelihood of the observation
<span class="math display">\[
L(\theta) = \prod_{i=1}^n\theta^{x_i}(1-\theta)^{1-x_i} = \theta^{n\bar x}(1-\theta)^{n-n\bar x},
\]</span>
where <span class="math inline">\(n \bar x = \sum_i x_i\)</span>. Then the posterior is
<span class="math display">\[
p(\theta\mid x) \propto L(\theta)p(\theta) \propto \theta^{n\bar x}(1-\theta)^{n-n\bar x} \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{\alpha + n\bar x -1}(1-\theta)^{\beta + n - n\bar x -1},
\]</span>
we recognize this as a <span class="math inline">\(\mathsf{Beta}(\alpha+n\bar x, \beta+n-n\bar x)\)</span> distribution. In this situation, when the prior and posterior happen to be in the same family of distributions, the prior is said to be <em>conjugate</em> with respect to the model.</p>
<p>From here we easily get that the mean of the posterior is
<span class="math display">\[
E\left[ \theta \mid x \right] = \frac{\alpha + n\bar x}{\alpha+\beta + n}.
\]</span>
It can be interesting to note here that as <span class="math inline">\(n\to \infty\)</span>, the above converges to <span class="math inline">\(\bar x\)</span>, i.e. for a large sample, the influence of the prior becomes small and the mean of the posterior is simply the MLE. If we wish to predict a new sample, we would calculate
<span class="math display">\[
p(x^\text{new} = 1 \mid x^n) = \int p(x^\text{new} = 1\mid \theta) p(\theta\mid x^n)d\theta = \int \theta p(\theta \mid x^n)d\theta = E\left[ \theta \mid x^n \right].
\]</span>
To make things more concrete, let us say that we observe <span class="math inline">\(\bar x = 0.3\)</span>. Note how the likelihood, and therefore also the posterior, is a function of data only through <span class="math inline">\(\bar x\)</span>. This is because <span class="math inline">\(\bar x\)</span> is a <em>sufficient statistic</em>. As an illustration, let us also choose <span class="math inline">\(\mathsf{Beta}(5.0, 2.0)\)</span> as the prior. Below we plot the prior, likelihood and posterior for <span class="math inline">\(n=10\)</span> and <span class="math inline">\(n=100\)</span>. Note how the posterior becomes more like the likelihood as the sample size increase.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bernoulliPosterior"></span>
<img src="02-bayesian_files/figure-html/bernoulliPosterior-1.png" alt="Prior, likelihood and posterior when n = 10 (top) and n=100 (bottom)" width="80%" />
<p class="caption">
Figure 2.2: Prior, likelihood and posterior when n = 10 (top) and n=100 (bottom)
</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-bayesian-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2.2-choosing-prior.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
