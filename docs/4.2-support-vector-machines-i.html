<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Support vector machines I | Lecture notes for Statistical Inference and Machine Learning</title>
  <meta name="description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Support vector machines I | Lecture notes for Statistical Inference and Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Support vector machines I | Lecture notes for Statistical Inference and Machine Learning" />
  
  <meta name="twitter:description" content="These are the lecture notes for the course Statistical Inference and Machine Learning at the Department of statistics, Uppsala University." />
  

<meta name="author" content="Patrik Andersson" />


<meta name="date" content="2023-08-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4.1-classification.html"/>
<link rel="next" href="4.3-hoeffdings-inequality.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inference and Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-ch-likelihood.html"><a href="1-ch-likelihood.html"><i class="fa fa-check"></i><b>1</b> Likelihood-based methods</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1.1-the-likelihood-function.html"><a href="1.1-the-likelihood-function.html"><i class="fa fa-check"></i><b>1.1</b> The likelihood function</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-maximum-likelihood-estimation.html"><a href="1.2-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.3" data-path="1.3-hypothesis-testing.html"><a href="1.3-hypothesis-testing.html"><i class="fa fa-check"></i><b>1.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-likelihood-ratio-test.html"><a href="1.4-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>1.4</b> Likelihood ratio test</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-mathematical-aside-taylor-expansion.html"><a href="1.5-mathematical-aside-taylor-expansion.html"><i class="fa fa-check"></i><b>1.5</b> Mathematical aside: Taylor expansion</a></li>
<li class="chapter" data-level="1.6" data-path="1.6-asymptotic-distribution-of-the-mle.html"><a href="1.6-asymptotic-distribution-of-the-mle.html"><i class="fa fa-check"></i><b>1.6</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.7" data-path="1.7-the-delta-method.html"><a href="1.7-the-delta-method.html"><i class="fa fa-check"></i><b>1.7</b> The delta method</a></li>
<li class="chapter" data-level="1.8" data-path="1.8-wilks-test.html"><a href="1.8-wilks-test.html"><i class="fa fa-check"></i><b>1.8</b> Wilks’ test</a></li>
<li class="chapter" data-level="1.9" data-path="1.9-walds-test.html"><a href="1.9-walds-test.html"><i class="fa fa-check"></i><b>1.9</b> Wald’s test</a></li>
<li class="chapter" data-level="1.10" data-path="1.10-score-test.html"><a href="1.10-score-test.html"><i class="fa fa-check"></i><b>1.10</b> Score test</a></li>
<li class="chapter" data-level="1.11" data-path="1.11-confidence-intervals.html"><a href="1.11-confidence-intervals.html"><i class="fa fa-check"></i><b>1.11</b> Confidence intervals</a></li>
<li class="chapter" data-level="1.12" data-path="1.12-an-application.html"><a href="1.12-an-application.html"><i class="fa fa-check"></i><b>1.12</b> An application</a></li>
<li class="chapter" data-level="1.13" data-path="1.13-summary.html"><a href="1.13-summary.html"><i class="fa fa-check"></i><b>1.13</b> Summary</a></li>
<li class="chapter" data-level="1.14" data-path="1.14-review-questions.html"><a href="1.14-review-questions.html"><i class="fa fa-check"></i><b>1.14</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-bayesian-statistics.html"><a href="2-bayesian-statistics.html"><i class="fa fa-check"></i><b>2</b> Bayesian statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2.1-bayesian-statistics-1.html"><a href="2.1-bayesian-statistics-1.html"><i class="fa fa-check"></i><b>2.1</b> Bayesian statistics</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-choosing-prior.html"><a href="2.2-choosing-prior.html"><i class="fa fa-check"></i><b>2.2</b> Choosing prior</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-multiparameter-problems.html"><a href="2.3-multiparameter-problems.html"><i class="fa fa-check"></i><b>2.3</b> Multiparameter problems</a></li>
<li class="chapter" data-level="2.4" data-path="2.4-markov-chain-monte-carlo.html"><a href="2.4-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>2.4</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-an-application-1.html"><a href="2.5-an-application-1.html"><i class="fa fa-check"></i><b>2.5</b> An application</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-summary-1.html"><a href="2.6-summary-1.html"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-review-questions-1.html"><a href="2.7-review-questions-1.html"><i class="fa fa-check"></i><b>2.7</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-bootstrap.html"><a href="3-ch-bootstrap.html"><i class="fa fa-check"></i><b>3</b> Bootstrap</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-parametric-vs-non-parametric.html"><a href="3.1-parametric-vs-non-parametric.html"><i class="fa fa-check"></i><b>3.1</b> Parametric vs non-parametric</a></li>
<li class="chapter" data-level="3.2" data-path="3.2-non-parametric-estimation.html"><a href="3.2-non-parametric-estimation.html"><i class="fa fa-check"></i><b>3.2</b> Non-parametric estimation</a></li>
<li class="chapter" data-level="3.3" data-path="3.3-bootstrap.html"><a href="3.3-bootstrap.html"><i class="fa fa-check"></i><b>3.3</b> Bootstrap</a></li>
<li class="chapter" data-level="3.4" data-path="3.4-parametric-bootstrap.html"><a href="3.4-parametric-bootstrap.html"><i class="fa fa-check"></i><b>3.4</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.5" data-path="3.5-an-application-2.html"><a href="3.5-an-application-2.html"><i class="fa fa-check"></i><b>3.5</b> An application</a></li>
<li class="chapter" data-level="3.6" data-path="3.6-summary-2.html"><a href="3.6-summary-2.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="3.7-review-questions-2.html"><a href="3.7-review-questions-2.html"><i class="fa fa-check"></i><b>3.7</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-statLearn.html"><a href="4-ch-statLearn.html"><i class="fa fa-check"></i><b>4</b> Statistical learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-classification.html"><a href="4.1-classification.html"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="4.2-support-vector-machines-i.html"><a href="4.2-support-vector-machines-i.html"><i class="fa fa-check"></i><b>4.2</b> Support vector machines I</a></li>
<li class="chapter" data-level="4.3" data-path="4.3-hoeffdings-inequality.html"><a href="4.3-hoeffdings-inequality.html"><i class="fa fa-check"></i><b>4.3</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="4.4" data-path="4.4-generalization-error.html"><a href="4.4-generalization-error.html"><i class="fa fa-check"></i><b>4.4</b> Generalization error</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-vc-dimension.html"><a href="4.5-vc-dimension.html"><i class="fa fa-check"></i><b>4.5</b> VC-dimension</a></li>
<li class="chapter" data-level="4.6" data-path="4.6-support-vector-machines-ii.html"><a href="4.6-support-vector-machines-ii.html"><i class="fa fa-check"></i><b>4.6</b> Support vector machines II</a></li>
<li class="chapter" data-level="4.7" data-path="4.7-bias-variance-decomposition.html"><a href="4.7-bias-variance-decomposition.html"><i class="fa fa-check"></i><b>4.7</b> Bias-Variance decomposition</a></li>
<li class="chapter" data-level="4.8" data-path="4.8-regression-regularization.html"><a href="4.8-regression-regularization.html"><i class="fa fa-check"></i><b>4.8</b> Regression regularization</a></li>
<li class="chapter" data-level="4.9" data-path="4.9-model-selection.html"><a href="4.9-model-selection.html"><i class="fa fa-check"></i><b>4.9</b> Model selection</a></li>
<li class="chapter" data-level="4.10" data-path="4.10-an-application-i.html"><a href="4.10-an-application-i.html"><i class="fa fa-check"></i><b>4.10</b> An application I</a></li>
<li class="chapter" data-level="4.11" data-path="4.11-an-application-ii.html"><a href="4.11-an-application-ii.html"><i class="fa fa-check"></i><b>4.11</b> An application II</a></li>
<li class="chapter" data-level="4.12" data-path="4.12-review-questions-3.html"><a href="4.12-review-questions-3.html"><i class="fa fa-check"></i><b>4.12</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-beyond-linearity.html"><a href="5-beyond-linearity.html"><i class="fa fa-check"></i><b>5</b> Beyond linearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-an-application-i-1.html"><a href="5.1-an-application-i-1.html"><i class="fa fa-check"></i><b>5.1</b> An application I</a></li>
<li class="chapter" data-level="5.2" data-path="5.2-an-application-ii-1.html"><a href="5.2-an-application-ii-1.html"><i class="fa fa-check"></i><b>5.2</b> An application II</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-review-questions-4.html"><a href="5.3-review-questions-4.html"><i class="fa fa-check"></i><b>5.3</b> Review questions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for Statistical Inference and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines-i" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Support vector machines I<a href="4.2-support-vector-machines-i.html#support-vector-machines-i" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section will discuss binary classification and in particular <em>support vector machines</em> (SVM). The approach taken here is different from the one in ISL. Our approach is easier to explain, generalizes to other method and perhaps also more modern. The approach in ISL however provides a different intuition and is also relevant when implementing the algorithms.</p>
<p>
<p>We are given training examples <span class="math inline">\((x_i,y_i)\)</span>, where <span class="math inline">\(x_i = (x_i^1,x_i^2,\ldots,x_i^p) \in \mathbb R^p\)</span> and <span class="math inline">\(y_i\in \left\{-1,1\right\}\)</span>. The equation
<span class="math display">\[
f(x):=\beta_0 + x^1\beta + \ldots + x^p\beta_p = 0,
\]</span>
defines a hyperplane (e.g. a line in <span class="math inline">\(\mathbb R^2\)</span>, a plane in <span class="math inline">\(\mathbb R^3\)</span>). We are going to classify as +1 if the point is on one side of the hyperplane, <span class="math inline">\(\beta_0 + x^1\beta + \ldots + x^p\beta_p&gt;0\)</span>, and -1 if it is on the other side, <span class="math inline">\(\beta_0 + x^1\beta + \ldots + x^p\beta_p&lt;0\)</span>. In other words, the classification rule is
<span class="math display">\[
h(x) = \text{sign}(\beta_0 + x^1\beta + \ldots + x^p\beta_p).
\]</span>
The value of <span class="math inline">\(f(x_i)\)</span> tells us how far away from the hyperplane the point is and if <span class="math inline">\(y_if(x_i)&gt;0\)</span> the point is classified correctly. That is, if <span class="math inline">\(y_if(x_i)\)</span> is large and positive, the point <span class="math inline">\(x_i\)</span> is classified correctly and with a safe margin. If <span class="math inline">\(y_if(x_i)\)</span> is large and negative, the point is classified incorrectly and is far away from being correctly classified. We will consider the hinge loss
<span class="math display">\[
l(y,f) = (1-yf)_+ ,
\]</span>
here <span class="math inline">\((\cdot)_+\)</span> indicates the positive part and we will minimize the in-sample error
<span class="math display">\[
\underset{\beta_0,\beta}{\text{minimize}}\quad \frac{1}{n}\sum_{i=1}^n (1-y_if(x_i))_+.
\]</span></p>
<p>As an example, we generate some training data from a mixture of normal distributions.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="4.2-support-vector-machines-i.html#cb114-1" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb114-2"><a href="4.2-support-vector-machines-i.html#cb114-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb114-3"><a href="4.2-support-vector-machines-i.html#cb114-3" tabindex="-1"></a></span>
<span id="cb114-4"><a href="4.2-support-vector-machines-i.html#cb114-4" tabindex="-1"></a>mu.p <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">10</span>,<span class="at">mean =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), <span class="at">sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>))</span>
<span id="cb114-5"><a href="4.2-support-vector-machines-i.html#cb114-5" tabindex="-1"></a>mu.n <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">10</span>,<span class="at">mean =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>))</span>
<span id="cb114-6"><a href="4.2-support-vector-machines-i.html#cb114-6" tabindex="-1"></a></span>
<span id="cb114-7"><a href="4.2-support-vector-machines-i.html#cb114-7" tabindex="-1"></a>n.samples <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb114-8"><a href="4.2-support-vector-machines-i.html#cb114-8" tabindex="-1"></a></span>
<span id="cb114-9"><a href="4.2-support-vector-machines-i.html#cb114-9" tabindex="-1"></a>data.matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">2</span><span class="sc">*</span>n.samples, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb114-10"><a href="4.2-support-vector-machines-i.html#cb114-10" tabindex="-1"></a></span>
<span id="cb114-11"><a href="4.2-support-vector-machines-i.html#cb114-11" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_len</span>(n.samples)) {</span>
<span id="cb114-12"><a href="4.2-support-vector-machines-i.html#cb114-12" tabindex="-1"></a>  mu <span class="ot">=</span> mu.p[<span class="fu">sample</span>(<span class="at">x=</span> <span class="fu">nrow</span>(mu.p), <span class="at">size =</span> <span class="dv">1</span>),]</span>
<span id="cb114-13"><a href="4.2-support-vector-machines-i.html#cb114-13" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mu, <span class="at">sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>)<span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb114-14"><a href="4.2-support-vector-machines-i.html#cb114-14" tabindex="-1"></a></span>
<span id="cb114-15"><a href="4.2-support-vector-machines-i.html#cb114-15" tabindex="-1"></a>  data.matrix[<span class="dv">2</span><span class="sc">*</span>i<span class="dv">-1</span>,] <span class="ot">&lt;-</span> <span class="fu">c</span>(sample, <span class="dv">1</span>)</span>
<span id="cb114-16"><a href="4.2-support-vector-machines-i.html#cb114-16" tabindex="-1"></a></span>
<span id="cb114-17"><a href="4.2-support-vector-machines-i.html#cb114-17" tabindex="-1"></a>  mu <span class="ot">=</span> mu.n[<span class="fu">sample</span>(<span class="at">x=</span> <span class="fu">nrow</span>(mu.n), <span class="at">size =</span> <span class="dv">1</span>),]</span>
<span id="cb114-18"><a href="4.2-support-vector-machines-i.html#cb114-18" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mu, <span class="at">sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>)<span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb114-19"><a href="4.2-support-vector-machines-i.html#cb114-19" tabindex="-1"></a>  data.matrix[<span class="dv">2</span><span class="sc">*</span>i,] <span class="ot">&lt;-</span> <span class="fu">c</span>(sample, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb114-20"><a href="4.2-support-vector-machines-i.html#cb114-20" tabindex="-1"></a>}</span>
<span id="cb114-21"><a href="4.2-support-vector-machines-i.html#cb114-21" tabindex="-1"></a></span>
<span id="cb114-22"><a href="4.2-support-vector-machines-i.html#cb114-22" tabindex="-1"></a>data.df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(data.matrix)</span>
<span id="cb114-23"><a href="4.2-support-vector-machines-i.html#cb114-23" tabindex="-1"></a><span class="fu">colnames</span>(data.df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;y&quot;</span>)</span>
<span id="cb114-24"><a href="4.2-support-vector-machines-i.html#cb114-24" tabindex="-1"></a>data.df<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data.df<span class="sc">$</span>y)</span></code></pre></div>
<p>Then use R to calculate the hyperplane that minimizes the in-sample error.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="4.2-support-vector-machines-i.html#cb115-1" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span>
<span id="cb115-2"><a href="4.2-support-vector-machines-i.html#cb115-2" tabindex="-1"></a></span>
<span id="cb115-3"><a href="4.2-support-vector-machines-i.html#cb115-3" tabindex="-1"></a>svm.model <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> data.df,</span>
<span id="cb115-4"><a href="4.2-support-vector-machines-i.html#cb115-4" tabindex="-1"></a>                  <span class="at">type =</span> <span class="st">&quot;C-svc&quot;</span>,</span>
<span id="cb115-5"><a href="4.2-support-vector-machines-i.html#cb115-5" tabindex="-1"></a>                  <span class="at">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>,</span>
<span id="cb115-6"><a href="4.2-support-vector-machines-i.html#cb115-6" tabindex="-1"></a>                  <span class="at">C =</span> <span class="dv">1000</span>)</span>
<span id="cb115-7"><a href="4.2-support-vector-machines-i.html#cb115-7" tabindex="-1"></a></span>
<span id="cb115-8"><a href="4.2-support-vector-machines-i.html#cb115-8" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="at">length =</span> <span class="dv">500</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="at">length =</span> <span class="dv">500</span>))</span>
<span id="cb115-9"><a href="4.2-support-vector-machines-i.html#cb115-9" tabindex="-1"></a>grid<span class="sc">$</span>predicted <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">predict</span>(svm.model, grid))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SVMlinear"></span>
<img src="04-statisticalLearning_files/figure-html/SVMlinear-1.png" alt="Training data and linear classification with hinge loss" width="80%" />
<p class="caption">
Figure 4.3: Training data and linear classification with hinge loss
</p>
</div>
<p>We can also calculate the in-sample error</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="4.2-support-vector-machines-i.html#cb116-1" tabindex="-1"></a><span class="fu">mean</span>(data.df<span class="sc">$</span>y <span class="sc">!=</span> <span class="fu">predict</span>(svm.model, data.df))</span></code></pre></div>
<pre><code>## [1] 0.285</code></pre>
<p>Not so bad, but let us try to improve it. We select some basis functions, <span class="math inline">\(\varphi_m(x)\)</span>, <span class="math inline">\(m=1,\ldots, M\)</span> and use the same classifier but with input features <span class="math inline">\(\varphi(x) = (\varphi_1(x),\ldots, \varphi_M(x))\)</span>. We can for example choose <span class="math inline">\(\varphi_m\)</span> to be polynomials of increasing order. For order 2, we get the classifier below, an ellipsoid.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="4.2-support-vector-machines-i.html#cb118-1" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span>
<span id="cb118-2"><a href="4.2-support-vector-machines-i.html#cb118-2" tabindex="-1"></a></span>
<span id="cb118-3"><a href="4.2-support-vector-machines-i.html#cb118-3" tabindex="-1"></a>svm.model <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(y<span class="sc">~</span><span class="fu">poly</span>(x1, x2, <span class="at">degree =</span> <span class="dv">2</span>), <span class="at">data =</span> data.df,</span>
<span id="cb118-4"><a href="4.2-support-vector-machines-i.html#cb118-4" tabindex="-1"></a>                  <span class="at">type =</span> <span class="st">&quot;C-svc&quot;</span>,</span>
<span id="cb118-5"><a href="4.2-support-vector-machines-i.html#cb118-5" tabindex="-1"></a>                  <span class="at">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>,</span>
<span id="cb118-6"><a href="4.2-support-vector-machines-i.html#cb118-6" tabindex="-1"></a>                  <span class="at">C =</span> <span class="dv">1000</span>)</span>
<span id="cb118-7"><a href="4.2-support-vector-machines-i.html#cb118-7" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="at">length =</span> <span class="dv">500</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="at">length =</span> <span class="dv">500</span>))</span>
<span id="cb118-8"><a href="4.2-support-vector-machines-i.html#cb118-8" tabindex="-1"></a>grid<span class="sc">$</span>predicted <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">predict</span>(svm.model, grid))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SVMpolynomial"></span>
<img src="04-statisticalLearning_files/figure-html/SVMpolynomial-1.png" alt="Training data and quadratic classification with hinge loss" width="80%" />
<p class="caption">
Figure 4.4: Training data and quadratic classification with hinge loss
</p>
</div>
<p>This time the in-sample error is</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="4.2-support-vector-machines-i.html#cb119-1" tabindex="-1"></a><span class="fu">mean</span>(data.df<span class="sc">$</span>y <span class="sc">!=</span> <span class="fu">fitted</span>(svm.model))</span></code></pre></div>
<pre><code>## [1] 0.275</code></pre>
<p>Better. Let us continue with increasing order polynomials, and calculate the error.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="4.2-support-vector-machines-i.html#cb121-1" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span>
<span id="cb121-2"><a href="4.2-support-vector-machines-i.html#cb121-2" tabindex="-1"></a></span>
<span id="cb121-3"><a href="4.2-support-vector-machines-i.html#cb121-3" tabindex="-1"></a>maxDegree <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb121-4"><a href="4.2-support-vector-machines-i.html#cb121-4" tabindex="-1"></a>error.df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;degree&quot;</span> <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span>maxDegree, <span class="at">inError =</span> <span class="cn">NA</span>, <span class="at">outError =</span> <span class="cn">NA</span>)</span>
<span id="cb121-5"><a href="4.2-support-vector-machines-i.html#cb121-5" tabindex="-1"></a></span>
<span id="cb121-6"><a href="4.2-support-vector-machines-i.html#cb121-6" tabindex="-1"></a>svm.model.list <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="at">length =</span> maxDegree)</span>
<span id="cb121-7"><a href="4.2-support-vector-machines-i.html#cb121-7" tabindex="-1"></a></span>
<span id="cb121-8"><a href="4.2-support-vector-machines-i.html#cb121-8" tabindex="-1"></a><span class="cf">for</span> (degree <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>,maxDegree)) {</span>
<span id="cb121-9"><a href="4.2-support-vector-machines-i.html#cb121-9" tabindex="-1"></a>  svm.model.list[degree] <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x1, x2, <span class="at">degree =</span> degree), <span class="at">data =</span> data.df,</span>
<span id="cb121-10"><a href="4.2-support-vector-machines-i.html#cb121-10" tabindex="-1"></a>                                 <span class="at">type =</span> <span class="st">&quot;C-svc&quot;</span>,</span>
<span id="cb121-11"><a href="4.2-support-vector-machines-i.html#cb121-11" tabindex="-1"></a>                                 <span class="at">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>,</span>
<span id="cb121-12"><a href="4.2-support-vector-machines-i.html#cb121-12" tabindex="-1"></a>                                 <span class="at">C =</span> <span class="dv">1000</span>)</span>
<span id="cb121-13"><a href="4.2-support-vector-machines-i.html#cb121-13" tabindex="-1"></a>  error.df<span class="sc">$</span>inError[degree] <span class="ot">&lt;-</span> <span class="fu">mean</span>(data.df<span class="sc">$</span>y <span class="sc">!=</span> <span class="fu">predict</span>(svm.model.list[[degree]], data.df))</span>
<span id="cb121-14"><a href="4.2-support-vector-machines-i.html#cb121-14" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:errorPlot"></span>
<img src="04-statisticalLearning_files/figure-html/errorPlot-1.png" alt="In sample error vs. degree of polynomial, using hinge loss" width="80%" />
<p class="caption">
Figure 4.5: In sample error vs. degree of polynomial, using hinge loss
</p>
</div>
<p>In-sample error gets smaller as we increase the order of the polynomial. For degree 20, the in-sample error is</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="4.2-support-vector-machines-i.html#cb122-1" tabindex="-1"></a>error.df<span class="sc">$</span>inError[<span class="dv">20</span>]</span></code></pre></div>
<pre><code>## [1] 0.03</code></pre>
The classifier looks complex.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SVMpolynomial20"></span>
<img src="04-statisticalLearning_files/figure-html/SVMpolynomial20-1.png" alt="Training data and degree 20 polynomial classification" width="80%" />
<p class="caption">
Figure 4.6: Training data and degree 20 polynomial classification
</p>
</div>
<p>Since we know the data generating distribution, we can approximate the out-of-sample error for each classifier, by simulation.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="4.2-support-vector-machines-i.html#cb124-1" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb124-2"><a href="4.2-support-vector-machines-i.html#cb124-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb124-3"><a href="4.2-support-vector-machines-i.html#cb124-3" tabindex="-1"></a></span>
<span id="cb124-4"><a href="4.2-support-vector-machines-i.html#cb124-4" tabindex="-1"></a>n.samples <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb124-5"><a href="4.2-support-vector-machines-i.html#cb124-5" tabindex="-1"></a></span>
<span id="cb124-6"><a href="4.2-support-vector-machines-i.html#cb124-6" tabindex="-1"></a>data.matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">2</span><span class="sc">*</span>n.samples, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb124-7"><a href="4.2-support-vector-machines-i.html#cb124-7" tabindex="-1"></a></span>
<span id="cb124-8"><a href="4.2-support-vector-machines-i.html#cb124-8" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_len</span>(n.samples)) {</span>
<span id="cb124-9"><a href="4.2-support-vector-machines-i.html#cb124-9" tabindex="-1"></a>  mu <span class="ot">=</span> mu.p[<span class="fu">sample</span>(<span class="at">x=</span> <span class="fu">nrow</span>(mu.p), <span class="at">size =</span> <span class="dv">1</span>),]</span>
<span id="cb124-10"><a href="4.2-support-vector-machines-i.html#cb124-10" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mu, <span class="at">sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>)<span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb124-11"><a href="4.2-support-vector-machines-i.html#cb124-11" tabindex="-1"></a></span>
<span id="cb124-12"><a href="4.2-support-vector-machines-i.html#cb124-12" tabindex="-1"></a>  data.matrix[<span class="dv">2</span><span class="sc">*</span>i<span class="dv">-1</span>,] <span class="ot">&lt;-</span> <span class="fu">c</span>(sample, <span class="dv">1</span>)</span>
<span id="cb124-13"><a href="4.2-support-vector-machines-i.html#cb124-13" tabindex="-1"></a></span>
<span id="cb124-14"><a href="4.2-support-vector-machines-i.html#cb124-14" tabindex="-1"></a>  mu <span class="ot">=</span> mu.n[<span class="fu">sample</span>(<span class="at">x=</span> <span class="fu">nrow</span>(mu.n), <span class="at">size =</span> <span class="dv">1</span>),]</span>
<span id="cb124-15"><a href="4.2-support-vector-machines-i.html#cb124-15" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mu, <span class="at">sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>)<span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb124-16"><a href="4.2-support-vector-machines-i.html#cb124-16" tabindex="-1"></a>  data.matrix[<span class="dv">2</span><span class="sc">*</span>i,] <span class="ot">&lt;-</span> <span class="fu">c</span>(sample, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb124-17"><a href="4.2-support-vector-machines-i.html#cb124-17" tabindex="-1"></a>}</span>
<span id="cb124-18"><a href="4.2-support-vector-machines-i.html#cb124-18" tabindex="-1"></a></span>
<span id="cb124-19"><a href="4.2-support-vector-machines-i.html#cb124-19" tabindex="-1"></a>data.test.df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(data.matrix)</span>
<span id="cb124-20"><a href="4.2-support-vector-machines-i.html#cb124-20" tabindex="-1"></a><span class="fu">colnames</span>(data.test.df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;y&quot;</span>)</span>
<span id="cb124-21"><a href="4.2-support-vector-machines-i.html#cb124-21" tabindex="-1"></a>data.test.df<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data.test.df<span class="sc">$</span>y)</span>
<span id="cb124-22"><a href="4.2-support-vector-machines-i.html#cb124-22" tabindex="-1"></a></span>
<span id="cb124-23"><a href="4.2-support-vector-machines-i.html#cb124-23" tabindex="-1"></a><span class="cf">for</span> (degree <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>,maxDegree)) {</span>
<span id="cb124-24"><a href="4.2-support-vector-machines-i.html#cb124-24" tabindex="-1"></a>  error.df<span class="sc">$</span>outError[degree] <span class="ot">&lt;-</span> <span class="fu">mean</span>(data.test.df<span class="sc">$</span>y <span class="sc">!=</span> <span class="fu">predict</span>(svm.model.list[[degree]],</span>
<span id="cb124-25"><a href="4.2-support-vector-machines-i.html#cb124-25" tabindex="-1"></a>                                                              data.test.df))</span>
<span id="cb124-26"><a href="4.2-support-vector-machines-i.html#cb124-26" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:inOutSampleError"></span>
<img src="04-statisticalLearning_files/figure-html/inOutSampleError-1.png" alt="Out-of-sample error vs. degree of polynomial." width="80%" />
<p class="caption">
Figure 4.7: Out-of-sample error vs. degree of polynomial.
</p>
</div>
<p>The in-sample error is decreasing as the degree of the polynomial increases, and so also the complexity of the model. When the degree is small, the in-sample error is a close approximation of the out-of-sample error, but as the degree increases the difference increases and for large degrees the in-sample error provides little information about the out-of-sample error. The out-of-sample error is at first decreasing, but unlike the in-sample-error, starts increasing after degree 4.</p>
<p>In the following sections we will investigate the connection between the in-sample and out-of-sample error.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4.1-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4.3-hoeffdings-inequality.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
