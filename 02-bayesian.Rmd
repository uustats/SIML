# Bayesian statistics

In this chapter we introduce a different way of thinking about statistical inference, *Bayesian statistics*. 

The type of statistics that we have seen so far in this course, and also in previous courses, is called *frequentist statistics*. The idea is to analyse the properties of estimators, confidence intervals etc., thinking of the sample as random. For example, we say that an estimator is unbiased if the estimator applied to many different samples give the correct value of the parameter, on average. A possible critique of this is that we typically only have one sample and it is therefore not relevant to rely on the average behavior of an estimator.

Bayesian statistics considers an alternative viewpoint. Here the sample is considered fixed and instead the unknown parameter is considered random. The distribution of the unknown parameter before making any observations is called the *prior distribution*. After making the observations, that distribution is updated, using Bayes' formula, and we call the updated distribution the *posterior distribution*. The posterior distribution can theb be used to construct point estimates of the parameter, intervals etc.

The aim of the chapter is to explain the basics, learn how to do Bayesian inference by hand in simple models and see how one can handle more complicated models with simulation methods.

Readings for this chapter is:

ASI 6.1, 6.2, 6.3, 8.4

<!-- ## Some basic decision theory -->

<!-- In this section we introduce *statistical decision theory*, a different way of thinking about what the task of the statistician is, compared to what we have seen so far. -->

<!-- <p>Consider a statistical model with a parameter $\theta\in\Theta$. Based on data $x$, we are going to make a *decision* $\delta(x)$. For example, the decision might be a point estimate $\delta(x)=\bar x$ or some interval estimator. To evaluate if our decision is good we have a loss function $l(\theta,\delta(x))$. It is the penalty of making decision $\delta(x)$ when the true parameter is $\theta$. If $\theta$ is a real-valued parameter we might take squared loss</p> -->
<!-- $$ -->
<!-- l(\theta,\delta(x)) = (\theta-\delta(x))^2, -->
<!-- $$ -->
<!-- and in the discrete case, a 0-1 loss -->
<!-- $$ -->
<!-- l(\theta,\delta(x)) =\begin{cases} -->
<!-- 0\text{ if } \theta=\delta(x)\\ -->
<!-- 1\text{ if } \theta\neq \delta(x). -->
<!-- \end{cases} -->
<!-- $$ -->
<!-- <p>We would like to choose $\delta$ such that the loss is as small as possible. However, the loss depends on both the data $x$ and the parameter $\theta$ and thus a comparison between different decisions becomes complicated. Instead we calculate the *risk* associated with a particular decision.</p> -->
<!-- ```{block2, type='note'} -->
<!-- The risk is defined as -->
<!-- $$ -->
<!-- R(\theta,\delta) = E_\theta\left[ l(\theta,\delta(X)) \right]. -->
<!-- $$ -->
<!-- ``` -->
<!-- Here the expectation is taken of a random sample $X$, while $\theta$ is held fixed. Still the comparison of different $\delta$ is not trivial. It may be that for $\delta_1$ the risk is lower than the risk of $\delta_2$ for some values of $\theta$, while for some other $\theta$, the risk is higher. As a trivial example, say that we have the squared loss and $\delta(x) \equiv 0$. Then, if the the true $\theta$ is 0, clearly this is a good decision. But if the true $\theta$ is something else, it is a bad decision. -->

<!-- Note that the distinguishing feature of the above risk function is that the expectation is taken over a random sample $X$. This is known as *frequentist statistics*. This makes sense in some situations, for example if you have some software that should work well in most situations, i.e.\ for most data $x$. However in many situations the statistician is given one data set and the task is to make valid inference for that particular data set. In that situation, one might argue that, it does not make sense to consider any other data set. -->

<!-- The Bayesian way is to instead consider $\theta$ as random, with a distribution $p(\theta)$. Even if we consider $\theta$ to have a fixed unknown value it could make sense to consider it having a distribution. For example, if we let $\theta$ be 1 if it rained in Uppsala exactly 10 years ago, and 0 otherwise, clearly this is not random. Either it rained or it did not. However, for us, since we do not know which is true, it seems natural to assign probabilities to each alternative. -->

<!-- <p>The Bayesian statistician would call $p(\theta)$ the *prior distribution* on $\theta$. Then, after observing the data $x$, calculate the *posterior distribution*, using Bayes' formula,</p> -->
<!-- $$ -->
<!-- p(\theta \mid x) = \frac{p(x\mid \theta)p(\theta)}{p(x)}. -->
<!-- $$ -->
<!-- <p>Here $p(x\mid \theta)$ is the model of how data is generated. That is, the probability or density of obtaining a particular sample, conditioned on knowing $\theta$. Also, $p(x)$ is the marginal density of the observed data. We can write it formally as</p> -->
<!-- $$ -->
<!-- p(x) = \int p(x,\theta)d\theta = \int p(x\mid \theta)p(\theta)d\theta. -->
<!-- $$ -->
<!-- ```{block2, type='note'} -->
<!-- The *posterior risk* is then defined as -->
<!-- $$ -->
<!-- \rho(p(\theta),\delta(x)) = E\left[  l(\theta,\delta(x)) \right] := \int l(\theta,\delta(x))p(\theta\mid x)d\theta. -->
<!-- $$ -->
<!-- ``` -->
<!-- Here the random quantity is $\theta$ while the data $x$ is held constant. The *Bayes action* $\delta^\star$ is the decision $\delta$ that minimizes the posterior risk. For example, if the decision is a point estimate of $\theta$ and $l(\theta,\delta) = (\theta - \delta)^2$. Then -->
<!-- \begin{align} -->
<!-- \rho(p(\theta),\delta) &= \int (\theta - \delta)^2 p(\theta\mid x)d\theta\\ -->
<!-- &= \delta^2 - 2\delta \int \theta p(\theta\mid x)d\theta + \int \theta^2p(\theta\mid x)d\theta,\\ -->
<!-- \implies \partial_\delta \rho(p(\theta),\delta) &= 2\delta -2\int \theta p(\theta\mid x)d\theta = 0\\ -->
<!-- \implies \delta^\star &=\int \theta p(\theta\mid x)d\theta. -->
<!-- \end{align} -->
<!-- That is, the Bayes action is to choose the mean of the posterior distribution. -->

<!-- A hybrid between the frequentist and the Bayesian approach is the *average risk* (with respect to $p(\theta)$): -->
<!-- ```{block2, type='note'} -->
<!-- $$ -->
<!-- r(p(\theta),\delta) = \int R(\theta,\delta)p(\theta)d\theta. -->
<!-- $$ -->
<!-- ``` -->
<!-- The *Bayes risk* is the minimum of the average risk, taken over all $\delta$ and the decision $\delta(x)$ that achieves the minimum is called the *Bayes rule*. Since the risk here is the frequentist risk, this is fundamentally a frequentist quantity. However, we can write -->
<!-- \begin{align} -->
<!-- r(p(\theta),\delta) &= \int \int l(\theta,\delta(x))p(x\mid\theta)dxp(\theta)d\theta\\ -->
<!-- & = \int\int l(\theta,\delta(x)) p(\theta\mid x)d\theta p(x)dx\\ -->
<!-- & = \int \rho(p(\theta),\delta(x)) p(x)dx. -->
<!-- \end{align} -->
<!-- This is minimized by minimizing $\rho(p(\theta),\delta(x))$ for each $x$. That is, we can find the Bayes rule by taking the Bayes action for each $x$. -->

## Bayesian statistics

In the previous section we saw that the difference between the frequentist methods and the Bayesian methods is that the frequentist averages over different samples while the Bayesian averages over different parameter values. The *Bayesian method* is therefore to assign a prior distribution $p(\theta)$ to the unknown parameter. This distribution reflects our belief about the parameter before we see the data.

We model how data is generated by $p(x\mid \theta)$. That is, the probability or density of obtaining a particular sample, conditioned on knowing $\theta$.

We then calculate the *posterior distribution* of the parameter, given the observations
$$
p(\theta \mid x) = \frac{p(x\mid \theta)p(\theta)}{p(x)}.
$$
<p>Here $p(x)$ is the marginal density of the observed data. We can write it formally as</p>
$$
p(x) = \int p(x,\theta)d\theta = \int p(x\mid \theta)p(\theta)d\theta.
$$
<p>However, note that since $x$ is fixed and we are interested in the distribution of $\theta$, $p(x)$ can be regarded as a constant. Also, $p(x\mid \theta)$ is the likelihood, $L(\theta)$. We may therefore write</p>
```{block2, type='note'}
$$
p(\theta\mid x) = c L(\theta)p(\theta) \propto L(\theta)p(\theta).
$$
```
That is, the posterior is proportional to the likelihood times the prior. Another way to write this is by applying the logarithm.
$$
\ln p(\theta\mid x) = l(\theta) + \ln p(\theta) + c
$$
We see that the total information regarding $\theta$, expressed as the posterior distribution, is a combination of the likelihood obtained from the observations and the knowledge before the observations, expressed in the prior distribution.

Once we have the posterior, we can keep it as it is. Perhaps $\theta$ is one part of a larger model and by using the full distribution we are able to take full account of the uncertainty. But we might also summarize the posterior in the posterior mean
$$
\bar \theta = \int \theta p(\theta\mid x)d\theta,
$$
or construct a $1-\alpha$ posterior interval That is $a$ and $b$ such that
$$
P\left( a\leq \theta \leq b \right) = \int_a^b p(\theta\mid x)d\theta = 1-\alpha.
$$
We might also want to predict a new observation $x^\text{new}$,
$$
p(x^\text{new}\mid x) = \int p(x^\text{new}\mid \theta) p(\theta\mid x)d\theta.
$$

<p>Let us examine a simple example: We flip a coin $n$ times and we want to make Bayesian inference regarding the probability of heads, $\theta$. The first step is to decide on the data generating model. It seems natural to assume that $X_1,\ldots X_n \overset{iid}\sim \mathsf{Bernoulli}(\theta)$. The second step is to decide on a prior distribution of $\theta$. Since $\theta$ represents a probability, the prior distribution should be confined to $[0,1]$. A popular choice of prior distribution on probabilities is the beta distribution. It has density,</p>
$$
p(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)},\quad 0\leq \theta\leq 1,
$$
<p>where $\alpha>0$ and $\beta>0$ are parameters. Also, $B(\alpha,\beta)$ is the beta function, defined simply such that the density integrates to one. We plot the density for a few choices of $\alpha$ and $\beta$.</p>
```{r betaDist, cache=TRUE, echo = FALSE, message=FALSE, fig.cap='Density of the beta distribution', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggplot2)
library(latex2exp)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(aes(color = "1"), fun = function(x){dbeta(x,0.5,0.5)}, size = 1.5) +
  stat_function(aes(color = "2"), fun = function(x){dbeta(x,1,1)}, size = 1.5) +
  stat_function(aes(color = "3"), fun = function(x){dbeta(x,2,2)}, size = 1.5) +
  stat_function(aes(color = "4"), fun = function(x){dbeta(x,3,2)}, size = 1.5) +
  theme_minimal() +
  xlab("x") + 
  ylab("density") +
  scale_colour_manual(values=cbp1, labels = unname(TeX(c("$\\alpha = 0.5,$ $\\beta = 0.5$",
                                                         "$\\alpha = 1.0,$ $\\beta = 1.0$",
                                                         "$\\alpha = 2.0,$ $\\beta = 2.0$",
                                                         "$\\alpha = 3.0,$ $\\beta = 2.0$"))))
```
We see from the figure that by changing $\alpha$ and $\beta$ the beta prior can reflect different kinds of prior information. For example, $\alpha = \beta = 1$ is a uniform distribution, that is a prior that gives equal probability to any value of $\theta$. On the other hand, $\alpha =3$, $\beta = 2$ gives small probability to $\theta$ close to 0 and 1 and puts more probability to $\theta>0.5$ than $\theta < 0.5$.

Now let us calculate the posterior distribution of $\theta$ after observing $x=(x_1,\ldots, x_n)$. First we need the likelihood of the observation
$$
L(\theta) = \prod_{i=1}^n\theta^{x_i}(1-\theta)^{1-x_i} = \theta^{n\bar x}(1-\theta)^{n-n\bar x},
$$
where $n \bar x = \sum_i x_i$. Then the posterior is
$$
p(\theta\mid x) \propto L(\theta)p(\theta) \propto \theta^{n\bar x}(1-\theta)^{n-n\bar x} \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{\alpha + n\bar x -1}(1-\theta)^{\beta + n - n\bar x -1},
$$
we recognize this as a $\mathsf{Beta}(\alpha+n\bar x, \beta+n-n\bar x)$ distribution. In this situation, when the prior and posterior happen to be in the same family of distributions, the prior is said to be *conjugate* with respect to the model.

From here we easily get that the mean of the posterior is 
$$
E\left[ \theta \mid x \right] = \frac{\alpha + n\bar x}{\alpha+\beta + n}.
$$
It can be interesting to note here that as $n\to \infty$, the above converges to $\bar x$, i.e.\ for a large sample, the influence of the prior becomes small and the mean of the posterior is simply the MLE. If we wish to predict a new sample, we would calculate
$$
p(x^\text{new} = 1 \mid x^n) = \int p(x^\text{new} = 1\mid \theta) p(\theta\mid x^n)d\theta = \int \theta p(\theta \mid x^n)d\theta = E\left[ \theta \mid x^n \right].
$$
To make things more concrete, let us say that we observe $\bar x = 0.3$. Note how the likelihood, and therefore also the posterior, is a function of data only through $\bar x$. This is because $\bar x$ is a *sufficient statistic*. As an illustration, let us also choose $\mathsf{Beta}(5.0, 2.0)$ as the prior. Below we plot the prior, likelihood and posterior for $n=10$ and $n=100$. Note how the posterior becomes more like the likelihood as the sample size increase.

```{r bernoulliPosterior, cache=TRUE, echo = FALSE, warning = FALSE, fig.cap='Prior, likelihood and posterior when n = 10 (top) and n=100 (bottom)', out.width='80%', fig.asp=.75, fig.align='center'}

library(ggplot2)

n1 <- 10
xbar <- 0.3
alpha <- 5.0
beta <- 2.0

prior <- function(theta){ dbeta(theta, alpha, beta) }
posterior10 <- function(theta) {  dbeta(theta, alpha+n1*xbar, beta+n1-n1*xbar) }
likelihood10 <- function(theta){ posterior10(theta) / prior(theta)}

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
p1 <-ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(aes(color = "1"), fun = prior, size = 1.5) +
  stat_function(aes(color = "2"), fun = likelihood10, size = 1.5) +
    stat_function(aes(color = "3"), fun = posterior10, size = 1.5) +
  theme_minimal() +
  xlab(TeX("$\\theta$")) + 
  ylab("") +
  scale_colour_manual(values=cbp1, labels = c("prior", "likelihood","posterior"))

n2 <- 100
posterior100 <- function(theta) {  dbeta(theta, alpha+n2*xbar, beta+n2-n2*xbar) }
likelihood100 <- function(theta){ posterior100(theta) / prior(theta)}

p2 <-ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
  stat_function(aes(color = "1"), fun = prior, size = 1.5) +
  stat_function(aes(color = "2"), fun = likelihood100, size = 1.5) +
    stat_function(aes(color = "3"), fun = posterior100, size = 1.5) +
  theme_minimal() +
  xlab(TeX("$\\theta$")) + 
  ylab("") +
  scale_colour_manual(values=cbp1, labels = c("prior", "likelihood","posterior"))

library(gridExtra)
grid.arrange(p1, p2)
```

## Choosing prior

It is clear that the choice of prior will influence the inference and so the choice of prior distribution becomes important.

If we have a subjective belief about what values the parameter is likely to take we can choose a prior that reflects this belief. For example we might know from experience that coins have a probability close to 0.5 of landing on heads and so we would choose a prior with most of the probability around 0.5.

However if the goal is to make scientific inference, for example convince the government that a particular drug is safe, choosing a subjective prior could face criticism. In such cases an alternative is to choose a *non-informative prior*. One way is to use a *flat prior*, $p(\theta)\propto \text{constant}$. For example in the Bernoulli example choosing $p(\theta)=1$ for $0\leq \theta \leq 1$ seems reasonable.

Consider the model $X\sim \mathsf N(\theta,\sigma^2)$, $\sigma^2$ known. Here the flat prior would be $p(\theta)=c>0$. Note however that $p$ can never be a proper density since
$$
\int_{-\infty}^\infty p(\theta)d\theta = \infty,
$$
for every choice of $c$. Then $p$ is said to be an *improper prior*. Although a little curious, we can still perform the Bayesian inference program. The posterior distribution is
$$
p(\theta \mid x) \propto L(\theta)p(\theta) \propto L(\theta).
$$
So that with this choice the posterior distribution is just the likelihood. Continuing the example, let us say that we have a sample of size $n$ with sample mean $\bar x$. Then the likelihood is
$$
L(\theta) \propto \prod_{i=1}^n e^{-\frac{1}{2\sigma^2}(x_i-\theta)^2} = e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2} \propto e^{-\frac{n}{2\sigma^2}(\theta-\bar x)^2},
$$
which is the density of $\mathsf N(\bar x, \sigma^2/n)$. Therefore the point estimates and interval estimators will be the same as in likelihood based inference.

A problem with flat priors is however that they are only non-informative in a particular parameterisation. Take again the Bernoulli example. If we say that we do not have any information about $\theta$, then it would be reasonable to say that we also do not have any information about some function of $\theta$. Take for example $\Psi = \ln (\theta/(1-\theta))$. If $p(\theta)=1$ for $0\leq\theta\leq 1$, then the density of $\Psi$ is
\begin{align}
p(\psi) & = \partial_\psi P(\Psi \leq \psi) = \partial_\psi P\left( \ln (\theta/(1-\theta)) \leq \psi \right)\\
&= \partial_\psi P\left( \theta \leq \frac{e^\psi}{1+e^\psi} \right) = \partial_\psi \frac{e^\psi}{1+e^\psi} = \frac{e^\psi}{(1+e^\psi)^2},
\end{align}
which is clearly no longer non-informative. We say that flat priors are not transformation invariant.

One way to construct a prior that does not change under a change of parameterisation is Jeffreys' prior:
$$
p(\theta)\propto \sqrt{I(\theta)},
$$
where $I$ is the Fisher information. Recall that in the $\mathsf{Bernoulli}(\theta)$ case $I(\theta)=(\theta(1-\theta))^{-1}$ and so the Jeffreys prior is
$$
p(\theta) \propto \theta^{-1/2}(1-\theta)^{-1/2},
$$
which is a $\mathsf{Beta}(1/2,1/2)$.

## Multiparameter problems

If there is more than one parameter, the previous discussion applies without any major changes. What might complicate things is if we wish to do inference on just one of the parameters. Then we need to calculate the marginal posterior distribution of the parameter, which may not be practical. One remedy is to simply draw samples from the joint posterior distribution and from the samples pick the parameters that are of interest.

Let us consider the example of comparing two binomials. The data generating model is
\begin{align}
X_1 &\sim \mathsf{Bin}(n_1,\theta_1),\\
X_2 &\sim \mathsf{Bin}(n_2,\theta_2).
\end{align}
The parameter that we are interested in is $\tau:=\theta_2-\theta_1$. If we assume a flat prior on $\theta_1$ and $\theta_2$ the posterior becomes,
$$
p(\theta_1,\theta_2\mid x) \propto L(\theta_1,\theta_2) \propto \theta_1^{x_1}(1-\theta_1)^{n_1-x_1}\theta_2^{x_2}(1-\theta_2)^{n_2-x_2}.
$$
Note that $\theta_1\mid x_1\sim \mathsf{Beta}(x_1+1,n_1-x_1+1)$ is independent of $\theta_2\mid x_2\sim \mathsf{Beta}(x_2+1,n_2-x_2+1).$ Therefore we can simulate from $\tau\mid x$ by drawing $\theta_1^\star$ and $\theta_2^\star$ from the respective distribution and setting $\tau^\star = \theta_2^\star - \theta_1^\star$.

```{r multiParameterExample, cache=TRUE, echo = TRUE, message=FALSE, warning = FALSE, fig.cap='Posterior distribution', out.width='80%', fig.asp=.75, fig.align='center'}

library(ggplot2)
library(latex2exp)

n1 <- 10
n2 <- 10
theta1 <- 0.5
theta2 <- 0.3
nPost <- 2000

set.seed(42)

#Generate data
x1 <- rbinom(n = 1, size = n1, prob = theta1)
x2 <- rbinom(n = 1, size = n2, prob = theta2)

#Posterior for theta
theta1.post <- rbeta(n = nPost, shape1 = x1+1, shape2 = n1-x1+1)
theta2.post <- rbeta(n = nPost, shape1 = x2+1, shape2 = n2-x2+1)

tau.post <- theta2.post - theta1.post

ggplot(data.frame(tau.post), aes(x=tau.post)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="black") +
  theme_minimal() +
  xlab(TeX("$\\tau$"))
```

## Markov chain Monte Carlo

So far we have been using that the posterior distribution is proportional to the likelihood times the prior distribution. We were also careful to choose the prior as the conjugate distribution so that we could recognize the posterior distribution. In general we have to calculate the normalizing constant:
$$
\int p(x\mid \theta)p(\theta)d\theta.
$$
Many times, and in particular if $\theta$ is high dimensional, it is not possible to calculate this integral.

In this section we will see how we can, instead of calculating it, generate samples from the posterior distribution. This is the method that is used in modern Bayesian statistics. Here we are only able to scratch the surface. Both in terms of theory, since explaining why the method works would require first learning about Markov chains, and in terms of complexity of the methods, since the methods that are used in practice are usually more advanced version of what we present here.

We assume that we are able to write down the likelihood of our data generating model and the prior on our parameters. Therefore we are also able to write down the posterior distribution up to a multiplying constant.

The method we will discuss is called the *Metropolis-Hastings algorithm* and it is an example of a *Markov chain Monte Carlo* (MCMC) method. Markov chain here means that we will obtain a sequence of samples from the posterior distributions and that the distribution of each sample only depends on the previous sample, and not on samples before that. Monte Carlo means that is is an algorithm that uses on random sampling.

The Metropolis-Hastings algorithm is as follows: Suppose we want to sample from the posterior $p(\theta\mid x).$ Suppose also that this is not possible directly, but that we are able to generate samples from some other distribution $q(\theta^\star \mid \theta).$ Then do the following:

Choose $\theta_0$ arbitrarily. For $i\geq 1$ do:

1. Generate a proposal $\theta^\star\sim q(\theta^\star\mid \theta_{i-1})$
2. Calculate
$$
r = \min\left\{ \frac{p(\theta^\star\mid x)}{p(\theta_{i-1}\mid x)}\frac{q(\theta_{i-1}\mid \theta^\star)}{q(\theta^\star\mid\theta_{i-1})}, 1 \right\}.
$$
3. Set
$$
\theta_i = \begin{cases}
\theta^\star\quad \text{with probability } r\\
\theta_{i-1}\quad  \text{with probability } 1-r.
\end{cases}
$$

Theory then tells us that as $i$ becomes large, the distribution of $\theta_i$ will be approximately that of $p(\theta\mid x).$

Note that here the normalizing constant in $p(\theta\mid x)$ is cancelled since it appears both in the numerator and denominator.

It remains to choose $q(\theta^\star\mid x)$. One common choice is called *random walk Metropolis Hastings* and consists of letting
$$
\theta^\star = \theta_{i-1} + \varepsilon_{i-1},
$$
where $\varepsilon_{i-1}$ is and independent random variable. For example $\varepsilon_{i-1}\overset{iid}{\sim} \mathsf N(0,b^2)$ for some variance $b^2$. In this case $q(\theta_{i-1}\mid \theta^\star)=q(\theta^\star\mid \theta_{i-1})$ and so the acceptance probability becomes
$$
r=\min\left\{ \frac{p(\theta^\star\mid x)}{p(\theta_{i-1}\mid x)} ,1\right\}
$$
Here it is important to choose $b$ such that the correlation between $\theta_i$ and $\theta_{i-1}$ is small. If $b$ is small, almost all proposals will be accepted, but since $\varepsilon_i$ also tends to be small, $\theta_{i+1}$ will be highly correlated with $\theta_{i}$. On the other hand, if $b$ is large, the proposal will rarely be accepted, and $\theta_i=\theta_{i-1}$. As a rule of thumb, $b$ should be set such that about $50\%$ of the proposals are accepted.

Another class of proposal distributions is $q(\theta^\star\mid \theta_{i-1})\equiv q(\theta^\star)$, that is $\theta^\star$ is independent of $\theta_{i-1}$. The acceptance probability is then
$$
r = \min\left\{ \frac{p(\theta^\star\mid x)}{p(\theta_{i-1}\mid x)}\frac{q(\theta_{i-1})}{q(\theta^\star)}, 1 \right\}.
$$
This is known as *independence Metropolis Hastings*. Here, since each time the proposal is accepted $\theta_i$ will change, we aim for as high an acceptance probability as possible.

As an example let us see how one can generate samples from the exponential distribution using Metropolis Hastings. To be clear, this is not the best way to generate exponentially distributed random numbers, it is just an example.

The density is
$$
p(x) = \lambda e^{-\lambda x}\propto e^{-\lambda x}.
$$
Here we know that the normalizing constant is $\lambda$, but the idea of MCMC is that we do not need to know it, so let us proceed as if we did not know it. Also note that here we see $x$ as random, in the typical Bayesian setting we would think of $\lambda$ as random, but is is just a matter of notation.

First we implement the density. We do it on a log-scale. This is because we need to divide two densities in the algorithm. On a log-scale this becomes subtraction, which is numerically more stable.
```{r echo=TRUE}
logDensity <- function(x) {
  lambda <- 1
  
  if (x >= 0) {
    -lambda * x
  }
  else{
    -Inf
  }
}
```
Next implement the random-walk Metropolis-Hastings algorithm with normally distributed steps.
```{r echo=TRUE}
mcmc.iter <- function(x, logDensity, sigma, n.iter){
  #Random walk Metropolis Hastings MCMC
  
  res <- matrix(NA, n.iter+1, length(x)) #Create empty matrix
  res[1,] <- x
  logD <- logDensity(x)
  
  accProb <- 0 #keep track of the proportion of proposals that are accepted
  
  for (i in seq_len(n.iter)){
    #New proposal
    xProp <- x + rnorm(length(x), 0, sigma)
    #Log density of proposal
    logDProp <- logDensity(xProp)
    #Acceptance probability
    r <- min( c(1, exp(logDProp - logD) ) )
    
    if(r>runif(1)){ #Accept with probability r, else keep old x
      x <- xProp
      logD <- logDProp
      accProb <- accProb + 1
    }
    res[i+1,] <- x
  }
  list(sample = res, accProb = accProb/n.iter)
}
```
Now we can run the algorithm. First for 1000 step that we throw away. This is since the distribution of the samples are correct only when the number of steps are large. Then we run it for as many steps as we need samples.
```{r echo=TRUE}
x.init <- 1 #Initial value
nIter <- 100000 #Number of MC steps
set.seed(42)

exp.mcmc <- mcmc.iter(x.init,
                       logDensity,
                       0.2,
                       1000)

exp.mcmc <- mcmc.iter(exp.mcmc$sample[nrow(exp.mcmc$sample),],
                       logDensity,
                       0.2,
                       nIter)
```
We can check for example that the mean and standard deviation of the samples are as expected, i.e. in our case 1.
```{r echo=TRUE}
mean(exp.mcmc$sample)
sd(exp.mcmc$sample)
```
We can plot the histogram of the samples against the density.

```{r mcmcExample, cache=TRUE, echo = FALSE, message=FALSE, warning = FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
data.df <- data.frame(sample = exp.mcmc$sample)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(data = data.df, aes(x = sample )) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",binwidth = 0.05) +
  #geom_density(alpha=.2, fill="black") +
  stat_function(fun = dexp, args = list(rate = 1), color = cbp1[2], size = 1) +
  theme_minimal()+ 
  xlab("x")
```


## An application

Here we consider the Bayesian probit model as an example.

The data generating model is
\begin{align}
Y_i&\sim \mathsf{Bin}(n_i,\pi_i),\\
\pi_i &= \Phi(\beta_0 + \beta_1 z_{i1} + \beta_2 z_{i2} + \beta_3 z_{i3}),
\end{align}
where $z_{ij}$ are indicators and $\Phi$ is the $\mathsf N(0,1)$ distribution function. The data is shown in the table.
```{r bayesProbit, cache=TRUE, echo = FALSE, warning = FALSE}

data.df <- read.csv("data/bayesProbit.dat", header=TRUE)
knitr::kable(
  data.df, booktabs = TRUE,
  caption = 'Data for the Bayesian probit model'
)
```

The likelihood is
$$
L(\beta)\propto \prod_{i=1}^n\Phi(\beta_0 + \beta_1 z_{i1} + \beta_2 z_{i2} + \beta_3 z_{i3})^{y_i}(1-\Phi( \beta_0 + \beta_1 z_{i1} + \beta_2 z_{i2} + \beta_3 z_{i3}))^{n_i-y_i}.
$$
We choose a normal distribution as prior for each $\beta_i$,
$$
\beta_i \overset{iid}\sim \mathsf N(0,\lambda),
$$
with $\lambda=10$. That is, the prior can be written as
$$
p(\beta) \propto \exp\left( -\frac{1}{2\lambda}\sum_{j=0}^3 \beta_j^2 \right).
$$
We will use a random walk Metropolis Hastings algorithm with $\varepsilon\overset{iid}\sim \mathsf N(0,\sigma^2)$. 

Now, we implement this in R. First we load the data.
```{r, cache=TRUE, echo = TRUE, warning = FALSE}
data.df <- read.csv("data/bayesProbit.dat", header=TRUE)
n <- nrow(data.df)
data.df$z0 <- rep(1, n)

col_order <- c('y', 'n','z0','z1','z2','z3')
data.df <- data.df[, col_order]
```
Then implement the likelihood function and prior density. We do this on a log-scale.
```{r, cache=TRUE, echo = TRUE, warning = FALSE}
logL <- function(beta){
  p <- pnorm(
    as.matrix(
      data.df['z0'] * beta[1] + data.df['z1'] * beta[2] + 
      data.df['z2'] * beta[3] + data.df['z3'] * beta[4]
      )
    )
 sum( data.df['y']*log(p) + (data.df['n']-data.df['y'])*log(1-p) )
}

lambda <- 10
logPrior <- function(beta){
    -sum(beta^2)/(2*lambda)
}

logPosterior <- function(beta){ logL(beta) + logPrior(beta)}
```

Now run the Markov chain. First a burn-in of 1000 steps, that we then discard. After that a longer run.
```{r, cache=TRUE, echo = TRUE, warning = FALSE}
beta <- c(0,0,0,0) #Initial value
nIter <- 100000 #Number of MC steps

set.seed(42)

beta.mcmc <- mcmc.iter(beta,
                       logPosterior,
                       0.08,
                       1000)

beta.mcmc <- mcmc.iter(beta.mcmc$sample[nrow(beta.mcmc$sample),],
                       logPosterior,
                       0.08,
                       nIter)
```
Now we do some diagnostics of the simulation. First check that the acceptance probability is reasonable.
```{r, cache=TRUE, echo = TRUE, warning = FALSE}
beta.mcmc$accProb
```
Then we plot the trajectories of the parameters. We want to see that the trajectories appear stationary and that they are not stuck in one state. We only plot the first 1000 steps.
```{r logProbBayesTrajectory, cache=TRUE, echo = FALSE, warning = FALSE, fig.cap='Trajectories of the Markov chain', out.width='80%', fig.asp=.75, fig.align='center'}
beta.df <- data.frame(data = beta.mcmc$sample)
colnames(beta.df) <- c('beta0','beta1','beta2','beta3')

library('ggplot2')
library('gridExtra')
library('latex2exp')

p0 <- ggplot(data = beta.df[1:1000,], aes(x = seq(1,1000), y = beta0 )) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_0$'))
p1 <- ggplot(data = beta.df[1:1000,], aes(x = seq(1,1000), y = beta1 )) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_1$'))
p2 <- ggplot(data = beta.df[1:1000,], aes(x = seq(1,1000), y = beta2 )) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_2$'))
p3 <- ggplot(data = beta.df[1:1000,], aes(x = seq(1,1000), y = beta3 )) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_3$'))
grid.arrange(p0,p1,p2,p3, nrow = 2)
```

Then we calculate the cumulative mean. We want to see that the simulation is long enough so that the law of large numbers have come in to effect.
```{r logProbBayesCumMean, cache=TRUE, echo = FALSE, warning = FALSE, fig.cap='Cumulative mean of the Markov chain', out.width='80%', fig.asp=.75, fig.align='center', message=FALSE}
library('dplyr')
p0 <- ggplot(data = beta.df, aes(x = seq(1,nrow(beta.df)), y = cummean(beta0))) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_0$'))
p1 <- ggplot(data = beta.df, aes(x = seq(1,nrow(beta.df)), y = cummean(beta1))) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_1$'))
p2 <- ggplot(data = beta.df, aes(x = seq(1,nrow(beta.df)), y = cummean(beta2))) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_2$'))
p3 <- ggplot(data = beta.df, aes(x = seq(1,nrow(beta.df)), y = cummean(beta3))) +
  geom_line() +
  theme_minimal() +
  xlab('') +
  ylab(TeX('$\\beta_3$'))
grid.arrange(p0,p1,p2,p3, nrow = 2)
```

Then we plot the posterior distribution of the parameters.
```{r logProbBayesPostDist, cache=TRUE, echo = FALSE, warning = FALSE, fig.cap='Posterior distribution', out.width='80%', fig.asp=.75, fig.align='center'}
p0 <- ggplot(data = beta.df, aes(x = beta0 )) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",binwidth = 0.05) +
  geom_density(alpha=.2, fill="black") +
  theme_minimal() +
  xlab(TeX('$\\beta_0$'))
p1 <- ggplot(data = beta.df, aes(x = beta1 )) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",binwidth = 0.05) +
  geom_density(alpha=.2, fill="black") +  
  theme_minimal() +
  xlab(TeX('$\\beta_1$'))
p2 <- ggplot(data = beta.df, aes(x = beta2 )) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",binwidth = 0.05) +
  geom_density(alpha=.2, fill="black") +  
  theme_minimal() +
  xlab(TeX('$\\beta_2$'))
p3 <- ggplot(data = beta.df, aes(x = beta3 )) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",binwidth = 0.05) +
  geom_density(alpha=.2, fill="black") +  
  theme_minimal() +
  xlab(TeX('$\\beta_3$'))
grid.arrange(p0,p1,p2,p3, nrow = 2)
```

From this we can get point estimates, the mean of the posterior distribution, and credible intervals.
```{r, cache=TRUE, echo = TRUE, warning = FALSE}
#Point estimates
colMeans(beta.mcmc$sample)
#95% CI
apply(beta.mcmc$sample, 2, quantile, probs = c(0.05,0.95))
```
As a final check, let us verify that our estimates makes sense by comparing our data to our predictions.
```{r, cache=TRUE, echo = TRUE, warning = FALSE}
beta.fit <- colMeans(beta.mcmc$sample)
p <- pnorm(as.matrix(data.df[3:6])%*%beta.fit)
y.pred <- data.df$n*p
y.pred
```
This is similar to the data.

## Summary

In this section we summarize the chapter by doing a simple example that can be solved by direct calculation.

In the 2020 season of the Swedish football league *Allsvenskan* there was 488 goals scored in 240 games. Assume that $X_1, \ldots X_{488}$, the number of goals scored in each game, are i.i.d\ and distributed as $\mathsf{Po}(\lambda)$. We wish to do Bayesian inference on the unknown parameter $\lambda$.

First we find the likelihood. Since the probability function of a Poisson distributed random variable is
$$
p(x) = \frac{\lambda^x e^{-\lambda}}{x!},~x = 0,1,2\ldots,
$$
if we denote the number of goals in game $i$ by $x_i$, the total likelihood is
$$
L(\lambda) = \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} \propto \lambda^{\sum x_i}e^{-n\lambda}.
$$

<p>Then we decide on an appropriate prior distribution. Since $\lambda >0$ we should choose a distribution with all probability on the positive real numbers. A convenient choice, it turns out, is the gamma distribution. Let us choose the parameters of the gamma distribution based on our subjective (prior) belief. As prior belief we will say that the mean of $\lambda$ should be 3, and that it is unlikely that the average number of goals is larger than 5. Since $\lambda \sim \mathsf{Gamma}(\alpha,\beta)$ implies that</p>
$$
E\left[ \lambda \right] = \frac{\alpha}{\beta},
$$
<p>we will choose $\beta = \alpha/3$. It remains to choose $\alpha$. To do this we plot the density for different choices of $\alpha$.</p>
```{r gammaDist, cache=TRUE, echo = FALSE, fig.cap='Density of the gamma distribution', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggplot2)
library(latex2exp)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
ggplot(data.frame(x = c(0, 5)), aes(x = x)) +
  stat_function(aes(color = "1"), fun = function(x){dgamma(x,shape = 10.0, rate = 10/3)}, size = 1.5) +
  stat_function(aes(color = "2"), fun = function(x){dgamma(x,shape = 20.0, rate = 20/3)}, size = 1.5) +
  stat_function(aes(color = "3"), fun = function(x){dgamma(x,shape = 30.0, rate = 30/3)}, size = 1.5) +
  stat_function(aes(color = "4"), fun = function(x){dgamma(x,shape = 40.0, rate = 40/3)}, size = 1.5) +
  theme_minimal() +
  xlab(TeX("$\\lambda$")) + 
  ylab("density") +
  scale_colour_manual(values=cbp1, labels = unname(TeX(c("$\\alpha = 10$ ",
                                                         "$\\alpha = 20$",
                                                         "$\\alpha = 30$",
                                                         "$\\alpha = 40$"))))
```

Based on this picture we decide that $\alpha = 20$ fits with our prior beliefs.

Now we shall calculate the posterior distribution. The prior distribution has the density (from a textbook)
$$
p(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta\lambda}.
$$
Therefore the posterior distribution has density
$$
p(\lambda \mid x) \propto L(\lambda)p(\lambda) \propto \lambda^{\sum x_i}e^{-n\lambda} \lambda^{\alpha-1} e^{-\beta\lambda} = \lambda^{\alpha + \sum x_i - 1}e^{-\lambda(\beta + n)}.
$$
<p>That is, the posterior distribution is $\mathsf{Gamma}(\alpha + \sum x_i, \beta + n)$. With our prior we therefore get the posterior $\mathsf{Gamma}(518, 246.7)$, illustrated in the figure.</p>
```{r gammaDistPostSubj, cache=TRUE, echo = FALSE, fig.cap='Density of the posterior distribution when prior is subjective', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggplot2)
library(latex2exp)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
ggplot(data.frame(x = c(1, 3)), aes(x = x)) +
  stat_function(aes(color = "1"), fun = function(x){dgamma(x,shape = 518, rate = 246.7)}, size = 1.5) +
  theme_minimal() +
  xlab("x") + 
  ylab("density") +
  scale_colour_manual(values=cbp1)
```
The mean of the posterior, and our point estimate of $\lambda$ is $518/246.7 \approx 2.1$ and a 95\% credibility interval is given by
```{r}
lower <- qgamma(0.025, shape = 518, 20/3 + 240)
upper <- qgamma(0.975, shape = 518, 20/3 + 240)
c( lower, upper )
```

<p>We could instead use a flat prior $p(\lambda) = c, \quad \lambda>0$. Note that this corresponds to a $\mathsf{Gamma}(1,0)$ distribution. So we may recycle the above calculations which gives the posterior $\mathsf{Gamma}(489,240)$.</p>
```{r gammaDistPostFlat, cache=TRUE, echo = FALSE, fig.cap='Density of the posterior distribution when prior is flat', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggplot2)
library(latex2exp)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
ggplot(data.frame(x = c(1, 3)), aes(x = x)) +
  stat_function(aes(color = "1"), fun = function(x){dgamma(x,shape = 489, rate = 240)}, size = 1.5) +
  theme_minimal() +
  xlab("x") + 
  ylab("density") +
  scale_colour_manual(values=cbp1)
```
Now our point estimate of $\lambda$ is $489/240 \approx 2$ and a 95\% credibility interval is given by
```{r}
lower <- qgamma(0.025, shape = 489, 240)
upper <- qgamma(0.975, shape = 489, 240)
c( lower, upper )
```

We could also use Jeffreys prior which is proportional to the square root of the Fisher information. The Fisher information can be calculated, for example, as
$$
I(\lambda) = Var(l'(\lambda)).
$$
The log-likelihod and its derivative is
\begin{align}
l(\lambda) &= \sum_{i=1}^n\left( -\ln x_i! + x_i\ln \lambda - \lambda \right)\\
l'(\lambda) &= \sum_{i=1}^n\left( \frac{x_i}{\lambda} -1  \right) = n\left( \frac{\bar x}{\lambda} -1 \right).
\end{align}
Which gives
$$
I(\lambda) = Var\left(\frac{n\bar X}{\lambda} \right) = \frac{n^2}{\lambda} Var\left(\bar X\right) = \frac{n^2}{\lambda}\frac{\lambda}{n} = \frac{n}{\lambda}.
$$
Then, finally, Jeffreys prior is $p(\lambda) \propto \lambda^{-1/2}$, corresponding to $\mathsf{Gamma}(1/2,0)$ and then the procedure follows the same steps as above.


## Review questions

1. What is the main difference between frequentist and Bayesian inference?
2. What is the risk associated with a decision?
3. What is the posterior risk?
4. What is the Bayes action?
5. What is the Bayes rule?
6. What is the relationship between the prior, the likelihood and the posterior?
7. What is a point estimate in Bayesian statistics?
8. What is a posterior interval?
9. When is the Beta distribution a good choice as a prior distribution?
10. What is a subjective prior?
11. What is a flat prior?
12. What is a possible critisism of flat priors?
13. What is Jeffreys prior?
14. What is the computational problem that Markov chain Monte Carlo solves?
15. What is the difference between random walk Metropolis Hastings and independence Metropolis Hastings?







