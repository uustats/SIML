# Statistical learning {#ch-statLearn}

In this chapter we will discuss *statistical learning* from a mostly theoretical perspective. That is, we will discuss when and why statistical learning can be expected to work.

We begin by defining what we mean by statistical learning and the different types of statistical learning. Then we will display some examples that motivate our theoretical discussion and after that we will see two different theoretical perspectives on learning.

We begin by defining two spaces:
\begin{align}
\mathcal X &= \text{ Input space}\\
\mathcal Y &= \text{ Output space}.
\end{align}

We assume that there is a distribution on $\mathcal X \times \mathcal Y$, that we call $P_{X,Y}$. This means for example that we can calculate, at least in principle, expectations like,
$$
E\left[ f(X,Y)\right] = \int f(x,y)p_{X,Y}(x,y)dxdy.
$$
There is also a loss function $l:\mathcal Y \times \mathcal Y \mapsto \mathbb R$ that tells how close two points in $\mathcal Y$ are to each other.

<p>The problem in statistical learning is to find a function $h:\mathcal X \mapsto \mathcal Y$ that takes an input $x\in \mathcal X$ and gives and output $y\in\mathcal Y$. For a given $h$ we define the *out-of-sample error* as</p>
```{block2, type='note'}
$$
E_{out}(h) = E_{X,Y}\left[l(h(X),Y)\right].
$$
```
This tells us how the function $h$ performs on average, on random observation from $P_{X,Y}$. We would like to choose the function $h$ so that this error is as small as possible. The fundamental problem of learning is that the distribution of $X,Y$ is unknown and so direct minimization of $E_{out}$ is not possible.

Methods in statistical learning can be partitioned in various ways. The two main ways is in *supervised* and *unsupervised* learning. In supervised learning we are given a set of $n$ examples $(x_1,y_1),\ldots,(x_n,y_n)$ and the task is to predict $y$ for a previously unseen point $x$. In unsupervised learning we are only given $x_1,\ldots, x_n$ and the goal is to describe the associations and patterns among a set of input measures. A third scenario is [*reinforcement learning*](https://en.wikipedia.org/wiki/Reinforcement_learning), that we will not cover in this course.

In supervised learning the two main tasks are *classification* and *regression*. In classification the task is to assign a class to each item. If it is binary classification then, in the terms defined above, $\mathcal Y = \{-1,1\}$. Many times the loss is taken to be the 0-1 loss, 
$$l(y_1,y_2)=1_{y_1\neq y_2}(y_1,y_2):=\begin{cases}
0\text{ if } y_1=y_2,\\
1\text{ otherwise }.
\end{cases}
$$

In regression the task is to predict a real value for each item, $\mathcal Y = \mathbb R$. Many times in regression the squared error is used, $l(y_1,y_2)=(y_1-y_2)^2$.

Readings for this chapter is:

ISLR 2

ISLR 4.1-2

ISLR 5

ISLR 6.1-2

ISLR 9

The sections on Hoeffding's inequality, generalization error and VC-dimension is not in the course literature. So it is enough to read these notes.

## Classification

In this section we discuss the theory of classification, without going in to detail about any particular classification method.

We are given *training data* $(x_i,y_i)\quad i=1,\ldots, n$ that we assume are a random sample from an unknown distribution $P_{X,Y}$. The goal is to find a *classifier*, that is a function $h:\mathcal X \mapsto \mathcal Y= \left\{-1,1\right\}$, based on the training data. There is a *loss function* $l:\mathcal Y\times \mathcal Y \mapsto \mathbb R$ and most commonly this is the 0-1 loss,
$$
l(h(x),y) :=1_{h(x)\neq y}(x,y).
$$
Then the out-of-sample error is
$$
E_{out}(h) = E_{X,Y}\left[ l(h(X),Y) \right] = P(h(X)\neq Y).
$$
That is, it is the probability of classifying a random observation $X$ incorrectly, when using the classifier $h.$

To simplify notation we also define the *conditional class probability*,
$$
\eta(x):=P\left( Y=1\mid X=x \right).
$$
```{block2, type='note'}
In the hypothetical case where we know the distribution of $X,Y$, the $h$ that minimizes this out-of-sample error is called the *Bayes classifier*. We claim that it is
\begin{align*}
h^\star(x) &= \begin{cases}
1 & \text{if } \eta(x)\geq 1/2\\
-1 & \text{if } \eta(x)< 1/2
\end{cases}\\
&= \text{sign} (\eta(x) - 1/2).
\end{align*}
```

That is, we should classify to the class that has the highest probability, conditioned on $X$. The minimal out-of-sample error is called the *Bayes risk*.

Let us prove this. The claim is that for any other classifier $h(x)$, the out-of-sample error is at least as large, that is
$$
E_{out}(h) = P\left( h(X)\neq Y \right) \geq P\left( h^\star(X)\neq Y \right) = E_{out}(h^\star).
$$
Or, equivalently, $P\left( h^\star(X)= Y \right)\geq P\left( h(X) = Y \right)$. First note that if we condition on $X=x$, then either $h(x)=-1$ or $h(x)=1$. Therefore,
\begin{align*}
P\left( h(X) = Y  \mid X=x\right) &= 1_{h(x) = 1}(x)P\left(Y=1 \mid X=x\right) + 1_{h(x) = -1}(x)P\left(Y=-1 \mid X=x\right)\\
&= 1_{ h(x) = 1}(x)\eta(x) + (1-1_{ h(x) = 1}(x))(1-\eta(x))\\
&= 1_{ h(x) = 1}(x)\left(  2\eta(x)-1 \right) +1-\eta(x).
\end{align*}
and the same is true if we replace $h$ by $h^\star$. Then,
$$
P\left( h^\star(X) = Y  \mid X=x\right) -P\left( h = Y  \mid X=x\right)  =  \left(1_{ h^\star(x) = 1}(x) - 1_{ h(x) = 1}(x)\right)\left(  2\eta(x)-1 \right).
$$
Now if $x$ is such that $\eta(x)\geq 1/2$, then
$$
\underbrace{\left(\underbrace{1_{ h^\star(x) = 1}(x)}_{=1} - \underbrace{1_{ h(x) = 1}(x)}_{= 0 \text{ or } 1}\right)}_{\geq 0}\underbrace{\left(  2\eta(x)-1 \right)}_{\geq 0}\geq 0.
$$
On the other hand, if $x$ is such that $\eta(x)< 1/2$, then
$$
\underbrace{\left(\underbrace{1_{ h^\star(x) = 1}(x)}_{=0} - \underbrace{1_{ h(x) = 1}(x)}_{= 0 \text{ or } 1}\right)}_{\leq 0}\underbrace{\left(  2\eta(x)-1 \right)}_{< 0}\geq 0.
$$
In any case, this implies that
$$
P\left( h^\star(X) = Y  \mid X=x\right) -P\left( h = Y  \mid X=x\right) \geq 0.
$$
Since this is true for any $x$, it also holds that
$$
P\left( h^\star(X) = Y  \right) -P\left( h(X) = Y  \right) \geq 0,
$$
as we claimed.

Since the Bayes classifier is not available in practice, one needs to take another approach. One is to estimate the function $\eta(x)$ and then construct a classifier by plugging in to the Bayes classifier. That is,
$$
h(x) = \text{sign} (\hat\eta(x) - 1/2).
$$
Logistic regression can be thought of as an example of this.

Another option is to find a function $h$ that directly minimizes the in-sample error,
$$
E_{in} = \frac{1}{n} \sum_{i=1}^n I(h(x_i)\neq y_i).
$$
It would be done by specifying a class of candidate classifiers $\mathcal H$ from which we pick the best $h$. For example, let us assume that $x=(x^1,x^2,\ldots, x^p)\in \mathbb R^p$ and we might then consider all the *linear classifiers*,
$$
\mathcal H = \left\{ h(x) = \text{sign}(\beta_0 + x^1\beta_1 + \ldots + x^p\beta_p) \mid \beta \in \mathbb R^p, \beta_0\in\mathbb R \right\}.
$$
Note that if $y_i(\beta_0 + x^1\beta_1 + \ldots + x^p\beta_p)>0$, then $y_i$ is classified correctly and if $y_i(\beta_0 + x^1\beta_1 + \ldots + x^p\beta_p)<0$, then $y_i$ is misclassified. We can therefore think of $y_i(\beta_0 + x^1\beta_1 + \ldots + x^p\beta_p)$ as measuring how close $y_i$ is to being classified correctly or incorrectly. In fact, we can write the 0-1 loss as
$$
l(h(x),y) = I_{y(\beta_0 + x^1\beta_1 + \ldots + x^p\beta_p)\leq 0}(x,y).
$$
More generally we can consider a function $f(x)$, a classifier $h(x) = \text{sign}(f(x))$ and a loss function that depends on the *margin* $yf(x)$.

It turns out that $E_{in}$, as defined above, is difficult to use for training. The reason for this can be understood in different ways. Mathematically, the 0-1 loss is non-convex, and non-convex functions are in general difficult to optimize. In terms of classification, consider the picture below. Two points are misclassified, but we can see that by moving the classification boundary, we can find a classifier that only misclassifies one point. However, the training algorithm will try to move the boundary a very small step, and see if that gives an improvement. If we use the 0-1 loss, the in-sample error will be the same as long as the boundary is not moved far enough. Therefore it is better to use a loss function that also measures how far away each point is from being classified correctly/incorrectly.
```{r, warning=FALSE, cache = TRUE, echo=FALSE}
library(mvtnorm)
set.seed(42)

mu.p <- c(0,1)
mu.n <- c(0,-1)

n.samples <- 5

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.df <- data.frame(data.matrix)
colnames(data.df) <- c("x1","x2","y")
data.df$y <- as.factor(data.df$y)
```
```{r, cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results="hide"}

h <- function(x){
  x1 <- x[1]
  x2 <- x[2]

  beta0 <- 0
  beta1 <- 0.5
  beta2 <- 1

  if (beta0+beta1*x1+beta2*x2<0) {
    -1
  }
  else{
    1
  }

}

grid <- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500))
grid$predicted <- as.factor(apply(grid, 1, h))
```
```{r classicificationExample, cache=TRUE, echo = FALSE, fig.cap='Example of linear classification', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message = FALSE}
library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(mapping = aes(x = x1, y = x2)) +
  geom_raster(data = grid, aes(fill = predicted), alpha = 0.2) +
  geom_point(data = data.df, aes(color = y)) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  theme_minimal()
```
Below we discuss two alternative loss functions that produce two much used methods for classification, the *hinge loss* and the *negative log-likelihood*. They are plotted in the picture below.
```{r lossFunctions, cache = TRUE, echo = FALSE, fig.cap='Loss functions for classification', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
library(ggplot2)

ind <- function(x){
  as.numeric(x<0)
}

pos <- function(x){
  as.numeric(x>0)*x
}

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot() +
  geom_function(aes(color = "0-1"), fun = ind, size = 1, n = 1000) +
  geom_function(aes(color = "hinge"), fun = function(x){pos(1-x)}, size = 1) +
  geom_function(aes(color = "logi"), fun = function(x){log2(1+exp(-x))}, size = 1) +
  xlim(-2, 2) +
  scale_colour_manual(name = "",
                      values = c("0-1" = cbp1[2],
                                 "hinge" = cbp1[3],
                                 "logi" = cbp1[4]),
                      labels = c("0-1",
                                 "Hinge",
                                 "Neg. log-likelihood")) +
  labs(x = "yf(x)", y = "loss") +
  theme_minimal()

```
The hinge loss is
$$
l(f(x),y)=(1-yf(x))_+,
$$
where $(\cdot)_+$ indicates the positive part. This function takes care of our complaints about the 0-1 loss function. If a point $x_i$ is correctly classified, and it is far away from being misclassified, so that $y_if(x_i)$ is large and positive, the loss is 0. However if it is close to being misclassified it incurs a loss, even if it is correctly classified. It might however worry some that we are using a different function for training (e.g. the hinge loss) and evaluation (0-1). Let us therefore see what the *population minimizer* of the hinge loss is. The population minimizer for the 0-1 loss is the Bayes classifier, and for the hinge loss,
$$
f^\star_{hinge} := \underset{f}{\text{argmin}}~ E\left[(1-Yf(X))_+ \right].
$$
Let us fix an arbitrary $x$ and then we should find $f(x)$ that minimizes
$$
E\left[(1-Yf(x))_+ \mid X=x\right] = (1-f(x))_+\eta(x) + (1+f(x))_+(1-\eta(x)).
$$
We should always have $-1\leq f(x)\leq 1$, because otherwise we could truncate $f(x)$ to get a smaller loss. So with that assumption,
$$
E\left[(1-Yf(x))_+ \mid X=x\right] = (1-f(x))\eta(x) + (1+f(x))(1-\eta(x)) = 1+(1-2\eta(x))f(x).
$$
Then we realize that depending on the sign of $1-2\eta(x)$ we should choose $f(x)$ to be as large positive or negative as possible, that is
$$
f^\star_{hinge} = \begin{cases}
1&\text{if } \eta\geq 1/2\\
-1&\text{if } \eta< 1/2,
\end{cases}
$$
which is exactly the Bayes classifier.

The other loss function we will discuss is the loss function of logistic regression. In logistic regression we model the conditional probability as
$$
\eta(x) = \frac{1}{1+e^{-f(x)}}.
$$
The likelihood of the observation $(x,y)$ is therefore,
$$
L(x,y) = \begin{cases}
\eta(x)&\text{if } y=1\\
1-\eta(x)&\text{if } y=-1.
\end{cases}
$$
Then the negative log-likelihood (which should be minimized) is,
$$
-\log_2(L(x,y)) = \begin{cases}
\log_2\left(1+e^{-f(x)}\right)&\text{if } y=1\\
\log_2\left(1+e^{f(x)}\right)&\text{if } y=-1
\end{cases}
= \log_2(1+e^{-yf(x)})=:l(f(x),y)
$$
Here we took base 2 logarithm since $\log_2(1+e^0)=1$ and then the loss function is on the same scale as the hinge and 0-1. Also for logistic regression you can show that if you classify according to the class with highest probability, the population minimizer is again the Bayes classifier.


## Support vector machines I

In this section will discuss binary classification and in particular *support vector machines* (SVM). The approach taken here is different from the one in ISL. Our approach is easier to explain, generalizes to other method and perhaps also more modern. The approach in ISL however provides a different intuition and is also relevant when implementing the algorithms.

<p>We are given training examples $(x_i,y_i)$, where $x_i = (x_i^1,x_i^2,\ldots,x_i^p) \in \mathbb R^p$ and $y_i\in \left\{-1,1\right\}$. The equation 
$$
f(x):=\beta_0 + x^1\beta + \ldots + x^p\beta_p = 0,
$$
defines a hyperplane (e.g. a line in $\mathbb R^2$, a plane in $\mathbb R^3$). We are going to classify as +1 if the point is on one side of the hyperplane, $\beta_0 + x^1\beta + \ldots + x^p\beta_p>0$, and -1 if it is on the other side, $\beta_0 + x^1\beta + \ldots + x^p\beta_p<0$. In other words, the classification rule is
$$
h(x) = \text{sign}(\beta_0 + x^1\beta + \ldots + x^p\beta_p).
$$
The value of $f(x_i)$ tells us how far away from the hyperplane the point is and if $y_if(x_i)>0$ the point is classified correctly. That is, if $y_if(x_i)$ is large and positive, the point $x_i$ is classified correctly and with a safe margin. If $y_if(x_i)$ is large and negative, the point is classified incorrectly and is far away from being correctly classified. We will consider the hinge loss
$$
l(y,f) = (1-yf)_+ ,
$$
here $(\cdot)_+$ indicates the positive part and we will minimize the in-sample error
$$
\underset{\beta_0,\beta}{\text{minimize}}\quad \frac{1}{n}\sum_{i=1}^n (1-y_if(x_i))_+.
$$


As an example, we generate some training data from a mixture of normal distributions.
```{r, warning=FALSE, cache = TRUE}
library(mvtnorm)
set.seed(42)

mu.p <- rmvnorm(10,mean = c(1,0), sigma = diag(2))
mu.n <- rmvnorm(10,mean = c(0,1), sigma = diag(2))

n.samples <- 100

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p[sample(x= nrow(mu.p), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n[sample(x= nrow(mu.n), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.df <- data.frame(data.matrix)
colnames(data.df) <- c("x1","x2","y")
data.df$y <- as.factor(data.df$y)
```

Then use R to calculate the hyperplane that minimizes the in-sample error.
```{r, cache = TRUE, echo = TRUE, warning = FALSE, message = FALSE, results="hide"}
library(kernlab)

svm.model <- ksvm(y~x1+x2, data = data.df,
                  type = "C-svc",
                  kernel = "vanilladot",
                  C = 1000)

grid <- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500))
grid$predicted <- as.factor(predict(svm.model, grid))
```

```{r SVMlinear, cache = TRUE, echo = FALSE, fig.cap='Training data and linear classification with hinge loss', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message = FALSE}
library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(mapping = aes(x = x1, y = x2)) +
  geom_raster(data = grid, aes(fill = predicted), alpha = 0.2) +
  geom_point(data = data.df, aes(color = y)) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  theme_minimal()
```
We can also calculate the in-sample error
```{r, cache = TRUE}
mean(data.df$y != predict(svm.model, data.df))
```

Not so bad, but let us try to improve it. We select some basis functions, $\varphi_m(x)$, $m=1,\ldots, M$ and use the same classifier but with input features $\varphi(x) = (\varphi_1(x),\ldots, \varphi_M(x))$. We can for example choose $\varphi_m$ to be polynomials of increasing order. For order 2, we get the classifier below, an ellipsoid.
```{r, results="hide", cache = TRUE}
library(kernlab)

svm.model <- ksvm(y~poly(x1, x2, degree = 2), data = data.df,
                  type = "C-svc",
                  kernel = "vanilladot",
                  C = 1000)
grid <- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500))
grid$predicted <- as.factor(predict(svm.model, grid))
```


```{r SVMpolynomial, cache=TRUE, echo = FALSE, fig.cap='Training data and quadratic classification with hinge loss', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message=FALSE}
library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(mapping = aes(x = x1, y = x2)) +
  geom_raster(data = grid, aes(fill = predicted), alpha = 0.2) +
  geom_point(data = data.df, aes(color = y)) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  theme_minimal()
```
This time the in-sample error is
```{r cache = TRUE}
mean(data.df$y != fitted(svm.model))
```

Better. Let us continue with increasing order polynomials, and calculate the error.
```{r message=FALSE, warning=FALSE, cache = TRUE, results = "hide"}
library(kernlab)

maxDegree <- 20
error.df <- data.frame("degree" = 1:maxDegree, inError = NA, outError = NA)

svm.model.list <- vector(mode = "list", length = maxDegree)

for (degree in seq(1,maxDegree)) {
  svm.model.list[degree] <- ksvm(y ~ poly(x1, x2, degree = degree), data = data.df,
                                 type = "C-svc",
                                 kernel = "vanilladot",
                                 C = 1000)
  error.df$inError[degree] <- mean(data.df$y != predict(svm.model.list[[degree]], data.df))
}
```
```{r errorPlot, cache=TRUE, echo = FALSE, fig.cap='In sample error vs. degree of polynomial, using hinge loss', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message=FALSE}
library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(data = error.df, aes(x = degree, y = inError)) +
  geom_line(color = cbp1[2], size = 1) +
  xlab("Degree") +
  ylab("In sample error") +
  theme_minimal()
```

In-sample error gets smaller as we increase the order of the polynomial. For degree 20, the in-sample error is
```{r, cache = TRUE}
error.df$inError[20]
```
The classifier looks complex.
```{r SVMpolynomial20, cache = TRUE, echo = FALSE, fig.cap='Training data and degree 20 polynomial classification', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message=FALSE, results="hide"}

grid <- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500))
grid$predicted <- as.factor(predict(svm.model.list[[20]], grid))

library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(mapping = aes(x = x1, y = x2)) +
  geom_raster(data = grid, aes(fill = predicted), alpha = 0.2) +
  geom_point(data = data.df, aes(color = y)) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  theme_minimal()
```


Since we know the data generating distribution, we can approximate the out-of-sample error for each classifier, by simulation.
```{r, warning=FALSE, cache = TRUE}
library(mvtnorm)
set.seed(42)

n.samples <- 1e4

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p[sample(x= nrow(mu.p), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n[sample(x= nrow(mu.n), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.test.df <- data.frame(data.matrix)
colnames(data.test.df) <- c("x1","x2","y")
data.test.df$y <- as.factor(data.test.df$y)

for (degree in seq(1,maxDegree)) {
  error.df$outError[degree] <- mean(data.test.df$y != predict(svm.model.list[[degree]],
                                                              data.test.df))
}
```

```{r inOutSampleError, cache = TRUE, echo = FALSE, fig.cap='Out-of-sample error vs. degree of polynomial.', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message=FALSE}
library(ggplot2)
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(data = error.df) +
  geom_line(mapping = aes(y = inError, x = degree, color = "in-sample"), size = 1) +
  geom_line(mapping = aes(y = outError, x = degree, color = "out-of-sample"), size = 1) +
  scale_color_manual(values = c(
    "in-sample" = cbp1[2],
    "out-of-sample" = cbp1[3])) +
  labs(y = "error") +
  theme_minimal()
```
The in-sample error is decreasing as the degree of the polynomial increases, and so also the complexity of the model. When the degree is small, the in-sample error is a close approximation of the out-of-sample error, but as the degree increases the difference increases and for large degrees the in-sample error provides little information about the out-of-sample error. The out-of-sample error is at first decreasing, but unlike the in-sample-error, starts increasing after degree 4.

In the following sections we will investigate the connection between the in-sample and out-of-sample error.


## Hoeffding's inequality

One tool to understand the connection between the in-sample and out-of-sample error is *Hoeffding's inequality*. This is a result from probability theory and so we will present it as such. That is, in this section we do not discuss any application to statistical learning.

```{block2, type='note'}
Hoeffding's inequality states that:
Let $Y_1,\ldots, Y_n$ be iid with $E[Y_i]=\mu$ and $a\leq Y_i \leq b$. Then for any $\varepsilon>0$,
$$
P\left( \left| \bar Y_n - \mu \right|>\varepsilon \right) \leq 2e^{-2n\varepsilon^2/(b-a)^2}.
$$
```
The inequality is true for any random variable satisfying the conditions. However, we will only prove the special case that if $Y_i\overset{iid}{\sim}\mathsf{Be}(p)$, then
$$
P\left( \left| \bar Y_n - p \right|>\varepsilon \right) \leq 2e^{-2n\varepsilon^2}.
$$
We start by deriving *Markov's inequality*: Let $X$ be a non-negative random variable. For the sake of this calculation we assume $X$ is continuous with density $p(x)$, although the inequality holds also when this is not the case. Then for any $t>0$,
\begin{align}
E[X] &= \int_0^\infty xp(x)dx = \int_0^t xp(x)dx + \int_t^\infty xp(x)dx \\
&\geq \int_t^\infty x p(x)dx \geq t \int_t^\infty p(x)dx = tP(X> t).
\end{align}
We usually write this as $P(X>t)\leq E(X)/t$.

Now we turn to Hoeffding's inequality. Firstly,
$$
P\left( \left| \bar Y_n  - p \right|\geq \varepsilon \right) = P\left( \bar Y_n \geq p+\varepsilon  \right) + P\left( \bar Y_n \leq p - \varepsilon  \right).
$$
Then we use that $\exp$ is an increasing function, together with Markov's inequality, for any $t>0$,
\begin{align*}
P\left( \bar Y_n \geq p +  \varepsilon  \right) &= P\left( t\sum_{i=1}^n Y_i \geq tn(p + \varepsilon)  \right) = P\left( e^{t\sum_{i=1}^n Y_i }\geq  e^{tn(p +\varepsilon) } \right) \\
&\overset{\text{Markov}}{\leq} e^{-tn(p+\varepsilon)}E\left[ e^{t\sum_{i=1}^n Y_i } \right] \overset{indep.}{=} e^{-tn(p+\varepsilon)}\prod_{n=1}^nE\left[ e^{t Y_i } \right] =  e^{-tn(p+\varepsilon)}E\left[ e^{t Y_i } \right]^n.
\end{align*}

Now we need to bound the last expression. Since $Y_i$ is Bernoulli,
$$
E\left[ e^{t Y_i } \right] = e^{t\cdot 1}P(Y_i=1) + e^{t\cdot 0}P(Y_i=0) = pe^t + (1-p).
$$
We would like to show that $pe^t+1-p\leq e^{tp+t^2/8}$. Since then,
$$
P\left( \bar Y_n \geq p +  \varepsilon  \right) \leq e^{-tn(p+\varepsilon)}E\left[ e^{t Y_i } \right]^n \leq e^{-tn(p+\varepsilon)} e^{ntp+nt^2/8} = e^{-nt\varepsilon + nt^2/8} \leq e^{-2n\varepsilon^2},
$$
which is what we want. In the last step we used that $-nt\varepsilon + nt^2/8$ has a maximum at $t=4\varepsilon$, which is easy to check. A similar argument will give that $P\left( \bar Y_n \leq p -  \varepsilon  \right)$ can be bounded in the same way and then we arrive at Hoeffding's inequality.

So, we need to study $pe^t+1-p$ and $e^{tp+t^2/8}$. First note that the inequality is true if and only if
$$
f(t):=\ln (pe^t+1-p) \leq tp+t^2/8,
$$
since $\ln$ is increasing. By Taylor's theorem, we can write
$$
f(t) = f(0) + f'(0)t + f''(\zeta)\frac{t^2}{2},
$$
for some $0\leq \zeta \leq t.$ However $f(0)=0$ and $f'(0) = \frac{pe^0}{pe^0+1-p} = p$ and
$$
f''(t) = \frac{pe^t}{pe^t+1-p} - \frac{(pe^t)^2}{(pe^t+1-p)^2} = \frac{pe^t}{pe^t+1-p}\left( 1-\frac{pe^t}{pe^t+1-p} \right) = \rho(1-\rho),
$$
with $\rho:= \frac{pe^t}{pe^t+1-p}.$ Now, it is easy to see that $0\leq\rho\leq 1$ therefore that  $\rho(1-\rho)\leq 1/4$ so that $f''(t)\leq 1/4.$ All together,
$$
f(t) = tp + f''(\zeta)\frac{t^2}{2} \leq tp + \frac{t^2}{8},
$$
and we are done.

If we choose
$$
\varepsilon = \sqrt{\frac{1}{2n}\ln \frac{2}{\delta}},
$$
we get
$$
P\left( \left| \bar Y_n - p \right|\leq \sqrt{\frac{1}{2n}\ln \frac{2}{\delta}} \right) \geq 1-2\exp\left( -2n\frac{1}{2n}\ln \frac{2}{\delta} \right) =1-\delta.
$$
```{block2, type='note'}

Stated another way, with probability at least $1-\delta$,
$$
\left| \bar Y_n - p \right|\leq \sqrt{\frac{1}{2n}\ln \frac{2}{\delta}}.
$$
````

## Generalization error

In this section we will study the *generalization error*, that is the difference between $E_{out}(h)$ and $E_{in}(h)$.

We will only consider binary classification and we will assume that there is a function $h$ such that $Y=h(X)$. This is a simplification since it means that we only need to consider the distribution of $X$. To make things concrete we study an example where $\mathcal X = \mathbb R^2$ and a linear classification, where observations are classified according to which side of a straight line they fall.

Before looking at the data, let us choose a classification algorithm completely willy-nilly. Let us choose:
$$
h(x_1,x_2)=\begin{cases}
1 \text{ if } x_1 >0\\
-1 \text{ otherwise}.
\end{cases}
$$
Now we generate some data:
```{r genError, cache = TRUE, echo = FALSE, fig.cap='Training data and classification rule', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}

set.seed(42)

mu.p <- c(rnorm(1),rnorm(1))
mu.n <- c(rnorm(1),rnorm(1))

sd.p <- c(abs(rnorm(1))+1,abs(rnorm(1))+1)
sd.n <- c(abs(rnorm(1))+1,abs(rnorm(1))+1)

library('mvtnorm')
library('ggplot2')

n.samples <- 50

data.matrix <- matrix(nrow = n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  if (runif(1)>0.5) {
    sample <- rmvnorm(1, mean = mu.p, sigma = diag(sd.p))
    data.matrix[i,] <- c(sample, 1)
  }
  else{
    sample <- rmvnorm(1, mean = mu.n, sigma = diag(sd.n))
    data.matrix[i,] <- c(sample, -1)
  }
}

data.df <- data.frame(data.matrix)
colnames(data.df) <- c("x1","x2","y")
data.df$y <- as.factor(data.df$y)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(data = data.df, aes(x = x1, y = x2, color = y)) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  annotate("rect", xmin = 0, xmax = 5, ymin = -5, ymax = 5, fill = cbp1[3], alpha = .2) +
    annotate("rect", xmin = -5, xmax = 0, ymin = -5, ymax = 5, fill = cbp1[2], alpha = .2)
```
We can now calculate the in-sample error:
```{r, cache = TRUE}
H <- function(a, b){
  function(x1, x2){
    if(a*x1 + b*x2 > 0) "1"
    else "-1"
  }
}

h <- H(1,0)

error <- function(data, classifier){
  missClass <- 0
  for (i in seq_len(nrow(data))) {
    x1 <- data[i,]$x1
    x2 <- data[i,]$x2
    y <- data[i,]$y
    if(classifier(x1,x2) != y )
      missClass <- missClass + 1
  }
  missClass/nrow(data)
}

error(data.df, h)
```
In this case, $h$ is not a function of the training data so the in-sample error is an unbiased estimate of the out-of-sample error. Since we know the distribution of $X,Y$ we can calculate the out-of-sample error by simulation.
```{r, cache = TRUE}
n.samples <- 1e4

data.matrix <- matrix(nrow = n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  if (runif(1)>0.5) {
    sample <- rmvnorm(1, mean = mu.p, sigma = diag(sd.p))
    data.matrix[i,] <- c(sample, 1)
  }
  else{
    sample <- rmvnorm(1, mean = mu.n, sigma = diag(sd.n))
    data.matrix[i,] <- c(sample, -1)
  }
}

data.test.df <- data.frame(data.matrix)
colnames(data.test.df) <- c("x1","x2","y")
data.test.df$y <- as.factor(data.test.df$y)
error(data.test.df, h)
```

Using Hoeffding's inequality, we can give a guarantee of the difference between the in-sample and the out-of-sample error. Let us review: A pair $X,Y$ is drawn from some distribution. We then apply the function $h$ and with some probability we make an error on the classification, this probability is $E_{out}(h)$ and corresponds to the probability $p$ in Hoeffding. What we have is a sample of size $n$ and an estimate $E_{in}(h)$ of the probability of an error. This corresponds to $\bar Y_n$ in Hoeffding. Therefore this situation is exactly like the Bernoulli experiment from the previous section. We can say that with probability at least $1-\delta$,
$$
E_{out}(h) \leq  E_{in}(h) + \sqrt{\frac{\ln \frac{2}{\delta}}{2n}}.
$$
Let us say we want to have confidence 95%, that is $\delta = 0.05$, we then have the generalization bound
```{r, cache = TRUE}
delta = 0.05
error(data.df,h) + sqrt(log(2/delta)/(2*nrow(data.df)))
```
That is, with confidence 95%, the out-of-sample error is not larger than this.

<p>Above we picked an $h$ without looking at the data and so it can not really be considered statistical learning. Therefore, for the generalization bound to be useful, we need to handle the situation where $h$ is chosen from some collection $\mathcal H$. Let us first consider the case where $\mathcal H$ is finite, that is there is a finite number of functions $h$ in the collection. Call this number $|\mathcal H|$. The statement of the learning bound then becomes</p>
```{block2, type='note'}
Let $\mathcal H$ be finite. Then for any $\delta>0$, with probability at least $1-\delta$:
$$
  \forall h\in\mathcal H,\quad E_{out}(h)\leq E_{in}(h) + \sqrt{\frac{ \ln\frac{2|\mathcal H|}{\delta}}{2n}}.
$$
```
The proof of this is again an application of Hoeffding. Let $h_1,\ldots,h_{|\mathcal H|}$ be the elements of $\mathcal H$. Then
\begin{align}
&P\left( \exists h\in\mathcal H : \left| E_{in}(h) - E_{out}(h)  \right|>\varepsilon \right)\\
=& P\left( \left\{ \left| E_{in}(h_1) - E_{out}(h_1)  \right| > \varepsilon \right\} \cup \ldots \cup \left\{ \left| E_{in}(h_{|\mathcal H |}) - E_{out}( (h_{| \mathcal H |})  \right| > \varepsilon \right\} \right)\\
\leq & \sum_{i=1}^{|\mathcal H|} P\left(\left| E_{in}(h_i) - E_{out}(h_i)  \right|>\varepsilon \right)\\
\leq & 2 |\mathcal H  | e^{-2n\varepsilon^2}.
\end{align}
Setting this equal to $\delta$ and solving for $\varepsilon$ gives the result.

Now we can handle also the case with a finite number of $h$. But it is still not very useful. Most classifications methods will have an infinite $\mathcal H$. For example, in the example above, we would consider all different straight lines and no only $x_1 = 0$. The above argument will not work for infinite $\mathcal H$. There is however some hope. We used that, for two events $A$ and $B$,
$$
P(A\cup B) = P(A) + P(B) - P(A\cap B) \leq P(A) + P(B).
$$
But this inequality is not very tight. When $A$ and $B$ tend to happen at the same time $P(A \cup B)\approx P(A)$ and so the right hand side will be roughly twice the left hand side. We can expect this to happen also for the generalization errors. That is, many $h$ in $\mathcal H$ are similar, so if $\left| E_{in}(h_1) - E_{out}(h_1)  \right|>\varepsilon$ it is likely that also $\left|  E_{in}(h_2) - E_{out}(h_2)  \right|>\varepsilon$.

## VC-dimension

In this section we discuss a measure of the complexity of a hypothesis set $\mathcal H$, the *Vapnik–Chervonenkis* (VC) dimension.

We call one possible way of labeling a set of points, $S$, a *dichotomy* and we say that $S$ is *shattered* by $\mathcal H$ when $\mathcal H$ can generate all possible dichotomies of $S$. The VC-dimension of $\mathcal H$ is the size of the largest set that can be shattered by $\mathcal H$.

First consider the example of the real line and $\mathcal H$ being the classifiers that classify to one class to the left of a point and to the other class to the right of the point. If $S$ consists of two points, all four dichotomies can be realized and so $S$ can be shattered by $\mathcal H$. However if $S$ consists of three points, the dichotomy (1,0,1) cannot be realized and $S$ cannot be shattered. Therefore the VC-dimension of $\mathcal H$ is $d_{VC}=2$.

```{r vcDim1, cache = TRUE, echo = FALSE, fig.cap='Two points can be shattered', out.width='80%', fig.asp=.3, fig.align='center', warning=FALSE}

library(ggplot2)

#Dichotomy 1
x <- c(-1,1)
category <- c("-1","1")

h <- function(x){
  if(x[1]>0){
    "1"
  }
  else{
    "-1"
  }
}


plotFcn <- function(x,category, h){
  grid <- expand.grid(x = seq(-2,2, length = 10), y = seq(-0.01,0.01, length = 10))
  grid$category <- as.factor(apply(grid, 1, h))
  
  cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
            "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
  
  p <- ggplot() +
    geom_point(mapping = aes(x=x, y = 0, color=category), data = data.frame(x, category),size = 5, show.legend = FALSE)  +
    annotate("segment",x=-2,xend=2, y=0, yend=0, size=1) +
    geom_raster(data = grid, mapping = aes(x=x, y=y, fill = category), alpha = 0.2, show.legend = FALSE) +
    scale_x_continuous(limits = c(-2,2)) +
    scale_y_continuous(limits = c(-0.1,0.1)) +
    scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
    scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
    theme(panel.background = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          axis.title = element_blank())
}
p1 <- plotFcn(x, category, h)

#Dichotomy 2
x <- c(-1,1)
category <- c("-1","-1")

h <- function(x){
  if(x[1]>0){
    "-1"
  }
  else{
    "-1"
  }
}

p2 <- plotFcn(x, category, h)

#Dichotomy 3
x <- c(-1,1)
category <- c("1","1")

h <- function(x){
  if(x[1]>0){
    "1"
  }
  else{
    "1"
  }
}

p3 <- plotFcn(x, category, h)

#Dichotomy 4
x <- c(1,-1)
category <- c("-1","1")


h <- function(x){
  if(x[1]>0){
    "-1"
  }
  else{
    "1"
  }
}
p4 <- plotFcn(x, category, h)

library(gridExtra)

grid.arrange(p1, p2, p3, p4, nrow = 1)
```
```{r vcDim2, cache = TRUE, echo = FALSE, fig.cap='Three points that can not be shattered', out.width='80%', fig.asp=.3, fig.align='center', warning=FALSE}

library(ggplot2)

#Non shattered set
x <- c(-1,0,1)
category <- c("-1","1","-1")


cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
            "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
  
ggplot() +
    geom_point(mapping = aes(x=x, y = 0, color=category), data = data.frame(x, category),size = 5, show.legend = FALSE)  +
    annotate("segment",x=-2,xend=2, y=0, yend=0, size=1) +
    scale_x_continuous(limits = c(-2,2)) +
    scale_y_continuous(limits = c(-0.1,0.1)) +
    scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
    theme(panel.background = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          axis.title = element_blank())

```


Next consider points on $\mathbb R^2$ and $\mathcal H$ being the set of straight lines, as in the example in the previous section. Clearly three points can be shattered by a straight line, as long as the points are not colinear. However four points can never be shattered. So the VC-dimension is $d_{VC}=3$. Similarly in $R^p$, if $\mathcal H$ are the hyperplanes, $d_{VC} = p+1$.

```{r vcDim3, cache = TRUE, echo = FALSE, fig.cap='Three points can be shattered but not four', out.width='80%', fig.asp=.3, fig.align='center', warning=FALSE}
#Dichotomy 1
x1 <- c(-1,0, 1)
x2 <- c(-1,1, -1)
category <- c("-1","1", "1")

data.df <- data.frame(x1,x2,category)

h <- function(x){
  if(x[1]>-0.5){
    "1"
  }
  else{
    "-1"
  }
}

  grid <- expand.grid(x1 = seq(-2,2, length = 10), x2 = seq(-2,2, length = 10))
  grid$category <- as.factor(apply(grid, 1, h))
  
  cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
            "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
  

p1 <- ggplot(mapping = aes(x = x1, y = x2)) +
  geom_raster(data = grid, aes(fill = category), alpha = 0.2, show.legend = FALSE) +
  geom_point(data = data.df, aes(color = category), show.legend = FALSE, size =5 ) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_x_continuous(limits = c(-2,2)) +
  scale_y_continuous(limits = c(-2,2)) +
  theme(panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

#Dichotomy 2
x1 <- c(1,1,-1,-1)
x2 <- c(1,-1,1, -1)
category <- c("-1","1", "1","-1")

data.df <- data.frame(x1,x2,category)

h <- function(x){
  if(x[1]>-0.5){
    "1"
  }
  else{
    "-1"
  }
}

grid <- expand.grid(x1 = seq(-2,2, length = 10), x2 = seq(-2,2, length = 10))
grid$category <- as.factor(apply(grid, 1, h))

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")


p2 <- ggplot(mapping = aes(x = x1, y = x2)) +
  geom_point(data = data.df, aes(color = category), show.legend = FALSE, size = 5) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_x_continuous(limits = c(-2,2)) +
  scale_y_continuous(limits = c(-2,2)) +
  theme(panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

library(gridExtra)

grid.arrange(p1,p2, nrow = 1)
```
<p>
We see a pattern here, the VC-dimension is equal to the number of parameters of $\mathcal H$. This is however not always true. As a counterexample, take the functions $\left\{ x\mapsto sin(\omega x)\mid  \omega\in \mathbb R \right\}$. Then label a point $x$ according to the sign of $sin(\omega x)$. There is only one parameter, but any $S$ can be shattered and so $d_{VC}=\infty$.</p>

```{r vcDim4, cache = TRUE, echo = FALSE, fig.cap='A one parameter function can shatter any number of points', out.width='80%', fig.asp=.3, fig.align='center', warning=FALSE}
x <- c(0.5,1,1.5,2,2.5,3)
category <- c("1","-1","1","-1", "-1", "1")

f <- function(w){
  function(x){sin(w*x)/100}
}

h <- function(f){
  out <- rep("-1", length(x))
  
  for(i in seq_along(x)){
    
    if(f(x[i])>0){
      out[i] <- "1"
    }
    
  }
  out
}

loss <- function(w){
  sum(h(f(w)) != category)
}

testW <- seq(0.1,10000,by =0.1)
losses <- sapply(testW,loss)
w <- testW[min(which(losses==0))]

f <- function(x){
  sin(w*x)/25
}

h <- function(x){
  if(f(x[1])>0){
       "1"
    }
  else{
    "-1"
  }
}

grid <- expand.grid(x = seq(0,4, length = 1000), y = seq(-0.05,0.05, length = 10))
grid$category <- as.factor(apply(grid, 1, h))

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

library(ggplot2)

ggplot() +
  geom_point(mapping = aes(x=x, y = 0, color=category), data = data.frame(x, category),size = 5, show.legend = FALSE)  +
  annotate("segment",x=0,xend=4, y=0, yend=0, size=1) +
  stat_function(data = data.frame(x = c(-2,2)), mapping = aes(x), fun = f, geom = "line") +
  geom_raster(data = grid, mapping = aes(x=x, y=y, fill = category), alpha = 0.2, show.legend = FALSE) +
  scale_x_continuous(limits = c(0,4)) +
  scale_y_continuous(limits = c(-0.1,0.1)) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  theme(panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())


```

The usefulness of the VC-dimension comes from the following result.
```{block2, type='note'}
Let $\mathcal H$ have VC-dimension $d_{VC}$. Then for any $\delta>0$, with probability at least $1-\delta$, the following holds for all $h\in \mathcal H$ [@mohri2018foundations]:
$$
  E_{out}(h) \leq E_{in}(h) +\sqrt{\frac{2d_{VC}\ln \frac{en}{d_{VC}}}{n}} + \sqrt{\frac{\ln \frac{1}{\delta}}{2n}   }
$$
```

The important quantity here is $n/d_{VC}$. If the sample size $n$ is large in relation to the VC-dimension $d_{VC}$, the out-of-sample error is guaranteed to be close to the in-sample error. The bound is loose, so the actual value is not useful in practice. Rather it provides a way of thinking about generalization from in-sample to out-of-sample error. In practice, it is usually the case that models with lower $d_{VC}$ generalizes better than models with higher $d_{VC}$. A popular rule of thumb is that $n$ should be at least $10\times d_{VC}$. However, there are also algorithms with $d_{VC}=\infty$ that work well in practice.

A different way of thinking of this results is
$$
E_{out}(h) = E_{in}(h) + \Omega(n,\mathcal H, \delta),
$$
where $\Omega$ is a penalty for model complexity. As we choose a more complex $\mathcal H$ (with higher $d_{VC}$) the in-sample error will become smaller. But, at the same time the penalty $\Omega$ gets larger. The penalty gets smaller when we have more samples. The optimal model, with smallest $E_{out}$, is therefore a compromise that minimize the sum of the two terms.

One commonly used way to estimate $E_{out}$ is to split the data into one <em>training set</em> and one <em>test set</em>. The test set is not used for training and so when we calculate $E_{test}$, we do this for only one $h$, the one with minimal in-sample error. Therefore Hoeffding's inequality applies as a generalization bound, which is much tighter than the VC-bound.

In the figure below we visualize the partition of the out-of-sample error into in-sample error and generalization error.

```{r errorPlotSketch, cache = TRUE, echo = FALSE, fig.cap='Sketch of the different components of the error', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot() +
  geom_function(aes(color = "in"), fun = function(x){1/sqrt(x/5)}, size = 1) +
  geom_function(aes(color = "gen"), fun = function(x){sqrt(x/5)}, size = 1) +
  geom_function(aes(color = "out"), fun = function(x){1/sqrt(x/5)+sqrt(x/5)}, size = 1) +
  xlim(1, 15) +
  scale_colour_manual(name = "",
                      values = c("in" = cbp1[2], "gen" = cbp1[3], "out" = cbp1[4]),
                      labels = c("in-sample","generalization", "out-of-sample")) +
  labs(x = "model complexity", y = "error") +
  theme_minimal()

```

## Support vector machines II

Based on the previous discussion, in this section we develop the final form of SVM.

The main conclusion from the previous sections is that the in-sample error is not an accurate estimate of the out-of-sample error and if we train a model with high complexity we may overfit the model. One way of dealing with this problem is to limit the complexity of the model, for example in terms of the VC-dimension. In the SVM case this could mean to limit the degree of the polynomial. Another way, which we will explore in this section, is to recall from the previous section that
$$
E_{out} = E_{in} + \Omega(N,\mathcal H, \delta).
$$
Now instead of minimizing the in-sample error of our model, we add a term that represents $\Omega$, which is increasing in the complexity of the model. Then the learning problem instead becomes,
$$
\min_{h\in\mathcal H}\left[ E_{in}(h) + c(h)\right],
$$
where $c$ is some function that increases with the complexity of the function $h$. SVMs are a special case of this. If we have a linear classifier $f(x) = \beta_0+\beta_1x^1 + \ldots \beta_p x^p$ the optimisation problem is
$$
\min_{\beta_0,\beta_1,\ldots,\beta_p} \sum_{i=1}^n \left( 1-y_if(x_i) \right)_+ + \lambda \sum_{i=1}^p{\beta_j^2},
$$
where $\lambda\geq 0$ is a tuning parameter. Here a large value of $\lambda$ will penalize complex models heavily and force the model to be simple, thereby preventing overfitting. A small value of $\lambda$ will allow more complex models, thereby making the in-sample error smaller, on the other hand risking overfitting.

As usual, we may take some function of the inputs $\varphi_i(x)$ and consider a linear function of these new features. Let us see what happens when we vary $\lambda$, using polynomial $\varphi$ of maximum degree 5.
```{r, warning=FALSE, cache = TRUE, echo=FALSE}
library(mvtnorm)
set.seed(42)

mu.p <- rmvnorm(10,mean = c(1,0), sigma = diag(2))
mu.n <- rmvnorm(10,mean = c(0,1), sigma = diag(2))

n.samples <- 100

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p[sample(x= nrow(mu.p), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n[sample(x= nrow(mu.n), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.df <- data.frame(data.matrix)
colnames(data.df) <- c("x1","x2","y")
data.df$y <- as.factor(data.df$y)
```
```{r message=FALSE, warning=FALSE, cache = TRUE, results = "hide", echo = FALSE}
library(kernlab)

pars <- seq(0.1,40.1,1)
error.df <- data.frame("parameter" = pars, inError = NA, outError = NA)

svm.model.list <- vector(mode = "list", length = length(pars))

for (i in seq_along(pars)) {
  svm.model.list[i] <- ksvm(y ~ poly(x1, x2, degree = 5), data = data.df,
                                 type = "C-svc",
                                 kernel = "vanilladot",
                                 C = pars[i])
  error.df$inError[i] <- mean(data.df$y != predict(svm.model.list[[i]], data.df))
}

```

```{r, warning=FALSE, cache = TRUE, echo =FALSE}
library(mvtnorm)
set.seed(42)

n.samples <- 1e5

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p[sample(x= nrow(mu.p), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n[sample(x= nrow(mu.n), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.test.df <- data.frame(data.matrix)
colnames(data.test.df) <- c("x1","x2","y")
data.test.df$y <- as.factor(data.test.df$y)

for (i in seq_along(pars)) {
  error.df$outError[i] <- mean(data.test.df$y != predict(svm.model.list[[i]],
                                                              data.test.df))
}
```

```{r inOutSampleErrorSVM, cache = TRUE, echo = FALSE, fig.cap='Error vs. reciprocal of lambda.', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message=FALSE}
library(ggplot2)
library(latex2exp)
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(data = error.df) +
  geom_line(mapping = aes(y = inError, x = parameter, color = "in-sample"), size = 1) +
  geom_line(mapping = aes(y = outError, x = parameter, color = "out-of-sample"), size = 1) +
  scale_color_manual(name = "",
                     values = c("in-sample" = cbp1[2],
                                "out-of-sample" = cbp1[3])) +
  labs(x = TeX('$1/\\lambda$'), y = "error") +
  theme_minimal()
```
We see from the picture that $\lambda$ controls the complexity of the model. When $\lambda$ is large ($1/\lambda$ small), the in-sample and out-of-sample errors are similar and as $\lambda$ becomes smaller, the in-sample error becomes smaller, but the difference between the errors are larger.

With this development, we do not need to limit the complexity of the class of function $\mathcal H$, since this is done by choosing $\lambda$. In fact, we can in some cases even choose $\mathcal H$ to be an infinite dimensional space of functions.

It turns out that the solution to the optimization problem of SVM can be written as
$$
f(x) = \beta_0 + \sum_{i=1}^n\alpha_i\langle x,x_i  \rangle,
$$
for some values $\alpha_i$, where we define the inner product of two points $x$ and $\tilde x$ as,
$$
\langle x,\tilde  x \rangle = \sum_{j=1}^p x^j\tilde x^j.
$$
If we now instead consider the input features $\varphi(x) = (\varphi_1(x),\ldots,\varphi_M(x))$ and functions of the form
$$
f(x) = \beta_0 + \varphi_1(x)\beta_1 + \ldots \varphi_M(x)\beta_M  ,
$$
in the same way as above, the SVM classifier is
$$
f(x) = \beta_0 + \sum_{i=1}^n\alpha_i\langle \varphi(x),\varphi(x_i)  \rangle.
$$
We then see that the features $\varphi$ only appears in the inner product. So it is enough if we specify the function
$$
K(x,x_i):= \langle \varphi(x),\varphi(x_i)  \rangle,
$$
which is called a *kernel*. We then write
$$
f(x) = \beta_0 + \sum_{i=1}^n\alpha_iK(x,x_i).
$$
An important point here is that the dimension of $\varphi$, $M$, does not appear here. The sum is over (at most) $n$ terms, even if let $M\to\infty$.

In practice, we therefore specify the kernel and not the features. Two examples of kernels are the polynomial kernel
$$
K(x,\tilde x) = (1+\sum_{i=1}^px_i\tilde x_i)^d,
$$
and the radial basis kernel,
$$
K(x,\tilde x) = \exp\left( -\gamma  \sum_{i=1}^p (x_i-\tilde x_i)^2\right).
$$
Let us try the radial basis function kernel on our data set.
```{r, warning=FALSE, cache = TRUE, echo=FALSE, message = FALSE}
library(mvtnorm)
set.seed(42)

mu.p <- rmvnorm(10,mean = c(1,0), sigma = diag(2))
mu.n <- rmvnorm(10,mean = c(0,1), sigma = diag(2))

n.samples <- 100

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p[sample(x= nrow(mu.p), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n[sample(x= nrow(mu.n), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.df <- data.frame(data.matrix)
colnames(data.df) <- c("x1","x2","y")
data.df$y <- as.factor(data.df$y)
```
```{r, warning=FALSE, cache = TRUE, echo =FALSE, message=FALSE}
library(mvtnorm)
set.seed(42)

n.samples <- 1e5

data.matrix <- matrix(nrow = 2*n.samples, ncol = 3)

for (i in seq_len(n.samples)) {
  mu = mu.p[sample(x= nrow(mu.p), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)

  data.matrix[2*i-1,] <- c(sample, 1)

  mu = mu.n[sample(x= nrow(mu.n), size = 1),]
  sample <- rmvnorm(1, mean = mu, sigma = diag(2)/5)
  data.matrix[2*i,] <- c(sample, -1)
}

data.test.df <- data.frame(data.matrix)
colnames(data.test.df) <- c("x1","x2","y")
data.test.df$y <- as.factor(data.test.df$y)
error.in <- mean(data.df$y != predict(svm.model,data.df))
error.out <- mean(data.test.df$y != predict(svm.model,data.test.df))
```
```{r radialBasisSVM, cache = TRUE, echo = FALSE, fig.cap='Training data a radial basis SVM classification', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE, message =FALSE}
library(caret)

svm.model <- train(
  y ~., data = data.df, method = "svmRadial",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
grid <- expand.grid(x1 = seq(-5,5, length = 500), x2 = seq(-5,5, length = 500))
grid$predicted <- as.factor(predict(svm.model, grid))

library(ggplot2)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(mapping = aes(x = x1, y = x2)) +
  geom_raster(data = grid, aes(fill = predicted), alpha = 0.2) +
  geom_point(data = data.df, aes(color = y)) +
  scale_colour_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  scale_fill_manual(values = c("-1" = cbp1[2], "1" = cbp1[3])) +
  theme_minimal()
```

This now has an in-sample error of `r error.in` and out-of-sample error `r error.out`




## Bias-Variance decomposition

In this section we will see another approach for understanding how model complexity affects statistical learning. Instead of bounding the difference between $E_{out}$ and $E_{in}$ we will decompose $E_{out}$ into two different error terms.

We consider the regression problem and therefore we assume a quadratic loss function,
$$
E_{out}(h) = E\left[ (Y- h(X))^2 \right].
$$
If $h^{\mathcal D}$ is the final hypothesis that was learnt from the data $\mathcal D$, we also define
$$
\bar h(x) := E\left[h^{\mathcal D}(x))\right].
$$
This is the average function that we would learn, averaged over all different datasets. It is just a theoretical tool and nothing that we would be able to calculate in practice.

First note that we can always write
$$
Y = f(X) + \varepsilon,
$$
where $E\left[ \varepsilon \right] = 0$. This is because if we take $f(X) = E[Y\mid X]$,
$$
E\left[\varepsilon\right] = E\left[ Y-f(X) \right] = E\left[ Y -  E\left[ Y\mid X \right] \right] = E\left[ Y\right]- E\left[ Y\right] = 0.
$$
First let us see what the population minimizer of the error is. That is, assuming that we know the distribution of $X,Y$, we would like to find the optimal choice of $h$. We do this by conditioning on $X$ and decomposing the error into two parts:
\begin{align*}
&E\left[ (Y-h(X))^2 \mid X \right] = E\left[ (Y - E[Y\mid X] + E[Y\mid X] -h(X))^2 \mid X \right] \\
=&E\left[ (Y- E[Y\mid X])^2 \mid X \right] + E\left[ ( E[Y\mid X] - h(X))^2 \mid X \right] \\
&+ \underbrace{2E\left[ (Y- E[Y\mid X])( E[Y\mid X] - h(X)) \mid X \right]}_{=0}\\
=&E\left[ (Y- E[Y\mid X])^2 \mid X \right] + E\left[ ( E[Y\mid X] - h(X))^2 \mid X \right]
\end{align*}
The first part does not depend on $h$ and the second part is minimized by choosing $h(x) = E[Y\mid X=x]$ for every $x$, which therefore is the population minimizer of the error.

Coming back slightly to reality, we do not know the distribution of $(X,Y)$ but we have a training set $\mathcal D$. So we now take the frequentist point of view and consider the squared difference between the function learnt from data, $h^\mathcal D$ and the true regression function $f(x)$, averaged with respect to $\mathcal D$.
\begin{align}
&E\left[ (h^{\mathcal D}(x) - f(x))^2\right]  = E\left[ (h^{\mathcal D}(x) - \bar h(x) + \bar h(x) - f(x) )^2\right] \\
=&  E\left[ (h^{\mathcal D}(x) - \bar h(x) )^2\right] +
 E\left[ (\bar h(x) - f(x) )^2\right] +2(\bar h(x) - f(x)) E\left[ (h^{\mathcal D}(x) - \bar h(x) )\right]\\
=& E\left[ (h^{\mathcal D}(x) - \bar h(x) )^2\right] +
 (\bar h(x) - f(x) )^2
\end{align}
The first term is the variance of $h^\mathcal D$ thinking of the data set $\mathcal D$ as random. The second term is the difference between the average function $\bar h$ and the true regression function $f(x)$ and is called the bias. This is the *Bias-variance decomposition*.

If we choose a large class of functions $\mathcal H$ from which we choose $h^\mathcal D$, it will be possible to closely approximate $f$, on average. In this case the bias will be small. However, many times if $\mathcal H$ is large, $h^\mathcal D$ will be sensitive to data, that is we will overfit, and then the variance term is large. This is known as the trade-off between bias and variance. However it is important to understand that this trade-off is an empirical observation and not an theoretical fact. We can illustrate this with a picture similar to what we saw in the discussion of generalization error.

```{r biasVarianceSketch, cache = TRUE, echo = FALSE, fig.cap='Sketch of the bias-variance decomposition', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot() +
  geom_function(aes(color = "in"), fun = function(x){1/sqrt(x/5)}, size = 1) +
  geom_function(aes(color = "gen"), fun = function(x){sqrt(x/5)}, size = 1) +
  geom_function(aes(color = "out"), fun = function(x){1/sqrt(x/5)+sqrt(x/5)}, size = 1) +
  xlim(1, 15) +
  scale_colour_manual(name = "",
                      values = c("in" = cbp1[2], "gen" = cbp1[3], "out" = cbp1[4]),
                      labels = c("variance", "bias", "total error")) +
  labs(x = "model complexity", y = "error") +
  theme_minimal()

```

## Regression regularization

In this section we discuss *regularization* in the context of regression as a way of controlling overfit.

Recall that in regression we have observations from $\mathbb R^p$ and our task is to predict a value in $\mathbb R$. We will do this by finding a function $h\in \mathcal H$ that gives a small out-of-sample error
$$
E_{out} = E\left[ (Y-h(X))^2 \right].
$$
In linear regression, the collection of functions $\mathcal H$ are the functions of the form
$$
h(x) = \beta_1x^1+ \cdots + \beta_p x^p,
$$
and we are thus to choose the parameters $\beta_1,\ldots,\beta_p$ in a good way. The $\beta$ that minimises the in-sample error is the least squares estimate of $\beta_1,\ldots,\beta_p$ and can be calculated easily.

As we have seen, we may specify some function $\varphi_i$ and use $\varphi_1(x),\ldots,\varphi_M(x)$ to predict $y$ and we are still in the case of linear regression. However we need to be careful not to overfit. Let us see a simple example were $\varphi$ are polynomials. First we construct the functions that will generate our data:
```{r, cache = TRUE}
library(ggplot2)
library(gridExtra)
library(resample)

gen_data <- function(truth, n.obs = 100) {
  x <- runif(n.obs, min = -2, max = 2)
  y <- truth(x) + rnorm(n.obs) * 0.4
  data.df <- data.frame(y = y, x = x)
}

truth <- function(x) {
  sin(x * 2 + 1) + x
}
```
For convenience, we write a function that fits three different polynomial regressions and returns the plot.
```{r, cache = TRUE}
fit_plot <- function(){
  data.train.df <- gen_data(truth)

  x.grid <- seq(-2, 2, 0.01)
  x.grid.df <- data.frame(x = x.grid)

  fit1 <- predict(lm(y ~ poly(x, 2), data.train.df),
                  x.grid.df)
  fit2 <- predict(lm(y ~ poly(x, 6), data.train.df),
                  x.grid.df)
  fit3 <- predict(lm(y ~ poly(x, 20), data.train.df),
                  x.grid.df)

  cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
            "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

  p <- ggplot(data.frame(
    x = x.grid,
    fit1 = fit1,
    fit2 = fit2,
    fit3 = fit3
  )) +
    geom_line(aes(x = x, y = fit1, color = "1"), size = 1) +
    geom_line(aes(x = x, y = fit2, color = "2"), size = 1) +
    geom_line(aes(x = x, y = fit3, color = "3"), size = 1) +
    scale_colour_manual(
      name = "",
      values = c(
        "train" = cbp1[1],
        "1" = cbp1[2],
        "2" = cbp1[3],
        "3" = cbp1[4]
      ),
      labels = c("1" = "2nd", "2" = "6th", "3" = "20th")
    ) +
    geom_point(aes(x = x, y = y, color = "train"),
               data = data.train.df,
               show.legend = FALSE) +
    ylab("y") +
    coord_cartesian(ylim = c(-3, 2)) +
    theme_minimal()

  p
}
```
Next we fit the polynomial models to four different training sets, generated from the same distribution.
```{r 4fits, cache = TRUE, echo = TRUE, fig.cap='Polynomial fits to four different sets of training data, sampled from the same distribution', out.width='100%', fig.asp=1, fig.align='center', warning=FALSE}
set.seed(42)
grid.arrange(fit_plot(), fit_plot(), fit_plot(), fit_plot(),
             nrow = 2)
```
We see from the pictures that the 2nd degree polynomial fit does not change very much on the different training sets, while the 20th degree polynomial looks very different in the four pictures.

<p>Now let us repeat this experiment many times, and only plot the mean and variance of $h$.</p>
```{r polynomialFits, cache=TRUE, echo = FALSE, fig.cap='Mean fit and 95% CI for the different polynomial regressions', out.width='100%', fig.asp=1, fig.align='center', warning=FALSE}
n.sim <- 1e3

x.grid <- seq(-2, 2, 0.01)
x.grid.df <- data.frame(x = x.grid)

h1 <- matrix(nrow = n.sim, ncol = length(x.grid))
h2 <- matrix(nrow = n.sim, ncol = length(x.grid))
h3 <- matrix(nrow = n.sim, ncol = length(x.grid))

for (i in seq_len(n.sim)) {
  data.df <- gen_data(truth)
  h1[i, ] <- predict(lm(y ~ poly(x, 2), data.df),
                     x.grid.df)
  h2[i, ] <- predict(lm(y ~ poly(x, 6), data.df),
                     x.grid.df)
  h3[i, ] <- predict(lm(y ~ poly(x, 20), data.df),
                     x.grid.df)
}

h.df <- rbind(data.frame(degree = "2",
                         hbar = colMeans(h1),
                         hmin = colMeans(h1) - 2 * colStdevs(h1),
                         hmax = colMeans(h1) + 2 * colStdevs(h1),
                         bias = (colMeans(h1) - truth(x.grid))^2,
                         std = colStdevs(h1),
                         var = colVars(h1),
                         meansq = (colMeans(h1) - truth(x.grid))^2 + colVars(h1),
                         x = x.grid),
              data.frame(degree = "6",
                         hbar = colMeans(h2),
                         hmin = colMeans(h2) - 2 * colStdevs(h2),
                         hmax = colMeans(h2) + 2 * colStdevs(h2),
                         bias = (colMeans(h2) - truth(x.grid))^2,
                         std = colStdevs(h2),
                         var = colVars(h2),
                         meansq = (colMeans(h2) - truth(x.grid))^2 + colVars(h2),
                         x = x.grid),
              data.frame(degree = "20",
                         hbar = colMeans(h3),
                         hmin = colMeans(h3) - 2 * colStdevs(h3),
                         hmax = colMeans(h3) + 2 * colStdevs(h3),
                         bias = (colMeans(h3) - truth(x.grid))^2,
                         std = colStdevs(h3),
                         var = colVars(h3),
                         meansq = (colMeans(h3) - truth(x.grid))^2 + colVars(h3),
                         x = x.grid))

h.df$degree <- as.factor(h.df$degree)

cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

data.example.df <- gen_data(truth)

p1 <- ggplot(data = subset(h.df, degree == "2")) +
  geom_line(aes(x = x, y = hbar, color = "h"), size = 1) +
  geom_ribbon(aes(x = x, ymin = hmin, ymax = hmax, fill = "h"), alpha = 0.4, show.legend = FALSE) +
  stat_function(aes(color= "truth"), fun = truth, size = 1, n = 200) +
  scale_colour_manual(name = "",
                       values = c("truth" = cbp1[1], "h" = cbp1[2]),
                       labels = c("truth" = "truth", "h" = "h")) +
  geom_point(aes(x=x, y = y), data.example.df) +
  ylab("y") +
  coord_cartesian(ylim = c(-3, 2)) +
  ggtitle("2nd degree") +
  theme_minimal()

p2 <- ggplot(data = subset(h.df, degree == "6")) +
  geom_line(aes(x = x, y = hbar, color = "h"), size = 1) +
  geom_ribbon(aes(x = x, ymin = hmin, ymax = hmax, fill = "h"), alpha = 0.4, show.legend = FALSE) +
  stat_function(aes(color= "truth"), fun = truth, size = 1, n = 200) +
  scale_colour_manual(name = "",
                      values = c("truth" = cbp1[1], "h" = cbp1[2]),
                      labels = c("truth" = "truth", "h" = "h")) +
  geom_point(aes(x=x, y = y), data.example.df) +
  ylab("y") +
  coord_cartesian(ylim = c(-3, 2)) +
  ggtitle("6th degree") +
  theme_minimal()

p3 <- ggplot(data = subset(h.df, degree == "20")) +
  geom_line(aes(x = x, y = hbar, color = "h"), size = 1) +
  geom_ribbon(aes(x = x, ymin = hmin, ymax = hmax, fill = "h"), alpha = 0.4, show.legend = FALSE) +
  stat_function(aes(color= "truth"), fun = truth, size = 1, n = 200) +
  scale_colour_manual(name = "",
                      values = c("truth" = cbp1[1], "h" = cbp1[2]),
                      labels = c("truth" = "truth", "h" = "h")) +
  geom_point(aes(x=x, y = y), data.example.df) +
  ylab("y") +
  coord_cartesian(ylim = c(-3, 2)) +
  ggtitle("20th degree") +
  theme_minimal()

p4 <- ggplot(h.df, aes(x = x, group = degree)) +
  geom_line(aes(y = bias, color = degree), size = 1, linetype = "dashed") +
  geom_line(aes(y = var, color = degree), size = 1, linetype = "dotted") +
  geom_line(aes(y = meansq, color = degree), size = 1) +
  scale_colour_manual(name = "",
                      values = c("truth" = cbp1[1], "2" = cbp1[2], "6" = cbp1[3], "20" = cbp1[4]),
                      labels = c("truth" = "truth", "2" = "2", "6" = "6", "20" = "20")) +
  ylab("Bias, variance") +
  coord_cartesian(ylim = c(0, 0.25)) +
  theme_minimal()


grid.arrange(p1, p2, p3,
             nrow = 2)
```
From the pictures we see that the 2nd degree polynomial has a bias, but for 6th degree, the bias is close to 0. The error in the 2nd degree polynomial is a sum of bias and variance, but for 6th and 20th the error is dominated by the variance term. The variance becomes larger as the degree of the polynomial becomes larger, while the bias is already close to 0 for 6th degree. Therefore, the total error, the sum of bias and variance, is smallest for a 6th degree polynomial. As before, we see that a very flexible model will tend to overfit the data and may perform worse than a less flexible model.

Another way to see the same phenomena is to choose one point, here $x=1.5$, and plot the bias and variance as we vary the degree of the polynomial. We do this for sample size $n=100$ and $n=10000$.

```{r errorVsDegree, cache=TRUE, echo = TRUE, fig.cap='Bias-variance decomposition when varying the degree of the polynomial. Left picture with n = 100, right with n = 10000', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
library(ggplot2)
library(gridExtra)

biasVariancePlot <- function(n.obs, x0 = 1.5, n.sim = 1e3, seed = 42){
  degrees <- seq(2,20,2)

  predictions <- matrix(nrow = n.sim, ncol = length(degrees))
  x.df = data.frame(x = x0)

  for (sim in seq_len(n.sim)) {
    data.df <- gen_data(truth, n.obs)
    for (i in seq(length(degrees))) {
      predictions[sim,i] <- predict(lm(y ~ poly(x, degrees[i]), data.df),
                                    x.df)
    }
  }

  plot.df <- data.frame(hbar = colMeans(predictions),
                         variance = colVars(predictions),
                         bias = (colMeans(predictions)-truth(x0))^2,
                         degree = degrees,
                         total = colVars(predictions) + (colMeans(predictions)-truth(x0))^2)

  cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
            "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

  ggplot(plot.df, aes(x = degree)) +
    geom_line(aes(y = bias, color = "bias"), size = 1) +
    geom_line(aes(y = variance, color = "variance"), size = 1) +
    geom_line(aes(y = total, color = "total"), size = 1) +
    scale_colour_manual(name = "",
                        values = c("bias" = cbp1[2], "variance" = cbp1[3], "total" = cbp1[4]),
                        labels = c("bias" = "bias", "variance" = "variance", "total" = "total")) +
    scale_y_continuous(trans='log10') +
    ylab("Error") +
    xlab("Degree") +
    theme_minimal()

}

grid.arrange(biasVariancePlot(100), biasVariancePlot(10000),
             nrow = 1)
```
From the pictures we see that the bias is decreasing and the variance is increasing, as the degree increases. In the left picture, $n=100$, the minimum total error is at a degree around 3. When the sample size increases, in the right picture, we can fit a more complex model, and the minimum error is at a degree around 8.

We have seen that we also in the regression setting need to bound the model complexity so that we do not overfit the model. This can be done by choosing a small family of models $\mathcal H$. In the above case this would mean limiting the degree of the polynomial. Another option, that we will explore here, is to include a regularization term in the loss function.

Recall that least squares regression minimizes the in-sample error:
$$
E_{in} = \sum_{i=1}^n\left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2.
$$
Ridge regression instead minimizes
$$
E_{in} + \lambda\sum_{j=1}^p \beta_j^2,
$$
and the lasso minimizes
$$
E_{in} + \lambda\sum_{j=1}^p | \beta_j |.
$$
Here $\lambda$ is a tuning parameter that needs to be determined separately. Ridge regression and lasso use different penalty terms of model complexity, but the main idea is the same. Models with many and large parameters will be penalized in favor of less complex models. Compared to least squares regression, the estimates of $\beta_j$ will be smaller in ridge and lasso. That is the estimates will shrink towards zero. These methods are therefore sometimes called *shrinkage methods*. We know that the estimates in least squares regression are unbiased and shrinkage methods therefore introduce a bias. The hope is that the variance will decrease sufficiently, so that the total prediction error is smaller. Note that neither of these methods are scale invariant, however usually software will take care of this automatically.

Let us look at the same example above, this time with a degree 20 polynomial, but penalized with ridge and lasso.
```{r, echo = TRUE, cache = TRUE, message=FALSE, error = FALSE}

library(ggplot2)
library(gridExtra)
library(glmnet)

data.df <- gen_data(truth)

model.ridge <- glmnet(poly(data.df$x,20), data.df$y, alpha = 0 )
model.lasso <- glmnet(poly(data.df$x,20), data.df$y, alpha = 1 )
```
```{r ridge, cache = TRUE, echo = FALSE, fig.cap='Coefficents as lambda is varied for ridge regression', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
plot(model.ridge, xvar = "lambda")
```
```{r lasso, cache = TRUE, echo = FALSE, fig.cap='Coefficients as lambda is varied for the lasso', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
plot(model.lasso, xvar = "lambda")
```
In the pictures we see that the fitted coefficients decrease towards 0 as $\lambda$ increases. The main difference between the two methods is that in the ridge regression all coefficients are strictly positive for any finite $\lambda$, while for the lasso, coefficients become 0 when $\lambda$ is large. This difference relates to model interpretability. That is, the lasso will produce a simple model where many variables can be disregarded. We also mention that the lasso and ridge regression can, unlike unregularized least squares, handle the case $p>>n$. There is a generalization of ridge and lasso called elastic net, where a parameter $0\leq \alpha \leq 1$ is introduced and the loss function is instead,
$$
E_{in} + \lambda\left( \alpha \sum_{j=1}^p | \beta_j | + (1-\alpha) \sum_{j=1}^p \beta_j^2\right).
$$

## Model selection

We have seen that in statistical learning a model that is too flexible will lead to overfitting and a model that is not flexible enough will lead to underfitting. We can control the flexibility of our model by either choosing the model class, e.g.\ the degree of a polynomial, or by introducing regularization. In either case this introduces a new parameter that we can not learn from the in-sample error. In this section we discuss how choose this parameter.

One approach is to use a *validation set*. We would take the available observations and divide them (randomly) into a training set and a validation set. The training set is used only for fitting the model(s) and the validation set is used for estimating the out-of-sample error. We could then choose the model that achieves the smallest error on the validation set. However note that the error of the chosen model is not a good estimate of the out-of-sample error, since again we have used the validation set for choosing a model. The drawback of this method is that the division into training and validation set is random and so if we are unlucky, the division does not represent the underlying distribution well.

An approach that tries to address this drawback is *leave-one-out cross-validation* (LOOCV). Here we again divide the observations into a training set and a validation set. The difference is that the validation set will only contain one observation, call this $(x_1,y_1)$. We fit the model on the $n-1$ observations in the training set and predict the observation in the validation set. We then calculate the prediction error and call this (in the regression case) $MSE_1$. We now repeat this, instead keeping $(x_2,y_2)$ in the validation set and all other in the training set. The LOOCV estimate of the out-of-sample error is then
$$
CV_{(n)}= \frac{1}{n}\sum_{i=1}^n MSE_i.
$$

The advantage compared to the validation set approach is that it always yields the same results and that there is less bias. This is because the size of the training set, $n-1$, is almost the same as the full data set, $n$. Therefore the fit should be almost as good as using the full data set.

The main drawback of LOOCV is that if $n$ is large, it could be expensive to implement. An alternative is therefore *k-fold cross-calidation*. Here the data is divided randomly into $k$ groups. We take the first group as validation set and the others as training set. This gives an out-of-sample error, call it $MSE_1$. We continue by treating the second group as validation set, giving $MSE_2$, and so on. The k-fold CV estimate of the out-of-sample error is then
$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i.
$$
In practice usually $k=5$ or $k=10$ is used.

## An application I

We will use the Hitters data set from the ISL book and predict the salary of a Baseball player based on various covariates.
```{r echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
library(caret)
library(ISLR)

data("Hitters", package = "ISLR")
Hitters <- na.omit(Hitters)
```
Then we randomly split the data into a train and a test set. We keep 70% of the observations in the training set and the rest in the test set.
```{r, cache = TRUE}
set.seed(3)
training.samples <- caret::createDataPartition(Hitters$Salary, 
                                               p = 0.7, 
                                               list = FALSE)
train.data  <- Hitters[training.samples, ]
test.data <- Hitters[-training.samples, ]
```
We then do linear regression and calculate the out-of-sample error
```{r, cache = TRUE}
ls <- lm(Salary ~., data = train.data)
predictions <- predict(ls, test.data)
sqrt(mean((predictions - test.data$Salary)^2))
```
Next we fit a lasso using 10-fold CV.
```{r, cache = TRUE}
lambda.grid <- 10^seq(-2, 2, length = 100)

set.seed(42)
lasso <- train(
  Salary ~., data = train.data, method = "glmnet",
  trControl = trainControl("repeatedcv", number = 10, repeats = 5),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda.grid)
)
```
We may plot the cross-validated error against the regularization parameter.
```{r lassoPlot, cache = TRUE, echo = TRUE, fig.cap='Cross-validated error against the regularization parameter for Lasso.', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
plot(lasso)
```
If we print the parameters, we see that some of them have been set to 0. 
```{r, cache = TRUE}
coef(lasso$finalModel, lasso$bestTune$lambda)
```
We calculate the out-of-sample error. 
```{r echo =TRUE, cache = TRUE}
predictions <- predict(lasso, test.data)
sqrt(mean((predictions - test.data$Salary)^2))
```
An improvement over least squares. We might also try elastic-net.
```{r, cache = TRUE}
lambda <- 10^seq(-2, 2.5, length = 100)
alpha <- seq(0, 0.5, length = 10)
set.seed(42)
e.net <- train(
  Salary ~., data = train.data, method = "glmnet",
  trControl = trainControl("repeatedcv", number = 10, repeats = 5),
  tuneGrid = expand.grid(alpha = alpha, lambda = lambda)
)
```
```{r elasticNetPlot, cache = TRUE, echo = TRUE, fig.cap='Cross-validated error against the regularization parameter for elastic net', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
plot(e.net)
```
```{r echo =TRUE, cache = TRUE}
predictions <- predict(e.net, test.data)
sqrt( mean((predictions - test.data$Salary)^2) )
```
We get a worse out-of-sample error. But we need to keep in mind that the number of observations in the test-set is not very large.

## An application II

Here we show an application using a dataset on whether or not a patient has diabetes, based on certain diagnostic measurements. First load the data and print some observations
```{r, cache = TRUE}
library(caret)
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)

knitr::kable(
  head(PimaIndiansDiabetes2), booktabs = TRUE,
  caption = 'Diabetes data')
```
Divide the data into a train and a test set.
```{r, cache = TRUE}
set.seed(42)

training.samples <- createDataPartition(PimaIndiansDiabetes2$diabetes, 
                                        p = 0.7, 
                                        list = FALSE)

train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
```
Then use a SVM, where the parameter is estimated by 10-fold CV. If you have a computer with multiple cores, there may be a speed-up by using the library doMC.
```{r echo = TRUE, warning=FALSE, error=FALSE, message=FALSE, cache = TRUE}
library(doMC)
registerDoMC(cores=8)
```
```{r echo = TRUE, warning=FALSE, error=FALSE, cache = TRUE}
model <- train(
  diabetes ~., data = train.data, method = "svmRadial",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10,
  preProcess = c("center","scale")
)
```
We may print a summary of the training of the model
```{r, cache = TRUE}
print(model)
```
We can also plot the cross-validated accuracy as a function of the regularization parameter
```{r SVMApplicationPlot, cache = TRUE, echo = TRUE, fig.cap='Cross-validated accuracy against the regularization parameter for the SVM', out.width='80%', fig.asp=.75, fig.align='center', warning=FALSE}
plot(model)
```
<p>The final results are</p>
```{r, cache = TRUE}
predicted <- predict(model, test.data)
mean(predicted == test.data$diabetes)
```

## Review questions

1. What is a classifier?
2. What is a loss function?
3. What is the out-of-sample error?
4. What is the in-sample error?
5. What is the Bayes classifier?
6. What is the Bayes risk?
7. What is the hinge loss?
8. Explain in general terms what Hoeffding's inequality is.
9. What is generalisation error?
10. What is the VC-dimension?
11. How does in general the generalisation error depend on the complexity of the model?
12. How does in general the generalisation error depend on the number of training samples?
13. What is the optimisation problem solved by support vector machines?
14. What is the bias-variance decomposition
15. What is the optimisation problem that Ridge regression/Lasso solves?
16. What is the main difference between the Ridge regression and Lasso?
17. What is a validation set?
18. What is the drawback of leave-one-out cross validation?
19. Explain k-fold cross validation.













